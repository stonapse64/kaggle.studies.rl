{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning).**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the tutorial, you learned a bit about reinforcement learning and used the `stable-baselines3` package to train an agent to beat a random opponent.  In this exercise, you will check your understanding and tinker with the code to deepen your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:45:23.924239Z",
     "iopub.status.busy": "2025-01-04T15:45:23.923814Z",
     "iopub.status.idle": "2025-01-04T15:45:24.351941Z",
     "shell.execute_reply": "2025-01-04T15:45:24.350918Z",
     "shell.execute_reply.started": "2025-01-04T15:45:23.924204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.game_ai.ex4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Set the architecture\n",
    "\n",
    "In the tutorial, you learned one way to design a neural network that can select moves in Connect Four.  The neural network had an output layer with seven nodes: one for each column in the game board.\n",
    "\n",
    "Say now you wanted to create a neural network that can play chess.  How many nodes should you put in the output layer?\n",
    "\n",
    "- Option A: 2 nodes (number of game players)\n",
    "- Option B: 16 nodes (number of game pieces that each player starts with)\n",
    "- Option C: 4672 nodes (number of possible moves)\n",
    "- Option D: 64 nodes (number of squares on the game board)\n",
    "\n",
    "Use your answer to set the value of the `best_option` variable below.  Your answer should be one of `'A'`, `'B'`, `'C'`, or `'D'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:18:59.987559Z",
     "iopub.status.busy": "2025-01-04T15:18:59.987072Z",
     "iopub.status.idle": "2025-01-04T15:18:59.997717Z",
     "shell.execute_reply": "2025-01-04T15:18:59.996356Z",
     "shell.execute_reply.started": "2025-01-04T15:18:59.987530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_PickBestOption\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc33\">Correct:</span> If we use a similar network as in the tutorial, the network should output a probability for each possible move."
      ],
      "text/plain": [
       "Correct: If we use a similar network as in the tutorial, the network should output a probability for each possible move."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fill in the blank\n",
    "best_option = 'C'\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-04T15:08:13.725315Z",
     "iopub.status.idle": "2025-01-04T15:08:13.725680Z",
     "shell.execute_reply": "2025-01-04T15:08:13.725546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lines below will give you solution code\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Decide reward\n",
    "\n",
    "In the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four.  Consider now training an agent to win at the game [Minesweeper](https://bit.ly/2T5xEY8).  The goal of the game is to clear the board without detonating any bombs.\n",
    "\n",
    "To play this game in Google Search, click on the **[Play]** button at [this link](https://www.google.com/search?q=minesweeper).  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/WzoEfKY.png\" width=50%><br/>\n",
    "</center>\n",
    "\n",
    "With each move, one of the following is true:\n",
    "- The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move).  Let's assume this ends the game, and the agent loses.\n",
    "- The agent clears a square that did not contain a hidden mine.  The agent wins the game, because all squares without mines are revealed.\n",
    "- The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.\n",
    "- The agent detonates a mine and loses the game.\n",
    "\n",
    "How might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game?\n",
    "\n",
    "After you have decided on your answer, run the code cell below to get credit for completing this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:19:19.630953Z",
     "iopub.status.busy": "2025-01-04T15:19:19.630459Z",
     "iopub.status.idle": "2025-01-04T15:19:19.640110Z",
     "shell.execute_reply": "2025-01-04T15:19:19.638915Z",
     "shell.execute_reply.started": "2025-01-04T15:19:19.630921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"2_DecideReward\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n",
       "- If agent wins the game in that move, it gets a reward of `+1`.\n",
       "- Else if the agent selects an invalid move, it gets a reward of `-10`.\n",
       "- Else if it detonates a mine, it gets a reward of `-1`.\n",
       "- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n",
       "\n",
       "To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."
      ],
      "text/plain": [
       "Solution: Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n",
       "- If agent wins the game in that move, it gets a reward of `+1`.\n",
       "- Else if the agent selects an invalid move, it gets a reward of `-10`.\n",
       "- Else if it detonates a mine, it gets a reward of `-1`.\n",
       "- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n",
       "\n",
       "To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "# invalid move > -10\n",
    "# clears square + win > 1000\n",
    "# clears square + continue > 1/288\n",
    "# mine > -1000\n",
    "q_2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (Optional) Amend the code\n",
    "\n",
    "In this next part of the exercise, you will amend the code from the tutorial to experiment with creating your own agents!  There are a lot of hyperparameters involved with specifying a reinforcement learning agent, and you'll have a chance to amend them, to see how performance is affected.\n",
    "\n",
    "First, we'll need to make sure that your Kaggle Notebook is set up to run the code.  Begin by looking at the \"Settings\" menu to the right of your notebook.  Your menu will look like one of the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/kR1az0y.png\" width=100%><br/>\n",
    "</center>\n",
    "\n",
    "If your \"Internet\" setting appears as a \"Requires phone verification\" link, click on this link.  This will bring you to a new window; then, follow the instructions to verify your account.  After following this step, your \"Internet\" setting will appear \"Off\", as in the example to the right.\n",
    "\n",
    "Once your \"Internet\" setting appears as \"Off\", click to turn it on.  You'll see a pop-up window that you'll need to \"Accept\" in order to complete the process and have the setting switched to \"On\".  Once the Internet is turned \"On\", you're ready to proceed!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/gOVh6Aa.png\" width=100%><br/>\n",
    "</center>\n",
    "\n",
    "Begin by running the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:26:19.728692Z",
     "iopub.status.busy": "2025-01-04T15:26:19.728484Z",
     "iopub.status.idle": "2025-01-04T15:26:40.595479Z",
     "shell.execute_reply": "2025-01-04T15:26:40.594720Z",
     "shell.execute_reply.started": "2025-01-04T15:26:19.728672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym(gym.Env):\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(1,self.rows,self.columns), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n",
    "    \n",
    "# Create ConnectFour environment \n",
    "env = ConnectFourGym(agent2=\"random\")\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "!pip install \"stable-baselines3\"\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "# Neural network for predicting action values\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # CxHxW images (channels first)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:26:40.599525Z",
     "iopub.status.busy": "2025-01-04T15:26:40.599294Z",
     "iopub.status.idle": "2025-01-04T15:33:37.122716Z",
     "shell.execute_reply": "2025-01-04T15:33:37.121907Z",
     "shell.execute_reply.started": "2025-01-04T15:26:40.599506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7ab7b10b7430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize agent\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:45:58.277140Z",
     "iopub.status.busy": "2025-01-04T15:45:58.276567Z",
     "iopub.status.idle": "2025-01-04T15:45:58.300047Z",
     "shell.execute_reply": "2025-01-04T15:45:58.299015Z",
     "shell.execute_reply.started": "2025-01-04T15:45:58.277107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# agent with n-step lookahead and alpha-beta pruning\n",
    "def my_nstep_lookahead_ab_pruning_agent(obs, config):\n",
    "    # Your code here: Amend the agent!\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    # Gets board at next step if agent drops piece in selected column\n",
    "    def drop_piece(grid, col, mark, config):\n",
    "        next_grid = grid.copy()\n",
    "        for row in range(config.rows-1, -1, -1):\n",
    "            if next_grid[row][col] == 0:\n",
    "                break\n",
    "        next_grid[row][col] = mark\n",
    "        return next_grid\n",
    "\n",
    "    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
    "    def check_window(window, num_discs, piece, config):\n",
    "        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "        \n",
    "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "    def count_windows(grid, num_discs, piece, config):\n",
    "        num_windows = 0\n",
    "        # horizontal\n",
    "        for row in range(config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[row, col:col+config.inarow])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # vertical\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns):\n",
    "                window = list(grid[row:row+config.inarow, col])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # positive diagonal\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # negative diagonal\n",
    "        for row in range(config.inarow-1, config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        return num_windows\n",
    "\n",
    "    # Helper function for get_heuristic: calculates value of heuristic for grid\n",
    "    def get_heuristic(grid, mark, config):\n",
    "        num_threes = count_windows(grid, 3, mark, config)\n",
    "        num_fours = count_windows(grid, 4, mark, config)\n",
    "        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n",
    "        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n",
    "        return score\n",
    "\n",
    "    # Uses minimax with alpha-beta pruning to calculate value of dropping piece in selected column\n",
    "    def score_move(grid, col, mark, config, nsteps, alpha=-float('inf'), beta=float('inf')):\n",
    "        next_grid = drop_piece(grid, col, mark, config)\n",
    "        score = minimax(next_grid, nsteps-1, False, mark, config, alpha, beta)\n",
    "        return score\n",
    "    \n",
    "    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
    "    def is_terminal_window(window, config):\n",
    "        return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
    "    \n",
    "    # Helper function for minimax: checks if game has ended\n",
    "    def is_terminal_node(grid, config):\n",
    "        # Check for draw \n",
    "        if list(grid[0, :]).count(0) == 0:\n",
    "            return True\n",
    "        # Check for win: horizontal, vertical, or diagonal\n",
    "        # horizontal \n",
    "        for row in range(config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[row, col:col+config.inarow])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # vertical\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns):\n",
    "                window = list(grid[row:row+config.inarow, col])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # positive diagonal\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # negative diagonal\n",
    "        for row in range(config.inarow-1, config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    # Minimax implementation with alpha-beta pruning\n",
    "    def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n",
    "        is_terminal = is_terminal_node(node, config)\n",
    "        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
    "        if depth == 0 or is_terminal:\n",
    "            return get_heuristic(node, mark, config)\n",
    "        if maximizingPlayer:\n",
    "            value = -np.inf\n",
    "            for col in valid_moves:\n",
    "                child = drop_piece(node, col, mark, config)\n",
    "                value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n",
    "                alpha = max(alpha, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return value\n",
    "        else:\n",
    "            value = np.inf\n",
    "            for col in valid_moves:\n",
    "                child = drop_piece(node, col, mark%2+1, config)\n",
    "                value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n",
    "                beta = min(beta, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return value\n",
    "\n",
    "    # agent driver\n",
    "    # How deep to make the game tree: higher values take longer to run!\n",
    "    # ConncectX comes with a max time per player move and also for all moves!\n",
    "    N_STEPS = 5\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:46:04.135580Z",
     "iopub.status.busy": "2025-01-04T15:46:04.135233Z",
     "iopub.status.idle": "2025-01-04T15:46:04.141003Z",
     "shell.execute_reply": "2025-01-04T15:46:04.139806Z",
     "shell.execute_reply.started": "2025-01-04T15:46:04.135553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def agent1(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:46:06.004409Z",
     "iopub.status.busy": "2025-01-04T15:46:06.004094Z",
     "iopub.status.idle": "2025-01-04T15:46:06.100962Z",
     "shell.execute_reply": "2025-01-04T15:46:06.099512Z",
     "shell.execute_reply.started": "2025-01-04T15:46:06.004384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c83fd52c6a61>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the game environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connectx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Two random agents play one game round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_nstep_lookahead_ab_pruning_agent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the game environment\n",
    "env = make(\"connectx\")\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([agent1, my_nstep_lookahead_ab_pruning_agent])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:34:44.866001Z",
     "iopub.status.busy": "2025-01-04T15:34:44.865726Z",
     "iopub.status.idle": "2025-01-04T15:34:44.871339Z",
     "shell.execute_reply": "2025-01-04T15:34:44.870441Z",
     "shell.execute_reply.started": "2025-01-04T15:34:44.865979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:37:28.893263Z",
     "iopub.status.busy": "2025-01-04T15:37:28.892944Z",
     "iopub.status.idle": "2025-01-04T15:38:39.231269Z",
     "shell.execute_reply": "2025-01-04T15:38:39.230589Z",
     "shell.execute_reply.started": "2025-01-04T15:37:28.893240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.06\n",
      "Agent 2 Win Percentage: 0.93\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_win_percentages_mc(agent1, agent2, n_rounds=100, num_cores=6):\n",
    "    \"\"\"\n",
    "    Calculates win percentages for two agents in a Connect Four game.\n",
    "\n",
    "    Args:\n",
    "        agent1: The first agent.\n",
    "        agent2: The second agent.\n",
    "        n_rounds: The total number of games to play.\n",
    "        num_cores: The number of CPU cores to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "\n",
    "    # Calculate the number of games per core\n",
    "    games_per_core = n_rounds // (2 * num_cores) \n",
    "    remaining_games = n_rounds % (2 * num_cores)\n",
    "\n",
    "    # Create a list of arguments for each process\n",
    "    args_list = []\n",
    "    for i in range(num_cores):\n",
    "        # Alternate agent order for each core\n",
    "        rounds = games_per_core\n",
    "        if i < remaining_games:\n",
    "            rounds += 1\n",
    "        args_list.append((\"connectx\", [agent1, agent2], config, [], rounds))\n",
    "        args_list.append((\"connectx\", [agent2, agent1], config, [], rounds))\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with Pool(processes=num_cores) as pool:\n",
    "        # Run the evaluation function in parallel\n",
    "        results = pool.starmap(evaluate, args_list)\n",
    "\n",
    "    # Combine the results from all cores\n",
    "    outcomes = []\n",
    "    for core_results in results:\n",
    "        outcomes.extend(core_results)\n",
    "\n",
    "    # Calculate and print win percentages and invalid play counts\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1, -1]) / len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1, 1]) / len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "get_win_percentages_mc(agent1=agent1, agent2=my_4step_lookahead_ab_pruning_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:36:09.066720Z",
     "iopub.status.busy": "2025-01-04T15:36:09.066423Z",
     "iopub.status.idle": "2025-01-04T15:36:09.074850Z",
     "shell.execute_reply": "2025-01-04T15:36:09.074148Z",
     "shell.execute_reply.started": "2025-01-04T15:36:09.066699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PPO in module stable_baselines3.ppo.ppo object:\n",
      "\n",
      "class PPO(stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm)\n",
      " |  PPO(policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0003, n_steps: int = 2048, batch_size: int = 64, n_epochs: int = 10, gamma: float = 0.99, gae_lambda: float = 0.95, clip_range: Union[float, Callable[[float], float]] = 0.2, clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None, normalize_advantage: bool = True, ent_coef: float = 0.0, vf_coef: float = 0.5, max_grad_norm: float = 0.5, use_sde: bool = False, sde_sample_freq: int = -1, target_kl: Optional[float] = None, stats_window_size: int = 100, tensorboard_log: Optional[str] = None, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n",
      " |  \n",
      " |  Proximal Policy Optimization algorithm (PPO) (clip version)\n",
      " |  \n",
      " |  Paper: https://arxiv.org/abs/1707.06347\n",
      " |  Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
      " |  https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
      " |  Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
      " |  \n",
      " |  Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
      " |  \n",
      " |  :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      " |  :param env: The environment to learn from (if registered in Gym, can be str)\n",
      " |  :param learning_rate: The learning rate, it can be a function\n",
      " |      of the current progress remaining (from 1 to 0)\n",
      " |  :param n_steps: The number of steps to run for each environment per update\n",
      " |      (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
      " |      NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
      " |      See https://github.com/pytorch/pytorch/issues/29372\n",
      " |  :param batch_size: Minibatch size\n",
      " |  :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
      " |  :param gamma: Discount factor\n",
      " |  :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
      " |  :param clip_range: Clipping parameter, it can be a function of the current progress\n",
      " |      remaining (from 1 to 0).\n",
      " |  :param clip_range_vf: Clipping parameter for the value function,\n",
      " |      it can be a function of the current progress remaining (from 1 to 0).\n",
      " |      This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
      " |      no clipping will be done on the value function.\n",
      " |      IMPORTANT: this clipping depends on the reward scaling.\n",
      " |  :param normalize_advantage: Whether to normalize or not the advantage\n",
      " |  :param ent_coef: Entropy coefficient for the loss calculation\n",
      " |  :param vf_coef: Value function coefficient for the loss calculation\n",
      " |  :param max_grad_norm: The maximum value for the gradient clipping\n",
      " |  :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
      " |      instead of action noise exploration (default: False)\n",
      " |  :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
      " |      Default: -1 (only sample at the beginning of the rollout)\n",
      " |  :param target_kl: Limit the KL divergence between updates,\n",
      " |      because the clipping is not enough to prevent large update\n",
      " |      see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
      " |      By default, there is no limit on the kl div.\n",
      " |  :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      " |      the reported success rate, mean episode length, and mean reward over\n",
      " |  :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      " |  :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      " |  :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      " |      debug messages\n",
      " |  :param seed: Seed for the pseudo random generators\n",
      " |  :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      " |      Setting it to auto, the code will be run on the GPU if possible.\n",
      " |  :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PPO\n",
      " |      stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm\n",
      " |      stable_baselines3.common.base_class.BaseAlgorithm\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0003, n_steps: int = 2048, batch_size: int = 64, n_epochs: int = 10, gamma: float = 0.99, gae_lambda: float = 0.95, clip_range: Union[float, Callable[[float], float]] = 0.2, clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None, normalize_advantage: bool = True, ent_coef: float = 0.0, vf_coef: float = 0.5, max_grad_norm: float = 0.5, use_sde: bool = False, sde_sample_freq: int = -1, target_kl: Optional[float] = None, stats_window_size: int = 100, tensorboard_log: Optional[str] = None, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  learn(self: ~SelfPPO, total_timesteps: int, callback: Union[NoneType, Callable, List[stable_baselines3.common.callbacks.BaseCallback], stable_baselines3.common.callbacks.BaseCallback] = None, log_interval: int = 1, tb_log_name: str = 'PPO', reset_num_timesteps: bool = True, progress_bar: bool = False) -> ~SelfPPO\n",
      " |      Return a trained model.\n",
      " |      \n",
      " |      :param total_timesteps: The total number of samples (env steps) to train on\n",
      " |      :param callback: callback(s) called at every step with state of the algorithm.\n",
      " |      :param log_interval: The number of episodes before logging.\n",
      " |      :param tb_log_name: the name of the run for TensorBoard logging\n",
      " |      :param reset_num_timesteps: whether or not to reset the current timestep number (used in logging)\n",
      " |      :param progress_bar: Display a progress bar using tqdm and rich.\n",
      " |      :return: the trained model\n",
      " |  \n",
      " |  train(self) -> None\n",
      " |      Update policy using the currently gathered rollout buffer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'policy_aliases': typing.ClassVar[typing.Dict[str, ...\n",
      " |  \n",
      " |  policy_aliases = {'CnnPolicy': <class 'stable_baselines3.common.polici...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm:\n",
      " |  \n",
      " |  collect_rollouts(self, env: stable_baselines3.common.vec_env.base_vec_env.VecEnv, callback: stable_baselines3.common.callbacks.BaseCallback, rollout_buffer: stable_baselines3.common.buffers.RolloutBuffer, n_rollout_steps: int) -> bool\n",
      " |      Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
      " |      The term rollout here refers to the model-free notion and should not\n",
      " |      be used with the concept of rollout used in model-based RL or planning.\n",
      " |      \n",
      " |      :param env: The training environment\n",
      " |      :param callback: Callback that will be called at each step\n",
      " |          (and at the beginning and end of the rollout)\n",
      " |      :param rollout_buffer: Buffer to fill with rollouts\n",
      " |      :param n_rollout_steps: Number of experiences to collect per environment\n",
      " |      :return: True if function returned with at least `n_rollout_steps`\n",
      " |          collected, False if callback terminated rollout prematurely.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  get_env(self) -> Optional[stable_baselines3.common.vec_env.base_vec_env.VecEnv]\n",
      " |      Returns the current environment (can be None if not defined).\n",
      " |      \n",
      " |      :return: The current environment\n",
      " |  \n",
      " |  get_parameters(self) -> Dict[str, Dict]\n",
      " |      Return the parameters of the agent. This includes parameters from different networks, e.g.\n",
      " |      critics (value functions) and policies (pi functions).\n",
      " |      \n",
      " |      :return: Mapping of from names of the objects to PyTorch state-dicts.\n",
      " |  \n",
      " |  get_vec_normalize_env(self) -> Optional[stable_baselines3.common.vec_env.vec_normalize.VecNormalize]\n",
      " |      Return the ``VecNormalize`` wrapper of the training env\n",
      " |      if it exists.\n",
      " |      \n",
      " |      :return: The ``VecNormalize`` env.\n",
      " |  \n",
      " |  predict(self, observation: Union[numpy.ndarray, Dict[str, numpy.ndarray]], state: Optional[Tuple[numpy.ndarray, ...]] = None, episode_start: Optional[numpy.ndarray] = None, deterministic: bool = False) -> Tuple[numpy.ndarray, Optional[Tuple[numpy.ndarray, ...]]]\n",
      " |      Get the policy action from an observation (and optional hidden state).\n",
      " |      Includes sugar-coating to handle different observations (e.g. normalizing images).\n",
      " |      \n",
      " |      :param observation: the input observation\n",
      " |      :param state: The last hidden states (can be None, used in recurrent policies)\n",
      " |      :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      " |          this correspond to beginning of episodes,\n",
      " |          where the hidden states of the RNN must be reset.\n",
      " |      :param deterministic: Whether or not to return deterministic actions.\n",
      " |      :return: the model's action and the next hidden state\n",
      " |          (used in recurrent policies)\n",
      " |  \n",
      " |  save(self, path: Union[str, pathlib.Path, io.BufferedIOBase], exclude: Optional[Iterable[str]] = None, include: Optional[Iterable[str]] = None) -> None\n",
      " |      Save all the attributes of the object and the model parameters in a zip-file.\n",
      " |      \n",
      " |      :param path: path to the file where the rl agent should be saved\n",
      " |      :param exclude: name of parameters that should be excluded in addition to the default ones\n",
      " |      :param include: name of parameters that might be excluded but should be included anyway\n",
      " |  \n",
      " |  set_env(self, env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv], force_reset: bool = True) -> None\n",
      " |      Checks the validity of the environment, and if it is coherent, set it as the current environment.\n",
      " |      Furthermore wrap any non vectorized env into a vectorized\n",
      " |      checked parameters:\n",
      " |      - observation_space\n",
      " |      - action_space\n",
      " |      \n",
      " |      :param env: The environment for learning a policy\n",
      " |      :param force_reset: Force call to ``reset()`` before training\n",
      " |          to avoid unexpected behavior.\n",
      " |          See issue https://github.com/DLR-RM/stable-baselines3/issues/597\n",
      " |  \n",
      " |  set_logger(self, logger: stable_baselines3.common.logger.Logger) -> None\n",
      " |      Setter for for logger object.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |        When passing a custom logger object,\n",
      " |        this will overwrite ``tensorboard_log`` and ``verbose`` settings\n",
      " |        passed to the constructor.\n",
      " |  \n",
      " |  set_parameters(self, load_path_or_dict: Union[str, Dict[str, torch.Tensor]], exact_match: bool = True, device: Union[torch.device, str] = 'auto') -> None\n",
      " |      Load parameters from a given zip-file or a nested dictionary containing parameters for\n",
      " |      different modules (see ``get_parameters``).\n",
      " |      \n",
      " |      :param load_path_or_iter: Location of the saved data (path or file-like, see ``save``), or a nested\n",
      " |          dictionary containing nn.Module parameters used by the policy. The dictionary maps\n",
      " |          object names to a state-dictionary returned by ``torch.nn.Module.state_dict()``.\n",
      " |      :param exact_match: If True, the given parameters should include parameters for each\n",
      " |          module and each of their parameters, otherwise raises an Exception. If set to False, this\n",
      " |          can be used to update only specific parameters.\n",
      " |      :param device: Device on which the code should run.\n",
      " |  \n",
      " |  set_random_seed(self, seed: Optional[int] = None) -> None\n",
      " |      Set the seed of the pseudo-random generators\n",
      " |      (python, numpy, pytorch, gym, action_space)\n",
      " |      \n",
      " |      :param seed:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  load(path: Union[str, pathlib.Path, io.BufferedIOBase], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, NoneType] = None, device: Union[torch.device, str] = 'auto', custom_objects: Optional[Dict[str, Any]] = None, print_system_info: bool = False, force_reset: bool = True, **kwargs) -> ~SelfBaseAlgorithm from abc.ABCMeta\n",
      " |      Load the model from a zip-file.\n",
      " |      Warning: ``load`` re-creates the model from scratch, it does not update it in-place!\n",
      " |      For an in-place load use ``set_parameters`` instead.\n",
      " |      \n",
      " |      :param path: path to the file (or a file-like) where to\n",
      " |          load the agent from\n",
      " |      :param env: the new environment to run the loaded model on\n",
      " |          (can be None if you only need prediction from a trained model) has priority over any saved environment\n",
      " |      :param device: Device on which the code should run.\n",
      " |      :param custom_objects: Dictionary of objects to replace\n",
      " |          upon loading. If a variable is present in this dictionary as a\n",
      " |          key, it will not be deserialized and the corresponding item\n",
      " |          will be used instead. Similar to custom_objects in\n",
      " |          ``keras.models.load_model``. Useful when you have an object in\n",
      " |          file that can not be deserialized.\n",
      " |      :param print_system_info: Whether to print system info from the saved model\n",
      " |          and the current system info (useful to debug loading issues)\n",
      " |      :param force_reset: Force call to ``reset()`` before training\n",
      " |          to avoid unexpected behavior.\n",
      " |          See https://github.com/DLR-RM/stable-baselines3/issues/597\n",
      " |      :param kwargs: extra arguments to change the model when loading\n",
      " |      :return: new model instance with loaded parameters\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  logger\n",
      " |      Getter for the logger object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n",
    "- learn more about the [`stable-baselines3` package](https://stable-baselines3.readthedocs.io/en/master/) to amend the agent.\n",
    "- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed the course, and it's time to put your new skills to work!  \n",
    "\n",
    "The next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n",
    "\n",
    "You can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n",
    "\n",
    "As we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning/discussion) to chat with other learners.*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 899221,
     "sourceId": 17592,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
