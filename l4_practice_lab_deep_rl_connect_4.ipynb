{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is a practice lab in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning). Some code snippets are from the tutorial [Reinforcement Learning in Python with Stable Baselines 3](https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/).**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this practice lab I want to train an agent against different opponent agents. My aims are\n",
    "- to understand how the performance changes after each training against a specific agent\n",
    "- to understand how the overall performance changes across all trainings against all agents\n",
    "- to benchmark the agent against a very strong opponent. See the [collection](connectX_agents_collection) gathered from [vyacheslavbolotin](https://www.kaggle.com/vyacheslavbolotin)'s [overview](https://www.kaggle.com/code/vyacheslavbolotin/agents-connect-x)\n",
    "- to potentially test various models\n",
    "- to potentially test various reward schemes (I liked the idea of [Pascal Pons](http://blog.gamesolver.org/solving-connect-four/01-introduction/) to take the number of moves required to win the game into account)\n",
    "- to apply the framework of my [practice lab cart pole](practice_lab_stable_baselines3_cartpole_v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "1. I want to develop the solution step-wise starting with the \"prooven\" approach from the Kaggle course.\n",
    "2. I'll add TensorBoard to better monitor the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set the architecture (tbd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-04T15:08:13.725315Z",
     "iopub.status.idle": "2025-01-04T15:08:13.725680Z",
     "shell.execute_reply": "2025-01-04T15:08:13.725546Z"
    },
    "trusted": true
   },
   "source": [
    "Here I'll add some thoughts about the model, its architecture, and the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decide reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: [Pascal Pons](http://blog.gamesolver.org/solving-connect-four/02-test-protocol/)\n",
    "We define a score for any non final position reflecting the outcome of the game for the player to play, considering that both players play perfectly and try to win as soon as possible or lose as late as possible. A position has:\n",
    "- a positive score if the current player can win. 1 if it wins with the last stone, 2 if it wins with your second last stone and so on…\n",
    "- a null score if the game will end by a draw game\n",
    "- a negative score if the current player lose whatever it plays. -1 if the opponent wins with the last stone, -2 if the opponent wins with the second last stone and so on…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: [Kaggle](https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning)\n",
    "- If the agent wins the game in that move, we give it a reward of +1.\n",
    "- Else if the agent plays an invalid move (which ends the game), we give it a reward of -10.\n",
    "- Else if the opponent wins the game in its next move (i.e., the agent failed to prevent its opponent from winning), we give the agent a reward of -1.\n",
    "- Else, the agent gets a reward of 1/42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic environment, agents playing the game and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from kaggle_environments import make, evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the game environment\n",
    "env = make(\"connectx\")\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([\"random\", \"random\"])\n",
    "\n",
    "# Enjoy the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function including print\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Draw Percentage:\", np.round(outcomes.count([0,0])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for evaluation\n",
    "agent1=\"random\"\n",
    "agent2=\"negamax\"\n",
    "n_rounds=100\n",
    "\n",
    "# Call evaluation function\n",
    "get_win_percentages(agent1, agent2, n_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom environment and neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:26:19.728692Z",
     "iopub.status.busy": "2025-01-04T15:26:19.728484Z",
     "iopub.status.idle": "2025-01-04T15:26:40.595479Z",
     "shell.execute_reply": "2025-01-04T15:26:40.594720Z",
     "shell.execute_reply.started": "2025-01-04T15:26:19.728672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym(gym.Env):\n",
    "    def __init__(self, agent2=\"negamax\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(1,self.rows,self.columns), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n",
    "    \n",
    "# Create ConnectFour environment \n",
    "env = ConnectFourGym(agent2=\"random\")\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "!pip install \"stable-baselines3\"\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "# Neural network for predicting action values\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # CxHxW images (channels first)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opponents for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent with n-step lookahead and alpha-beta pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:45:58.277140Z",
     "iopub.status.busy": "2025-01-04T15:45:58.276567Z",
     "iopub.status.idle": "2025-01-04T15:45:58.300047Z",
     "shell.execute_reply": "2025-01-04T15:45:58.299015Z",
     "shell.execute_reply.started": "2025-01-04T15:45:58.277107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# agent with n-step lookahead and alpha-beta pruning\n",
    "def my_nstep_lookahead_ab_pruning_agent(obs, config):\n",
    "    # Your code here: Amend the agent!\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    # Gets board at next step if agent drops piece in selected column\n",
    "    def drop_piece(grid, col, mark, config):\n",
    "        next_grid = grid.copy()\n",
    "        for row in range(config.rows-1, -1, -1):\n",
    "            if next_grid[row][col] == 0:\n",
    "                break\n",
    "        next_grid[row][col] = mark\n",
    "        return next_grid\n",
    "\n",
    "    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
    "    def check_window(window, num_discs, piece, config):\n",
    "        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "        \n",
    "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "    def count_windows(grid, num_discs, piece, config):\n",
    "        num_windows = 0\n",
    "        # horizontal\n",
    "        for row in range(config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[row, col:col+config.inarow])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # vertical\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns):\n",
    "                window = list(grid[row:row+config.inarow, col])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # positive diagonal\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # negative diagonal\n",
    "        for row in range(config.inarow-1, config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        return num_windows\n",
    "\n",
    "    # Helper function for get_heuristic: calculates value of heuristic for grid\n",
    "    def get_heuristic(grid, mark, config):\n",
    "        num_threes = count_windows(grid, 3, mark, config)\n",
    "        num_fours = count_windows(grid, 4, mark, config)\n",
    "        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n",
    "        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n",
    "        return score\n",
    "\n",
    "    # Uses minimax with alpha-beta pruning to calculate value of dropping piece in selected column\n",
    "    def score_move(grid, col, mark, config, nsteps, alpha=-float('inf'), beta=float('inf')):\n",
    "        next_grid = drop_piece(grid, col, mark, config)\n",
    "        score = minimax(next_grid, nsteps-1, False, mark, config, alpha, beta)\n",
    "        return score\n",
    "    \n",
    "    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
    "    def is_terminal_window(window, config):\n",
    "        return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
    "    \n",
    "    # Helper function for minimax: checks if game has ended\n",
    "    def is_terminal_node(grid, config):\n",
    "        # Check for draw \n",
    "        if list(grid[0, :]).count(0) == 0:\n",
    "            return True\n",
    "        # Check for win: horizontal, vertical, or diagonal\n",
    "        # horizontal \n",
    "        for row in range(config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[row, col:col+config.inarow])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # vertical\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns):\n",
    "                window = list(grid[row:row+config.inarow, col])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # positive diagonal\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # negative diagonal\n",
    "        for row in range(config.inarow-1, config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    # Minimax implementation with alpha-beta pruning\n",
    "    def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n",
    "        is_terminal = is_terminal_node(node, config)\n",
    "        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
    "        if depth == 0 or is_terminal:\n",
    "            return get_heuristic(node, mark, config)\n",
    "        if maximizingPlayer:\n",
    "            value = -np.inf\n",
    "            for col in valid_moves:\n",
    "                child = drop_piece(node, col, mark, config)\n",
    "                value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n",
    "                alpha = max(alpha, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return value\n",
    "        else:\n",
    "            value = np.inf\n",
    "            for col in valid_moves:\n",
    "                child = drop_piece(node, col, mark%2+1, config)\n",
    "                value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n",
    "                beta = min(beta, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return value\n",
    "\n",
    "    # agent driver\n",
    "    # How deep to make the game tree: higher values take longer to run!\n",
    "    # ConncectX comes with a max time per player move and also for all moves!\n",
    "    N_STEPS = 5\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:26:40.599525Z",
     "iopub.status.busy": "2025-01-04T15:26:40.599294Z",
     "iopub.status.idle": "2025-01-04T15:33:37.122716Z",
     "shell.execute_reply": "2025-01-04T15:33:37.121907Z",
     "shell.execute_reply.started": "2025-01-04T15:26:40.599506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:46:04.135580Z",
     "iopub.status.busy": "2025-01-04T15:46:04.135233Z",
     "iopub.status.idle": "2025-01-04T15:46:04.141003Z",
     "shell.execute_reply": "2025-01-04T15:46:04.139806Z",
     "shell.execute_reply.started": "2025-01-04T15:46:04.135553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def agent1(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:46:06.004409Z",
     "iopub.status.busy": "2025-01-04T15:46:06.004094Z",
     "iopub.status.idle": "2025-01-04T15:46:06.100962Z",
     "shell.execute_reply": "2025-01-04T15:46:06.099512Z",
     "shell.execute_reply.started": "2025-01-04T15:46:06.004384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the game environment\n",
    "env = make(\"connectx\")\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([agent1, \"random\"])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:34:44.866001Z",
     "iopub.status.busy": "2025-01-04T15:34:44.865726Z",
     "iopub.status.idle": "2025-01-04T15:34:44.871339Z",
     "shell.execute_reply": "2025-01-04T15:34:44.870441Z",
     "shell.execute_reply.started": "2025-01-04T15:34:44.865979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:37:28.893263Z",
     "iopub.status.busy": "2025-01-04T15:37:28.892944Z",
     "iopub.status.idle": "2025-01-04T15:38:39.231269Z",
     "shell.execute_reply": "2025-01-04T15:38:39.230589Z",
     "shell.execute_reply.started": "2025-01-04T15:37:28.893240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_win_percentages_mc(agent1, agent2, n_rounds=100, num_cores=6):\n",
    "    \"\"\"\n",
    "    Calculates win percentages for two agents in a Connect Four game.\n",
    "\n",
    "    Args:\n",
    "        agent1: The first agent.\n",
    "        agent2: The second agent.\n",
    "        n_rounds: The total number of games to play.\n",
    "        num_cores: The number of CPU cores to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "\n",
    "    # Calculate the number of games per core\n",
    "    games_per_core = n_rounds // (2 * num_cores) \n",
    "    remaining_games = n_rounds % (2 * num_cores)\n",
    "\n",
    "    # Create a list of arguments for each process\n",
    "    args_list = []\n",
    "    for i in range(num_cores):\n",
    "        # Alternate agent order for each core\n",
    "        rounds = games_per_core\n",
    "        if i < remaining_games:\n",
    "            rounds += 1\n",
    "        args_list.append((\"connectx\", [agent1, agent2], config, [], rounds))\n",
    "        args_list.append((\"connectx\", [agent2, agent1], config, [], rounds))\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with Pool(processes=num_cores) as pool:\n",
    "        # Run the evaluation function in parallel\n",
    "        results = pool.starmap(evaluate, args_list)\n",
    "\n",
    "    # Combine the results from all cores\n",
    "    outcomes = []\n",
    "    for core_results in results:\n",
    "        outcomes.extend(core_results)\n",
    "\n",
    "    # Calculate and print win percentages and invalid play counts\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1, -1]) / len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1, 1]) / len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "get_win_percentages_mc(agent1=agent1, agent2=my_4step_lookahead_ab_pruning_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning/discussion) to chat with other learners.*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 899221,
     "sourceId": 17592,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
