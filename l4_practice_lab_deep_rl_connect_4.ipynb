{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is a practice lab in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning).**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this practice lab I want to train an agent against different opponent agents. My aims are\n",
    "- to understand how the performance changes after each training against a specific agent\n",
    "- to understand how the overall performance changes across all trainings against all agents\n",
    "- to benchmark the agent against a very strong opponent. See the [collection](connectX_agents_collection) gathered from [vyacheslavbolotin](https://www.kaggle.com/vyacheslavbolotin)'s [overview](https://www.kaggle.com/code/vyacheslavbolotin/agents-connect-x)\n",
    "- to potentially test various models\n",
    "- to potentially test various reward schemes (I liked the idea of [Pascal Pons](http://blog.gamesolver.org/solving-connect-four/01-introduction/) to take the number of moves required to win the game into account)\n",
    "- to apply the framework of my [practice lab cart pole](practice_lab_stable_baselines3_cartpole_v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:45:23.924239Z",
     "iopub.status.busy": "2025-01-04T15:45:23.923814Z",
     "iopub.status.idle": "2025-01-04T15:45:24.351941Z",
     "shell.execute_reply": "2025-01-04T15:45:24.350918Z",
     "shell.execute_reply.started": "2025-01-04T15:45:23.924204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.game_ai.ex4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Set the architecture\n",
    "\n",
    "In the tutorial, you learned one way to design a neural network that can select moves in Connect Four.  The neural network had an output layer with seven nodes: one for each column in the game board.\n",
    "\n",
    "Say now you wanted to create a neural network that can play chess.  How many nodes should you put in the output layer?\n",
    "\n",
    "- Option A: 2 nodes (number of game players)\n",
    "- Option B: 16 nodes (number of game pieces that each player starts with)\n",
    "- Option C: 4672 nodes (number of possible moves)\n",
    "- Option D: 64 nodes (number of squares on the game board)\n",
    "\n",
    "Use your answer to set the value of the `best_option` variable below.  Your answer should be one of `'A'`, `'B'`, `'C'`, or `'D'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:18:59.987559Z",
     "iopub.status.busy": "2025-01-04T15:18:59.987072Z",
     "iopub.status.idle": "2025-01-04T15:18:59.997717Z",
     "shell.execute_reply": "2025-01-04T15:18:59.996356Z",
     "shell.execute_reply.started": "2025-01-04T15:18:59.987530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_PickBestOption\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc33\">Correct:</span> If we use a similar network as in the tutorial, the network should output a probability for each possible move."
      ],
      "text/plain": [
       "Correct: If we use a similar network as in the tutorial, the network should output a probability for each possible move."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fill in the blank\n",
    "best_option = 'C'\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-04T15:08:13.725315Z",
     "iopub.status.idle": "2025-01-04T15:08:13.725680Z",
     "shell.execute_reply": "2025-01-04T15:08:13.725546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lines below will give you solution code\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Decide reward\n",
    "\n",
    "In the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four.  Consider now training an agent to win at the game [Minesweeper](https://bit.ly/2T5xEY8).  The goal of the game is to clear the board without detonating any bombs.\n",
    "\n",
    "To play this game in Google Search, click on the **[Play]** button at [this link](https://www.google.com/search?q=minesweeper).  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/WzoEfKY.png\" width=50%><br/>\n",
    "</center>\n",
    "\n",
    "With each move, one of the following is true:\n",
    "- The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move).  Let's assume this ends the game, and the agent loses.\n",
    "- The agent clears a square that did not contain a hidden mine.  The agent wins the game, because all squares without mines are revealed.\n",
    "- The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.\n",
    "- The agent detonates a mine and loses the game.\n",
    "\n",
    "How might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game?\n",
    "\n",
    "After you have decided on your answer, run the code cell below to get credit for completing this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:19:19.630953Z",
     "iopub.status.busy": "2025-01-04T15:19:19.630459Z",
     "iopub.status.idle": "2025-01-04T15:19:19.640110Z",
     "shell.execute_reply": "2025-01-04T15:19:19.638915Z",
     "shell.execute_reply.started": "2025-01-04T15:19:19.630921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"2_DecideReward\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n",
       "- If agent wins the game in that move, it gets a reward of `+1`.\n",
       "- Else if the agent selects an invalid move, it gets a reward of `-10`.\n",
       "- Else if it detonates a mine, it gets a reward of `-1`.\n",
       "- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n",
       "\n",
       "To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."
      ],
      "text/plain": [
       "Solution: Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n",
       "- If agent wins the game in that move, it gets a reward of `+1`.\n",
       "- Else if the agent selects an invalid move, it gets a reward of `-10`.\n",
       "- Else if it detonates a mine, it gets a reward of `-1`.\n",
       "- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n",
       "\n",
       "To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "# invalid move > -10\n",
    "# clears square + win > 1000\n",
    "# clears square + continue > 1/288\n",
    "# mine > -1000\n",
    "q_2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom environment and neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:26:19.728692Z",
     "iopub.status.busy": "2025-01-04T15:26:19.728484Z",
     "iopub.status.idle": "2025-01-04T15:26:40.595479Z",
     "shell.execute_reply": "2025-01-04T15:26:40.594720Z",
     "shell.execute_reply.started": "2025-01-04T15:26:19.728672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym(gym.Env):\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(1,self.rows,self.columns), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n",
    "    \n",
    "# Create ConnectFour environment \n",
    "env = ConnectFourGym(agent2=\"random\")\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "!pip install \"stable-baselines3\"\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "# Neural network for predicting action values\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # CxHxW images (channels first)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opponents for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent with n-step lookahead and alpha-beta pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:45:58.277140Z",
     "iopub.status.busy": "2025-01-04T15:45:58.276567Z",
     "iopub.status.idle": "2025-01-04T15:45:58.300047Z",
     "shell.execute_reply": "2025-01-04T15:45:58.299015Z",
     "shell.execute_reply.started": "2025-01-04T15:45:58.277107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# agent with n-step lookahead and alpha-beta pruning\n",
    "def my_nstep_lookahead_ab_pruning_agent(obs, config):\n",
    "    # Your code here: Amend the agent!\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    # Gets board at next step if agent drops piece in selected column\n",
    "    def drop_piece(grid, col, mark, config):\n",
    "        next_grid = grid.copy()\n",
    "        for row in range(config.rows-1, -1, -1):\n",
    "            if next_grid[row][col] == 0:\n",
    "                break\n",
    "        next_grid[row][col] = mark\n",
    "        return next_grid\n",
    "\n",
    "    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
    "    def check_window(window, num_discs, piece, config):\n",
    "        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "        \n",
    "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "    def count_windows(grid, num_discs, piece, config):\n",
    "        num_windows = 0\n",
    "        # horizontal\n",
    "        for row in range(config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[row, col:col+config.inarow])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # vertical\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns):\n",
    "                window = list(grid[row:row+config.inarow, col])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # positive diagonal\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        # negative diagonal\n",
    "        for row in range(config.inarow-1, config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                if check_window(window, num_discs, piece, config):\n",
    "                    num_windows += 1\n",
    "        return num_windows\n",
    "\n",
    "    # Helper function for get_heuristic: calculates value of heuristic for grid\n",
    "    def get_heuristic(grid, mark, config):\n",
    "        num_threes = count_windows(grid, 3, mark, config)\n",
    "        num_fours = count_windows(grid, 4, mark, config)\n",
    "        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n",
    "        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n",
    "        return score\n",
    "\n",
    "    # Uses minimax with alpha-beta pruning to calculate value of dropping piece in selected column\n",
    "    def score_move(grid, col, mark, config, nsteps, alpha=-float('inf'), beta=float('inf')):\n",
    "        next_grid = drop_piece(grid, col, mark, config)\n",
    "        score = minimax(next_grid, nsteps-1, False, mark, config, alpha, beta)\n",
    "        return score\n",
    "    \n",
    "    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
    "    def is_terminal_window(window, config):\n",
    "        return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
    "    \n",
    "    # Helper function for minimax: checks if game has ended\n",
    "    def is_terminal_node(grid, config):\n",
    "        # Check for draw \n",
    "        if list(grid[0, :]).count(0) == 0:\n",
    "            return True\n",
    "        # Check for win: horizontal, vertical, or diagonal\n",
    "        # horizontal \n",
    "        for row in range(config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[row, col:col+config.inarow])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # vertical\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns):\n",
    "                window = list(grid[row:row+config.inarow, col])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # positive diagonal\n",
    "        for row in range(config.rows-(config.inarow-1)):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        # negative diagonal\n",
    "        for row in range(config.inarow-1, config.rows):\n",
    "            for col in range(config.columns-(config.inarow-1)):\n",
    "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                if is_terminal_window(window, config):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    # Minimax implementation with alpha-beta pruning\n",
    "    def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n",
    "        is_terminal = is_terminal_node(node, config)\n",
    "        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
    "        if depth == 0 or is_terminal:\n",
    "            return get_heuristic(node, mark, config)\n",
    "        if maximizingPlayer:\n",
    "            value = -np.inf\n",
    "            for col in valid_moves:\n",
    "                child = drop_piece(node, col, mark, config)\n",
    "                value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n",
    "                alpha = max(alpha, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return value\n",
    "        else:\n",
    "            value = np.inf\n",
    "            for col in valid_moves:\n",
    "                child = drop_piece(node, col, mark%2+1, config)\n",
    "                value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n",
    "                beta = min(beta, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return value\n",
    "\n",
    "    # agent driver\n",
    "    # How deep to make the game tree: higher values take longer to run!\n",
    "    # ConncectX comes with a max time per player move and also for all moves!\n",
    "    N_STEPS = 5\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:26:40.599525Z",
     "iopub.status.busy": "2025-01-04T15:26:40.599294Z",
     "iopub.status.idle": "2025-01-04T15:33:37.122716Z",
     "shell.execute_reply": "2025-01-04T15:33:37.121907Z",
     "shell.execute_reply.started": "2025-01-04T15:26:40.599506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7ab7b10b7430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize agent\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:46:04.135580Z",
     "iopub.status.busy": "2025-01-04T15:46:04.135233Z",
     "iopub.status.idle": "2025-01-04T15:46:04.141003Z",
     "shell.execute_reply": "2025-01-04T15:46:04.139806Z",
     "shell.execute_reply.started": "2025-01-04T15:46:04.135553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def agent1(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:46:06.004409Z",
     "iopub.status.busy": "2025-01-04T15:46:06.004094Z",
     "iopub.status.idle": "2025-01-04T15:46:06.100962Z",
     "shell.execute_reply": "2025-01-04T15:46:06.099512Z",
     "shell.execute_reply.started": "2025-01-04T15:46:06.004384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c83fd52c6a61>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the game environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connectx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Two random agents play one game round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_nstep_lookahead_ab_pruning_agent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the game environment\n",
    "env = make(\"connectx\")\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([agent1, my_nstep_lookahead_ab_pruning_agent])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:34:44.866001Z",
     "iopub.status.busy": "2025-01-04T15:34:44.865726Z",
     "iopub.status.idle": "2025-01-04T15:34:44.871339Z",
     "shell.execute_reply": "2025-01-04T15:34:44.870441Z",
     "shell.execute_reply.started": "2025-01-04T15:34:44.865979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T15:37:28.893263Z",
     "iopub.status.busy": "2025-01-04T15:37:28.892944Z",
     "iopub.status.idle": "2025-01-04T15:38:39.231269Z",
     "shell.execute_reply": "2025-01-04T15:38:39.230589Z",
     "shell.execute_reply.started": "2025-01-04T15:37:28.893240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.06\n",
      "Agent 2 Win Percentage: 0.93\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(agent1=agent1, agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_win_percentages_mc(agent1, agent2, n_rounds=100, num_cores=6):\n",
    "    \"\"\"\n",
    "    Calculates win percentages for two agents in a Connect Four game.\n",
    "\n",
    "    Args:\n",
    "        agent1: The first agent.\n",
    "        agent2: The second agent.\n",
    "        n_rounds: The total number of games to play.\n",
    "        num_cores: The number of CPU cores to use for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "\n",
    "    # Calculate the number of games per core\n",
    "    games_per_core = n_rounds // (2 * num_cores) \n",
    "    remaining_games = n_rounds % (2 * num_cores)\n",
    "\n",
    "    # Create a list of arguments for each process\n",
    "    args_list = []\n",
    "    for i in range(num_cores):\n",
    "        # Alternate agent order for each core\n",
    "        rounds = games_per_core\n",
    "        if i < remaining_games:\n",
    "            rounds += 1\n",
    "        args_list.append((\"connectx\", [agent1, agent2], config, [], rounds))\n",
    "        args_list.append((\"connectx\", [agent2, agent1], config, [], rounds))\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with Pool(processes=num_cores) as pool:\n",
    "        # Run the evaluation function in parallel\n",
    "        results = pool.starmap(evaluate, args_list)\n",
    "\n",
    "    # Combine the results from all cores\n",
    "    outcomes = []\n",
    "    for core_results in results:\n",
    "        outcomes.extend(core_results)\n",
    "\n",
    "    # Calculate and print win percentages and invalid play counts\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1, -1]) / len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1, 1]) / len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "get_win_percentages_mc(agent1=agent1, agent2=my_4step_lookahead_ab_pruning_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning/discussion) to chat with other learners.*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 899221,
     "sourceId": 17592,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
