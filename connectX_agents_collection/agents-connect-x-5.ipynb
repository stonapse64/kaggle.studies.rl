{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adce1b4",
   "metadata": {
    "papermill": {
     "duration": 0.018033,
     "end_time": "2024-02-03T19:24:58.351554",
     "exception": false,
     "start_time": "2024-02-03T19:24:58.333521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[**Connect 4**](https://www.kaggle.com/competitions/connect-4)\n",
    "\n",
    "[**Connect X**](https://www.kaggle.com/competitions/connectx/leaderboard)\n",
    "\n",
    "\n",
    "[debugger](https://www.kaggle.com/vyacheslavbolotin/debugger-c4)\n",
    "\n",
    "[round_robin](https://www.kaggle.com/code/dott1718/kdb-workshop)\n",
    "\n",
    "[agent_Connect X - treebased](https://www.kaggle.com/code/dott1718/kdb-workshop) - [*dott*](https://www.kaggle.com/dott1718)\n",
    "\n",
    "[agent_Connect_X - MCTS](https://www.kaggle.com/code/matant/monte-carlo-tree-search-connectx) - [*Matan Tsipory*](https://www.kaggle.com/matant)\n",
    "\n",
    "[agent_Connect_X - MCTS w/ Adaptive Playouts](https://www.kaggle.com/code/glazed/monte-carlo-w-adaptive-playouts) - [*glazed*](https://www.kaggle.com/glazed)\n",
    "\n",
    "[agent_Connect_X - Monty Carlo Tree Search](https://www.kaggle.com/code/matant/monte-carlo-tree-search-connectx) - [*James McGuigan*](https://www.kaggle.com/jamesmcguigan)\n",
    "\n",
    "[agent_Connect_X - MCTS Bitboard + Bitsquares Heuristic](https://www.kaggle.com/code/jamesmcguigan/connectx-mcts-bitboard-bitsquares-heuristic) - [*James McGuigan*](https://www.kaggle.com/jamesmcguigan)\n",
    "\n",
    "[agent_Connect_X - The Fast mini Max II](https://www.kaggle.com/code/martynovandrey/the-fast-mini-max-ii) - [The Fast mini Max IV](https://www.kaggle.com/code/martynovandrey/the-fast-mini-max-iv) - [*Martynov Andrey*](https://www.kaggle.com/martynovandrey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9ce10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:24:58.387904Z",
     "iopub.status.busy": "2024-02-03T19:24:58.386955Z",
     "iopub.status.idle": "2024-02-03T19:25:08.605993Z",
     "shell.execute_reply": "2024-02-03T19:25:08.604478Z"
    },
    "papermill": {
     "duration": 10.241097,
     "end_time": "2024-02-03T19:25:08.609211",
     "exception": false,
     "start_time": "2024-02-03T19:24:58.368114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pygame installed, ignoring import\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b56229",
   "metadata": {
    "papermill": {
     "duration": 0.016395,
     "end_time": "2024-02-03T19:25:08.642501",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.626106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### [round_robin](https://www.kaggle.com/code/dott1718/kdb-workshop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa956afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:08.677745Z",
     "iopub.status.busy": "2024-02-03T19:25:08.676774Z",
     "iopub.status.idle": "2024-02-03T19:25:08.768648Z",
     "shell.execute_reply": "2024-02-03T19:25:08.767028Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.1128,
     "end_time": "2024-02-03T19:25:08.771691",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.658891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def create(): \n",
    "    return make(\"connectx\", configuration={\"inarow\":4,\"columns\":7,\"rows\":6})\n",
    "       \n",
    "    \n",
    "def run_game(agent1, agent2, shuffle=False):\n",
    "    env = create()\n",
    "    swap = shuffle and random.choice([True, False])\n",
    "    if swap:\n",
    "        env.run([agent2, agent1])\n",
    "    else:\n",
    "        env.run([agent1, agent2])\n",
    "\n",
    "    if env.steps[-1][0].status == \"TIMEOUT\":\n",
    "        rew1 = 0\n",
    "        rew2 = 1\n",
    "    elif env.steps[-1][1].status == \"TIMEOUT\":\n",
    "        rew2 = 0\n",
    "        rew1 = 1\n",
    "    else:\n",
    "        rew1 = (env.steps[-1][0].reward + 1.0) / 2\n",
    "        rew2 = (env.steps[-1][1].reward + 1.0) / 2\n",
    "\n",
    "    time1 = max(0, env.steps[-1][0].observation.remainingOverageTime)\n",
    "    time2 = max(0, env.steps[-1][1].observation.remainingOverageTime)\n",
    "\n",
    "    if swap:\n",
    "        return rew2, rew1, time2, time1, len(env.steps)\n",
    "    else:\n",
    "        return rew1, rew2, time1, time2, len(env.steps)\n",
    "\n",
    "\n",
    "def run_games(agent1, agent2, n_games=1, n_jobs=-1, shuffle=True):\n",
    "    if n_jobs != 1:\n",
    "        game_res = Parallel(\n",
    "            n_jobs=-1, temp_folder=\"/tmp\", max_nbytes=None, backend=\"loky\"\n",
    "        )(delayed(run_game)(agent1, agent2, shuffle) for _ in range(n_games))\n",
    "    else:\n",
    "        game_res = [run_game(agent1, agent2, shuffle) for _ in range(n_games)]\n",
    "\n",
    "    return [np.mean([x[i] for x in game_res]) for i in range(5)]\n",
    "\n",
    "\n",
    "def round_robin(agents, n_games=1, n_jobs=-1):\n",
    "    res = {\n",
    "        \"name\": [],\n",
    "        \"win_rate\": [],\n",
    "        \"rem_time\": [],\n",
    "    }\n",
    "    for i in range(len(agents)):\n",
    "        for j in range(len(agents)):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            game_res = run_games(agents[i], agents[j], n_games, n_jobs)\n",
    "            res[\"name\"].append(agents[i] if type(agents[i]) == str else agents[i].__name__)\n",
    "            res[\"win_rate\"].append(game_res[0])\n",
    "            res[\"rem_time\"].append(game_res[2])\n",
    "            res[\"name\"].append(agents[j] if type(agents[j]) == str else agents[j].__name__)\n",
    "            res[\"win_rate\"].append(game_res[1])\n",
    "            res[\"rem_time\"].append(game_res[3])\n",
    "    return (\n",
    "        pd.DataFrame(res)\n",
    "        .groupby(\"name\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values([\"win_rate\", \"rem_time\"], ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7f68f",
   "metadata": {
    "papermill": {
     "duration": 0.016462,
     "end_time": "2024-02-03T19:25:08.804703",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.788241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [treebased](https://www.kaggle.com/code/dott1718/kdb-workshop) - [*dott*](https://www.kaggle.com/dott1718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda01708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:08.839930Z",
     "iopub.status.busy": "2024-02-03T19:25:08.839505Z",
     "iopub.status.idle": "2024-02-03T19:25:08.851674Z",
     "shell.execute_reply": "2024-02-03T19:25:08.850097Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.032864,
     "end_time": "2024-02-03T19:25:08.854434",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.821570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agent_treebased_dott.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent_treebased_dott.py\n",
    "\n",
    "def agent__treebased__dott__(observation, configuration, MAX_DEPTH=5, abp=True, shuffle_actions=True):\n",
    "    import numpy as np\n",
    "\n",
    "    MAX_SCORE = 9999\n",
    "    ht = {}\n",
    "\n",
    "    def score_block(d):\n",
    "        if d[3] > 0:\n",
    "            return 0\n",
    "        if d[2] == configuration.inarow:\n",
    "            return -MAX_SCORE\n",
    "        if d[1] == configuration.inarow:\n",
    "            return MAX_SCORE\n",
    "        if (d[1] > 0) & (d[2] > 0):\n",
    "            return 0\n",
    "        if d[1] > 0:\n",
    "            return d[1]\n",
    "        else:\n",
    "            return -d[2]\n",
    "\n",
    "    def _score_seq(seq):\n",
    "        sc = 0\n",
    "        if len(seq) < configuration.inarow:\n",
    "            return 0\n",
    "\n",
    "        cnt = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "        for i in range(configuration.inarow):\n",
    "            cnt[seq[i]] += 1\n",
    "        sc += score_block(cnt)\n",
    "        for i in range(len(seq) - configuration.inarow):\n",
    "            cnt[seq[i]] -= 1\n",
    "            cnt[seq[i + configuration.inarow]] += 1\n",
    "            sc += score_block(cnt)\n",
    "\n",
    "        return sc\n",
    "\n",
    "    def _score_delta(board, i, j):\n",
    "        sc = 0\n",
    "        for seq in [\n",
    "            board[i, :],\n",
    "            board[:, j],\n",
    "            np.diagonal(board, j - i),\n",
    "            np.diagonal(np.flip(board, axis=1), configuration.columns - j - 1 - i),\n",
    "        ]:\n",
    "            if len(seq) >= configuration.inarow:\n",
    "                sc += ht[seq.data.tobytes()]\n",
    "\n",
    "        return sc\n",
    "\n",
    "    def _make_action(board, action, marker):\n",
    "        new_board = board.copy()\n",
    "        ind = np.where(board[:, action] == 0)[0][-1]\n",
    "        new_board[ind, action] = marker\n",
    "        return new_board, ind, action\n",
    "\n",
    "    def _minimax(\n",
    "        board,\n",
    "        current_depth=1,\n",
    "        base_score=0,\n",
    "        last_action=None,\n",
    "        alpha=-np.inf,\n",
    "        beta=np.inf,\n",
    "    ):\n",
    "\n",
    "        if last_action is None:\n",
    "            score = base_score\n",
    "        else:\n",
    "            score = base_score + _score_delta(board, last_action[0], last_action[1])\n",
    "\n",
    "        if score <= -MAX_SCORE + 100:\n",
    "            return score - MAX_DEPTH + current_depth, -1\n",
    "        if score >= MAX_SCORE - 100:\n",
    "            return score + MAX_DEPTH - current_depth, -1\n",
    "        if current_depth == MAX_DEPTH:\n",
    "            return score, -1\n",
    "\n",
    "        marker = 1 if current_depth % 2 == 1 else 2\n",
    "\n",
    "        actions = list(np.where(board[0, :] == 0)[0])\n",
    "        if len(actions) == 0:\n",
    "            return score, -1\n",
    "        if shuffle_actions:\n",
    "            np.random.shuffle(actions)\n",
    "\n",
    "        is_max_turn = marker == 1\n",
    "        best_score = -MAX_SCORE if is_max_turn else MAX_SCORE\n",
    "        action_target = actions[0]\n",
    "\n",
    "        for action in actions:\n",
    "            new_board, a_i, a_j = _make_action(board, action, marker)\n",
    "            child_score, child_action = _minimax(\n",
    "                new_board,\n",
    "                current_depth + 1,\n",
    "                score - _score_delta(board, a_i, a_j),\n",
    "                (a_i, a_j),\n",
    "                alpha,\n",
    "                beta,\n",
    "            )\n",
    "\n",
    "            if is_max_turn and best_score < child_score:\n",
    "                best_score = child_score\n",
    "                action_target = action\n",
    "                if abp:\n",
    "                    alpha = max(alpha, best_score)\n",
    "                    if alpha >= beta:\n",
    "                        break\n",
    "\n",
    "            elif (not is_max_turn) and best_score > child_score:\n",
    "                best_score = child_score\n",
    "                action_target = action\n",
    "                if abp:\n",
    "                    beta = min(beta, best_score)\n",
    "                    if beta <= alpha:\n",
    "                        break\n",
    "\n",
    "        return best_score, action_target\n",
    "\n",
    "    # Pre-calculate hash-table of scores\n",
    "    for l in np.arange(\n",
    "        configuration.inarow, max(configuration.rows, configuration.columns) + 1\n",
    "    ):\n",
    "        for n in range(3**l):\n",
    "            ar = np.zeros(l, dtype=int)\n",
    "            nn = n\n",
    "            for i in range(l):\n",
    "                ar[i] = nn % 3\n",
    "                nn = nn // 3\n",
    "            ht[ar.data.tobytes()] = _score_seq(ar)\n",
    "\n",
    "    # Import board and pre-process it\n",
    "    board = np.array(observation.board, dtype=int).reshape(\n",
    "        configuration.rows, configuration.columns\n",
    "    )\n",
    "    if observation.mark == 2:\n",
    "        board = board * 2\n",
    "        board[board == 4] = 1\n",
    "\n",
    "    actions = (board == 0).sum(axis=0)\n",
    "\n",
    "    # Run minmax\n",
    "    res_score, res_action = _minimax(board)\n",
    "\n",
    "    if res_action < 0:\n",
    "        return np.where(actions > 0)[0][0]\n",
    "\n",
    "    return int(res_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0003c178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:08.891285Z",
     "iopub.status.busy": "2024-02-03T19:25:08.890743Z",
     "iopub.status.idle": "2024-02-03T19:25:08.900472Z",
     "shell.execute_reply": "2024-02-03T19:25:08.898954Z"
    },
    "papermill": {
     "duration": 0.032218,
     "end_time": "2024-02-03T19:25:08.903519",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.871301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run agent_treebased_dott.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfca47e",
   "metadata": {
    "papermill": {
     "duration": 0.016892,
     "end_time": "2024-02-03T19:25:08.937420",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.920528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [MCTS](https://www.kaggle.com/code/matant/monte-carlo-tree-search-connectx) - [*Matan Tsipory*](https://www.kaggle.com/matant)\n",
    "\n",
    "This is an agent that plays with UCT-MCTS. Code is highly commented to be readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5173a911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:08.973751Z",
     "iopub.status.busy": "2024-02-03T19:25:08.973228Z",
     "iopub.status.idle": "2024-02-03T19:25:08.990405Z",
     "shell.execute_reply": "2024-02-03T19:25:08.988702Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.038729,
     "end_time": "2024-02-03T19:25:08.993035",
     "exception": false,
     "start_time": "2024-02-03T19:25:08.954306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agent_MCTS_MatanTsipory.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent_MCTS_MatanTsipory.py\n",
    "\n",
    "def agent_MCTS_MatanTsipory_(observation, configuration):\n",
    "    \"\"\"\n",
    "    Connect X agent based on MCTS.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import math\n",
    "    import time\n",
    "    global current_state  # so tree can be recycled\n",
    "\n",
    "    init_time = time.time()\n",
    "    EMPTY = 0\n",
    "    T_max = 1.0\n",
    "    # T_max = configuration.timeout - 0.34  # time per move, left some overhead\n",
    "    Cp_default = 1\n",
    "\n",
    "    def play(board, column, mark, config):\n",
    "        \"\"\" Plays a move. Taken from the Kaggle environment. \"\"\"\n",
    "        columns = config.columns\n",
    "        rows = config.rows\n",
    "        row = max([r for r in range(rows) if board[column + (r * columns)] == EMPTY])\n",
    "        board[column + (row * columns)] = mark\n",
    "\n",
    "    def is_win(board, column, mark, config):\n",
    "        \"\"\" Checks for a win. Taken from the Kaggle environment. \"\"\"\n",
    "        columns = config.columns\n",
    "        rows = config.rows\n",
    "        inarow = config.inarow - 1\n",
    "        row = min([r for r in range(rows) if board[column + (r * columns)] == mark])\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, inarow + 1):\n",
    "                r = row + offset_row * i\n",
    "                c = column + offset_column * i\n",
    "                if (\n",
    "                        r < 0\n",
    "                        or r >= rows\n",
    "                        or c < 0\n",
    "                        or c >= columns\n",
    "                        or board[c + (r * columns)] != mark\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return inarow\n",
    "\n",
    "        return (\n",
    "                count(1, 0) >= inarow  # vertical.\n",
    "                or (count(0, 1) + count(0, -1)) >= inarow  # horizontal.\n",
    "                or (count(-1, -1) + count(1, 1)) >= inarow  # top left diagonal.\n",
    "                or (count(-1, 1) + count(1, -1)) >= inarow  # top right diagonal.\n",
    "        )\n",
    "\n",
    "    def is_tie(board):\n",
    "        \"\"\" Checks if a tie occured. \"\"\"\n",
    "        return not(any(mark == EMPTY for mark in board))\n",
    "\n",
    "    def check_finish_and_score(board, column, mark, config):\n",
    "        \"\"\" Returns a tuple where the first argument states whether game is finished and second argument returns score if game has finished. \"\"\"\n",
    "        if is_win(board, column, mark, config):\n",
    "            return (True, 1)\n",
    "        if is_tie(board):\n",
    "            return (True, 0.5)\n",
    "        else:\n",
    "            return (False, None)\n",
    "\n",
    "    def uct_score(node_total_score, node_total_visits, parent_total_visits, Cp=Cp_default):\n",
    "        \"\"\" UCB1 calculation. \"\"\"\n",
    "        if node_total_visits == 0:\n",
    "            return math.inf\n",
    "        return node_total_score / node_total_visits + Cp * math.sqrt(\n",
    "            2 * math.log(parent_total_visits) / node_total_visits)\n",
    "\n",
    "    def opponent_mark(mark):\n",
    "        \"\"\" The mark indicates which player is active - player 1 or player 2. \"\"\"\n",
    "        return 3 - mark\n",
    "\n",
    "    def opponent_score(score):\n",
    "        \"\"\" To backpropagate scores on the tree. \"\"\"\n",
    "        return 1 - score\n",
    "\n",
    "    def random_action(board, config):\n",
    "        \"\"\" Returns a random legal action (from the open columns). \"\"\"\n",
    "        return random.choice([c for c in range(config.columns) if board[c] == EMPTY])\n",
    "\n",
    "    def default_policy_simulation(board, mark, config):\n",
    "        \"\"\"\n",
    "        Run a random play simulation. Starting state is assumed to be a non-terminal state.\n",
    "        Returns score of the game for the player with the given mark.\n",
    "        \"\"\"\n",
    "        original_mark = mark\n",
    "        board = board.copy()\n",
    "        column = random_action(board, config)\n",
    "        play(board, column, mark, config)\n",
    "        is_finish, score = check_finish_and_score(board, column, mark, config)\n",
    "        while not is_finish:\n",
    "            mark = opponent_mark(mark)\n",
    "            column = random_action(board, config)\n",
    "            play(board, column, mark, config)\n",
    "            is_finish, score = check_finish_and_score(board, column, mark, config)\n",
    "        if mark == original_mark:\n",
    "            return score\n",
    "        return opponent_score(score)\n",
    "    \n",
    "    def find_action_taken_by_opponent(new_board, old_board, config):\n",
    "        \"\"\" Given a new board state and a previous one, finds which move was taken. Used for recycling tree between moves. \"\"\"\n",
    "        for i, piece in enumerate(new_board):\n",
    "            if piece != old_board[i]:\n",
    "                return i % config.columns\n",
    "        return -1  # shouldn't get here\n",
    "\n",
    "    class State():\n",
    "        \"\"\" \n",
    "        A class that represents nodes in the game tree.\n",
    "        \n",
    "        \"\"\"\n",
    "        def __init__(self, board, mark, config, parent=None, is_terminal=False, terminal_score=None, action_taken=None):\n",
    "            self.board = board.copy()\n",
    "            self.mark = mark\n",
    "            self.config = config\n",
    "            self.children = []\n",
    "            self.parent = parent\n",
    "            self.node_total_score = 0\n",
    "            self.node_total_visits = 0\n",
    "            self.available_moves = [c for c in range(config.columns) if board[c] == EMPTY]\n",
    "            self.expandable_moves = self.available_moves.copy()\n",
    "            self.is_terminal = is_terminal\n",
    "            self.terminal_score = terminal_score\n",
    "            self.action_taken = action_taken\n",
    "\n",
    "        def is_expandable(self):\n",
    "            \"\"\" Checks if the node has unexplored children. \"\"\"\n",
    "            return (not self.is_terminal) and (len(self.expandable_moves) > 0)\n",
    "\n",
    "        def expand_and_simulate_child(self):\n",
    "            \"\"\" Expands a random move from the legal unexplored moves, and runs a simulation of it \n",
    "            (Expansion + Simulation + Backpropagation stages in the MCTS algorithm description). \"\"\"\n",
    "            column = random.choice(self.expandable_moves)\n",
    "            child_board = self.board.copy()\n",
    "            play(child_board, column, self.mark, self.config)\n",
    "            is_terminal, terminal_score = check_finish_and_score(child_board, column, self.mark, self.config)\n",
    "            self.children.append(State(child_board, opponent_mark(self.mark),\n",
    "                                       self.config, parent=self,\n",
    "                                       is_terminal=is_terminal,\n",
    "                                       terminal_score=terminal_score,\n",
    "                                       action_taken=column\n",
    "                                       ))\n",
    "            simulation_score = self.children[-1].simulate()\n",
    "            self.children[-1].backpropagate(simulation_score)\n",
    "            self.expandable_moves.remove(column)\n",
    "\n",
    "        def choose_strongest_child(self, Cp):\n",
    "            \"\"\"\n",
    "            Chooses child that maximizes UCB1 score (Selection stage in the MCTS algorithm description).\n",
    "            \"\"\"\n",
    "            children_scores = [uct_score(child.node_total_score,\n",
    "                                         child.node_total_visits,\n",
    "                                         self.node_total_visits,\n",
    "                                         Cp) for child in self.children]\n",
    "            max_score = max(children_scores)\n",
    "            best_child_index = children_scores.index(max_score)\n",
    "            return self.children[best_child_index]\n",
    "            \n",
    "        def choose_play_child(self):\n",
    "            \"\"\" Choose child with maximum total score.\"\"\"\n",
    "            children_scores = [child.node_total_score for child in self.children]\n",
    "            max_score = max(children_scores)\n",
    "            best_child_index = children_scores.index(max_score)\n",
    "            return self.children[best_child_index]\n",
    "\n",
    "        def tree_single_run(self):\n",
    "            \"\"\"\n",
    "            A single iteration of the 4 stages of the MCTS algorithm.\n",
    "            \"\"\"\n",
    "            if self.is_terminal:\n",
    "                self.backpropagate(self.terminal_score)\n",
    "                return\n",
    "            if self.is_expandable():\n",
    "                self.expand_and_simulate_child()\n",
    "                return\n",
    "            self.choose_strongest_child(Cp_default).tree_single_run()\n",
    "\n",
    "        def simulate(self):\n",
    "            \"\"\"\n",
    "            Runs a simulation from the current state. \n",
    "            This method is used to simulate a game after move of current player, so if a terminal state was reached,\n",
    "            the score would belong to the current player who made the move.\n",
    "            But otherwise the score received from the simulation run is the opponent's score and thus needs to be flipped with the function opponent_score().            \n",
    "            \"\"\"\n",
    "            if self.is_terminal:\n",
    "                return self.terminal_score\n",
    "            return opponent_score(default_policy_simulation(self.board, self.mark, self.config))\n",
    "\n",
    "        def backpropagate(self, simulation_score):\n",
    "            \"\"\"\n",
    "            Backpropagates score and visit count to parents.\n",
    "            \"\"\"\n",
    "            self.node_total_score += simulation_score\n",
    "            self.node_total_visits += 1\n",
    "            if self.parent is not None:\n",
    "                self.parent.backpropagate(opponent_score(simulation_score))\n",
    "                \n",
    "        def choose_child_via_action(self, action):\n",
    "            \"\"\" Choose child given the action taken from the state. Used for recycling of tree. \"\"\"\n",
    "            for child in self.children:\n",
    "                if child.action_taken == action:\n",
    "                    return child\n",
    "            return None\n",
    "\n",
    "    board = observation.board\n",
    "    mark = observation.mark\n",
    "    \n",
    "    # If current_state already exists, recycle it based on action taken by opponent\n",
    "    try:  \n",
    "        current_state = current_state.choose_child_via_action(\n",
    "            find_action_taken_by_opponent(board, current_state.board, configuration))\n",
    "        current_state.parent = None  # make current_state the root node, dereference parents and siblings\n",
    "        \n",
    "    except:  # new game or other error in recycling attempt due to Kaggle mechanism\n",
    "        current_state = State(board, mark,  # This state is considered after the opponent's move\n",
    "                              configuration, parent=None, is_terminal=False, terminal_score=None, action_taken=None)\n",
    "   \n",
    "    # Run MCTS iterations until time limit is reached.\n",
    "    while time.time() - init_time <= T_max:\n",
    "        current_state.tree_single_run()\n",
    "        \n",
    "    current_state = current_state.choose_play_child()\n",
    "    return current_state.action_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd129b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:09.029448Z",
     "iopub.status.busy": "2024-02-03T19:25:09.028608Z",
     "iopub.status.idle": "2024-02-03T19:25:09.038105Z",
     "shell.execute_reply": "2024-02-03T19:25:09.036535Z"
    },
    "papermill": {
     "duration": 0.030517,
     "end_time": "2024-02-03T19:25:09.040902",
     "exception": false,
     "start_time": "2024-02-03T19:25:09.010385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run agent_MCTS_MatanTsipory.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b388a3",
   "metadata": {
    "papermill": {
     "duration": 0.017034,
     "end_time": "2024-02-03T19:25:09.075426",
     "exception": false,
     "start_time": "2024-02-03T19:25:09.058392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [MCTS w/ Adaptive Playouts](https://www.kaggle.com/code/glazed/monte-carlo-w-adaptive-playouts) - [*glazed*](https://www.kaggle.com/glazed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7883423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:09.114149Z",
     "iopub.status.busy": "2024-02-03T19:25:09.113076Z",
     "iopub.status.idle": "2024-02-03T19:25:09.142801Z",
     "shell.execute_reply": "2024-02-03T19:25:09.141021Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.053084,
     "end_time": "2024-02-03T19:25:09.145783",
     "exception": false,
     "start_time": "2024-02-03T19:25:09.092699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MCTS_w_Adaptive_Playouts_glazed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MCTS_w_Adaptive_Playouts_glazed.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from math import sqrt\n",
    "from numba import njit\n",
    "from numba import prange\n",
    "\n",
    "# The following several functions are run with the Numba JIT compiler\n",
    "# resulting in a dramatic speed increase.\n",
    "\n",
    "@njit() # Numba function\n",
    "def win_for_tup_jit(b, tup, I, color):\n",
    "    # b[] holds the board state.\n",
    "    # tup[] holds 4 board positions that are in a row.\n",
    "    # I is one of those positions.\n",
    "    # Would placing a color piece on I, result in 4 in a row?\n",
    "    for x in tup:\n",
    "        if (not ((b[x] == color) or (b[x] == 0))):\n",
    "            return False\n",
    "        if (not ((b[x] == color) or (x == I))):\n",
    "            return False\n",
    "    return True  \n",
    "\n",
    "@njit()\n",
    "def winning_move_jit(b, tupCounts, spaceTups, x_in, color_in):\n",
    "    # Would placing a color_in piece on I, result in 4 in a row?\n",
    "    # b[] holds the board state.\n",
    "    # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n",
    "    # different groups of 4 positions in a row.\n",
    "    tup = np.zeros(4, np.int32)\n",
    "    x = int(x_in)\n",
    "    color = int(color_in)\n",
    "    for n in range(tupCounts[x]):# for each tup\n",
    "        for z in range(4):\n",
    "            tup[z] = spaceTups [x, n, z]\n",
    "        if (win_for_tup_jit(b, tup, x, color)):\n",
    "            return(True)\n",
    "    return False   \n",
    "\n",
    "@njit()\n",
    "def lowest_empty_row_jit(b, j):\n",
    "    # Return the row for a stone placed in column j.\n",
    "    # (My coordinates are upside down wrt the contest. )\n",
    "    r = 6\n",
    "    c = 7\n",
    "    for i in range(r): # rows\n",
    "        x = (i * c) + j\n",
    "        if (b[x] == 0):\n",
    "            return i\n",
    "        # Paronoid check to avoid race conditions if Numba makes this parallel.\n",
    "        #if ((b[x] == 0) and (i == 0)):\n",
    "        #    return i\n",
    "        #if ((b[x] == 0) and (i > 0) and (b[x - c] != 0)):\n",
    "        #    return i\n",
    "    return(r)\n",
    "\n",
    "@njit()\n",
    "def get_sensible_moves_jit(b, tupCounts, spaceTups, color, otherColor):\n",
    "    # Return a list of moves worth considering, plus the status of the board.\n",
    "    # Is there a winning or forced move? Return a list of length one.\n",
    "    # Also return a status flag.\n",
    "    # status 0 : tie game, no legal moves\n",
    "    # status 1 : ongoing game\n",
    "    # status 2 : Color will win by playing the (single) move in obviousMoves[]\n",
    "    r = 6 # number of rows\n",
    "    c = 7 # number of columns\n",
    "    N = 42 # number of board spaces\n",
    "    legalMoves = [np.int32(x) for x in range(0)] # weird hack so numba can guess what type is in the list\n",
    "    for j in range(c):\n",
    "        i = lowest_empty_row_jit(b, j)\n",
    "        if (i < r): # legal move\n",
    "            x = (i * c) + j\n",
    "            if (winning_move_jit(b, tupCounts, spaceTups, x, color)):\n",
    "                #print(\"win\")\n",
    "                obviousMoves = [x]\n",
    "                return (obviousMoves, 2)\n",
    "            legalMoves.append(x)\n",
    "    \n",
    "    if (len(legalMoves) == 0):\n",
    "        return (legalMoves, 0)  # tie game\n",
    "    \n",
    "    if (len(legalMoves) == 1):\n",
    "        return (legalMoves, 1)  # ongoing game\n",
    "    \n",
    "    for x in legalMoves:\n",
    "        if (winning_move_jit(b, tupCounts, spaceTups, x, otherColor)):\n",
    "            #print(\"forced\")\n",
    "            obviousMoves = [x]\n",
    "            return (obviousMoves, 1)\n",
    "    \n",
    "    lm = legalMoves.copy()\n",
    "    for x in lm:\n",
    "        if (x + c < N):\n",
    "            b[x] = color # temporarily place stone here\n",
    "            if ((len(legalMoves) > 1) and (winning_move_jit(b, tupCounts, spaceTups, x + c, otherColor))):\n",
    "                legalMoves.remove(x)\n",
    "            b[x] = 0 # remove temporarily stone\n",
    "    return (legalMoves, 1)  # no obvious move\n",
    "\n",
    "\n",
    "@njit()\n",
    "def friendly_tupple_jit(b, tup, x, color):\n",
    "    # tup[] holds 4 board positions that are in a row.\n",
    "    # x is one of those positions.\n",
    "    # If there are no othercolor pieces in tup, how many color pieces are there?\n",
    "    count = np.int32(0)\n",
    "    nope = False\n",
    "    if (b[x] != 0):\n",
    "        return (-1)\n",
    "    for i in range(4):\n",
    "        z = tup[i]\n",
    "        if ((b[z] != color) and (b[z] != 0)):\n",
    "            nope = True\n",
    "        if (b[z] == color):\n",
    "            count += 1\n",
    "    if (nope): return(-1)\n",
    "    return count\n",
    "\n",
    "@njit()\n",
    "def calcMoveFeatures_jit(b, tupCounts, spaceTups, x, color):\n",
    "    # calculate features for a move at position x and return in feat[] \n",
    "    # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n",
    "    # different groups of 4 positions in a row.\n",
    "    c = 7\n",
    "    r = 6\n",
    "    feat = np.zeros(22, np.int32)\n",
    "    tup = np.zeros(4, np.int32)\n",
    "    #self.feat.fill(0)\n",
    "    otherColor = 2\n",
    "    if (color == 2):\n",
    "        otherColor = 1\n",
    "    i = x // c\n",
    "    j =  x % c\n",
    "    #feat = feat * 0  # clear feat[]\n",
    "    feat[0] = min(j, (c - 1) - j)  # distance from edge\n",
    "    tups = tupCounts[x]\n",
    "    for n in range(tups):\n",
    "        #tup = self.spaceTups [x, n, :]\n",
    "        for z in range(4): \n",
    "            tup[z] = spaceTups [x, n, z]\n",
    "        count = friendly_tupple_jit(b, tup, x, color)\n",
    "        if (count > -1):\n",
    "            feat[count + 1] += 1\n",
    "        count = friendly_tupple_jit(b, tup, x, otherColor)\n",
    "        if (count > 0):\n",
    "            feat[count + 4] += 1\n",
    "            \n",
    "    if (i >= r - 1):\n",
    "        return feat # we're on the top row, so leave all other features at zero\n",
    "    I = i + 1 # looking at space above x\n",
    "    b[x] = color  # temporarily put friendly stone on space x\n",
    "    xp = (I * c) + j\n",
    "    tups = tupCounts[xp]\n",
    "    for n in range(tups):\n",
    "        #tup = self.spaceTups [xp, n, :]\n",
    "        for z in range(4):\n",
    "            tup[z] = spaceTups [xp, n, z]\n",
    "        count = friendly_tupple_jit(b, tup, xp, color)\n",
    "        if (count > -1):\n",
    "            feat[count + 8] += 1\n",
    "        count = friendly_tupple_jit(b, tup, xp, otherColor)\n",
    "        if (count > 0):\n",
    "            feat[count + 11] += 1\n",
    "        \n",
    "    b[x] = 0  # remove friendly stone from space x\n",
    "    \n",
    "    if (i >= r - 2):\n",
    "        return feat # we're on the next-to top row, so leave all other features at zero \n",
    "    I = i + 2 # looking at space above x\n",
    "    b[x] = color  # temporarily put friendly stone on space x\n",
    "    b[xp] = otherColor  # temporarily put enemy stone on space xp\n",
    "    xpp = (I * c) + j\n",
    "    tups = tupCounts[xpp]\n",
    "    for n in range(tups):\n",
    "        #tup = self.spaceTups [xpp, n, :]\n",
    "        for z in range(4):\n",
    "            tup[z] = spaceTups [xpp, n, z]\n",
    "        count = friendly_tupple_jit(b, tup, xpp, color)\n",
    "        if (count > -1):\n",
    "            feat[count + 15] += 1\n",
    "        count = friendly_tupple_jit(b, tup, xpp, otherColor)\n",
    "        if (count > 0):\n",
    "            feat[count + 18] += 1\n",
    "    \n",
    "    b[x] = 0  # remove friendly stone from space x\n",
    "    b[xp] = 0  # remove enemy stone from space xp\n",
    "    return feat\n",
    "    \n",
    "@njit()          \n",
    "def calc_meta_features_jit(feat, x):\n",
    "    #calculate meta-features for a move at position x and return in metaFeat[]\n",
    "    \n",
    "    # all binary features\n",
    "    #metaFeat = metaFeat * 0  # clear metaFeat[]\n",
    "    metaFeat = np.zeros((4 + 6 + (21 * 3)), np.int32)\n",
    "    #self.metaFeat .fill(0)\n",
    "    c = 7\n",
    "    i = x // c\n",
    "    n = 0\n",
    "    y = feat [0] # distance from edge -> 4 possibilities, 4 'binary' variables\n",
    "    metaFeat [y] = 1 # only 1 can be non-zero\n",
    "    n += 4\n",
    "    # row -> 6 (essentially boolean) parameters\n",
    "    metaFeat [n + i] = 1 # only 1 can be non-zero\n",
    "    n += 6\n",
    "    for f in range(1, len(feat)):\n",
    "        if (feat[f] == 0):\n",
    "            metaFeat[n] = 1\n",
    "        elif (feat[f] == 1):\n",
    "            metaFeat[n + 1] = 1\n",
    "        elif (feat[f] > 1):\n",
    "            metaFeat[n + 2] = 1\n",
    "        n += 3\n",
    "    return metaFeat\n",
    "\n",
    "@njit()\n",
    "def linear_move_scores_jit(b, tupCounts, spaceTups, wts, moves, color):\n",
    "    # For every move in the list, calculate a score and return them in scores[]\n",
    "    # moves[] holds the list of moves.\n",
    "    # b[] holds the board state.\n",
    "    # wts[] holds the linear weights for the features.\n",
    "    # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n",
    "    # different groups of 4 positions in a row.\n",
    "    #min_score = 0.05\n",
    "    scores = [np.float64(x) for x in range(0)] # weird hack so numba can guess what type belongs in the list\n",
    "    total = 0.0\n",
    "    for i in prange(len(moves)):\n",
    "        x = moves[i]\n",
    "        feat = calcMoveFeatures_jit(b, tupCounts, spaceTups, x, color) # fills feat\n",
    "        metaFeat = calc_meta_features_jit(feat, x)         # fills metafeat\n",
    "        score = np.sum (metaFeat * wts) \n",
    "        scores.append(score)\n",
    "        total += score\n",
    "    #scores = scores / np.sum(scores)\n",
    "    for i in range(len(scores)):\n",
    "        scores[i] /= total\n",
    "    #for sc in scores:\n",
    "    #    if (sc < min_score): sc = min_score\n",
    "    return scores\n",
    "\n",
    "\n",
    "@njit()  \n",
    "def choose_linear_move_jit(b, tupCounts, spaceTups, wts, color):\n",
    "    # Get the sensible_moves[], score them, return the move with the highest score.\n",
    "    # score is a weighted sum of features.\n",
    "    #print(\"choose_linear_move_jit start\")\n",
    "    otherColor = 2\n",
    "    if (color == 2):\n",
    "        otherColor = 1\n",
    "    sensible_moves, status = get_sensible_moves_jit(b, tupCounts, spaceTups, color, otherColor) # also sets self.status\n",
    "    #print(\"choose_linear_move_jit calculated\")\n",
    "    if (len(sensible_moves) == 1):   # If it's a win, there will only be one move, so it returns w/ correct status.\n",
    "        return(sensible_moves[0], status)\n",
    "    if (len(sensible_moves) == 0):\n",
    "        return(-1, 0)   # tie  \n",
    "    scores = linear_move_scores_jit(b, tupCounts, spaceTups, wts, np.array(sensible_moves), color)\n",
    "    \n",
    "    #print(\"choose_linear_move_jit calculated\")\n",
    "    x = sensible_moves[int(np.argmax(np.array(scores)))]\n",
    "    #print(sensible_moves)\n",
    "    #print(scores)\n",
    "    #print(x)\n",
    "    return (x, 1)\n",
    "\n",
    "\n",
    "class GAME_MANAGER():\n",
    "    \n",
    "    # This class holds the board, the weights for move features and some lookup tables.\n",
    "    # It has a method that calculates the tables.\n",
    "    # Note: \"tupples\" doesn't mean Python tupples.\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.c = 7\n",
    "        self.r = 6\n",
    "        self.N = self.c * self.r\n",
    "        self.K = 4\n",
    "        self.b = np.zeros((self.N), np.int32)\n",
    "        self.tupCounts = np.zeros((self.N), np.int32) # how many tupples contain this space\n",
    "        self.spaceTups = np.zeros((self.N, 16, self.K), np.int32)# all the tupples \n",
    "        self.feat = np.zeros(22, np.int32)\n",
    "        self.metaFeat = np.zeros((4 + 6 + (21 * 3)), np.int32)\n",
    "        self.precalcTups()  # calculates tupCounts[] and spaceTups[,,]\n",
    "        self.wts = np.array([0, 3.30366111407672, 6.48699261816764, 16.0570530870234, 0, 6.89154554317436, 12.2291091073301, 14.9610249625214, 8.2764687731129, 5.36422024027117, 14.3516002179586, 2.1021183007845, 0, 0.719857647774551, 2.34487198327812, 11.2057343305379, 0, 27.8825518166704, 43.8228479471015, 0, 1.99103479234425, 0.726840470816211, 0.560352057248385, 4.22057034862125, 6.83487190573809, 0, 17.491336765736, 35.714660516821, 0, 5.87818176004432, 39.6996173910764, 0, 3.6234074451734, 4.49879127350228, 0.806205543215103, 1.71224017973768, 3.8749091066669, 2.53500463742859, 4.460882129649, 1.37971933373037, 44.3252027404174, 29.7283585631117, 0, 0, 2.19363385696125, 1.47116517810174, 11.3140778960712, 2.27101838697622, 0, 1.74611338807736, 6.51927915172379, 0, 2.94225634035759, 0, 3.50453812682987, 0, 0.267382395928564, 1.98693848260618, 0, 1.58478060332119, 4.71565983853622, 0, 17.8551290536231, 7.25762646495066, 5.7077735522044, 1.10042450222865, 0, 8.22587152564711, 4.70559492215376, 0.324579480451124, 16.3108479710915, 8.11547320076288, 0, ]) \n",
    "           \n",
    "    def precalcTups(self):\n",
    "        # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n",
    "        # different groups of 4 positions in a row.\n",
    "        #tupCounts *= 0\n",
    "        # horizontal\n",
    "        tup = np.zeros((self.K), np.int32)\n",
    "        for j in range (0, (self.c - self.K) + 1):\n",
    "            for i in range(self.r):\n",
    "                # fill tup[]\n",
    "                for h in range(self.K):\n",
    "                    tup[h] = (i * self.c) + j + h\n",
    "                for h in range(self.K):\n",
    "                    x = tup [h]\n",
    "                    n = self.tupCounts [x]\n",
    "                    for z in range(self.K):\n",
    "                        self.spaceTups [x, n, z] = tup [z]\n",
    "                    self.tupCounts [x] = n + 1\n",
    "        # vertical\n",
    "        for j in range (self.c):\n",
    "            for i in range(0, (self.r - self.K) + 1):\n",
    "                # fill tup[]\n",
    "                for h in range(self.K):\n",
    "                    tup[h] = ((i + h) * self.c) + j\n",
    "                for h in range(self.K):\n",
    "                    x = tup [h]\n",
    "                    n = self.tupCounts [x]\n",
    "                    for z in range(self.K):\n",
    "                        self.spaceTups [x, n, z] = tup [z]\n",
    "                    self.tupCounts [x] = n + 1\n",
    "        # diagonal up, up\n",
    "        for j in range (0, (self.c - self.K) + 1):\n",
    "            for i in range(0, (self.r - self.K) + 1):\n",
    "                # fill tup[]\n",
    "                for h in range(self.K):\n",
    "                    tup[h] = ((i + h) * self.c) + j + h\n",
    "                for h in range(self.K):\n",
    "                    x = tup [h]\n",
    "                    n = self.tupCounts [x]\n",
    "                    for z in range(self.K):\n",
    "                        self.spaceTups [x, n, z] = tup [z]\n",
    "                    self.tupCounts [x] = n + 1\n",
    "        # diagonal something, something...\n",
    "        for j in range (0, (self.c - self.K) + 1):\n",
    "            for i in range(self.r-1, self.r - self.K, -1):\n",
    "                # fill tup[]\n",
    "                for h in range(self.K):\n",
    "                    tup[h] = ((i - h) * self.c) + j + h\n",
    "                for h in range(self.K):\n",
    "                    x = tup [h]\n",
    "                    n = self.tupCounts [x]\n",
    "                    for z in range(self.K):\n",
    "                        self.spaceTups [x, n, z] = tup [z]\n",
    "                    self.tupCounts [x] = n + 1\n",
    "        #print(\"precalcTups\",self.tupCounts)\n",
    "        return\n",
    "     \n",
    "    #@njit()       \n",
    "    def reset_b(self, B):\n",
    "        for i in range(len(B)):\n",
    "            self.b[i] = B[i]\n",
    "\n",
    "\n",
    "# This class uses all moves as first (AMAF) which is similar to RAVE in MCTS\n",
    "# There are 42 positions on the board. \n",
    "# raveS[] holds the score for each position.\n",
    "# raveV[] holds the number of visits for each position.\n",
    "# This class holds methods for UCT-like move selection using RAVE plus policy scores.\n",
    "class AMAF():\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.raveS = np.zeros (N, int)  # myColor AMAF\n",
    "        self.raveV = np.zeros (N, int)\n",
    "        \n",
    "    def clearit(self):\n",
    "        self.raveS.fill(0)\n",
    "        self.raveV.fill(0)\n",
    "   \n",
    "    def amaf_scores(self, moves):\n",
    "        #return amaf scores for moves \n",
    "        scores = []\n",
    "        for x in moves:\n",
    "            scores.append( self.raveS[x] / max(1, self.raveV[x]) )\n",
    "        return(scores)\n",
    "    \n",
    "    def PUCT_scores(self, Cpuct, moves, scores):\n",
    "        if (len(moves) == 0) : print(\"PUCT_scores BADNESS! length 0\")\n",
    "        if (len(scores) == 0) : print(\"scores PUCT_scores BADNESS! length 0\")\n",
    "        encounters = 0\n",
    "        pscores = scores.copy()\n",
    "        for x in moves:\n",
    "            encounters += self.raveV[x]\n",
    "        if (encounters == 0):\n",
    "            return (pscores)\n",
    "        for k in range(len(moves)):\n",
    "            x = moves[k]\n",
    "            u = self.raveS[x] / max(1, self.raveV[x])\n",
    "            u += scores[k] * Cpuct * sqrt(encounters) / (self.raveV[x] + 1)\n",
    "            pscores[k] = u\n",
    "        return(pscores)\n",
    "    \n",
    "    def PUCT_no_policy(self, Cpuct, moves):\n",
    "        # Returns a move using UCT-like algorithm.\n",
    "        # Used when no policy scores are avaulable.\n",
    "        # moves[] holds the moves to be considered.\n",
    "        if (len(moves) == 0) : print(\"PUCT_no_policy BADNESS! length 0\")\n",
    "        encounters = 0\n",
    "        best_score = 0\n",
    "        best = moves[0]\n",
    "        for x in moves:\n",
    "            encounters += self.raveV[x]\n",
    "        if (encounters == 0):\n",
    "            return (choice(moves))\n",
    "        for x in moves:\n",
    "            u = self.raveS[x] / max(1, self.raveV[x])\n",
    "            if (u == 2):\n",
    "                return(x)  # perfect score, so why explore\n",
    "            u += (1.0 /len(moves) ) * Cpuct * sqrt(encounters) / (self.raveV[x] + 1)\n",
    "            if (u >= best_score):\n",
    "                best_score = u\n",
    "                best = x\n",
    "        #print(\"amaf_puct\", x)\n",
    "        return(best)\n",
    "    \n",
    "    def PUCT(self, Cpuct, moves, scores):\n",
    "        # Returns a move using an UCT-like algorithm.\n",
    "        # scores[] holds the policy scores.\n",
    "        # moves[] holds the moves to be considered.\n",
    "        if (len(moves) == 0) : print(\"PUCT BADNESS! length 0\")\n",
    "        if (len(scores) == 0) : print(\"scores PUCT BADNESS! length 0\")\n",
    "        encounters = 0\n",
    "        best_score = 0\n",
    "        best = moves[0]\n",
    "        for x in moves:\n",
    "            encounters += self.raveV[x]\n",
    "        if (encounters == 0):\n",
    "            return (choice(moves))\n",
    "        for k in range(len(moves)):\n",
    "            x = moves[k]\n",
    "            u = self.raveS[x] / max(1, self.raveV[x])\n",
    "            if (u == 2):\n",
    "                return(x)  # perfect score, so why explore\n",
    "            u += scores[k] * Cpuct * sqrt(encounters) / (self.raveV[x] + 1)\n",
    "            if (u >= best_score):\n",
    "                best_score = u\n",
    "                best = x\n",
    "        #print(\"amaf_puct\", x)\n",
    "        return(best)\n",
    "    \n",
    "    \n",
    "    def reinforce(self, moves, reward):\n",
    "        # moves holds a list of all the moves (for this color) in one playout.\n",
    "        for k in moves:\n",
    "            self.raveV[k] += 1  \n",
    "            self.raveS[k] += reward\n",
    "\n",
    "\n",
    "class BRAIN():\n",
    "    # This class chooses a move using MonteCarlo with adaptive playouts.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gm = GAME_MANAGER() # holds the board and some tables\n",
    "        self.colorAMAF = AMAF(self.gm.N) # AMAF object for color moves in playouts\n",
    "        self.otherAMAF = AMAF(self.gm.N) # AMAF object for otherColor moves in playouts\n",
    "        \n",
    "     \n",
    "    def brain_linear_move(self, b, color, V = False):\n",
    "        # Pick a move just using heuristics. (Not called)\n",
    "        ts = time.time()\n",
    "        self.gm.reset_b(b)\n",
    "        otherColor = 1 + (2 - color)\n",
    "        moves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, color, otherColor)\n",
    "        linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(moves), otherColor) \n",
    "       \n",
    "        x, status = choose_linear_move_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, color)\n",
    "        if (V): print(\" JIT tm\",time.time() - ts)   \n",
    "   \n",
    "        return(x, status)\n",
    "    \n",
    "    \n",
    "    def clearAMAF(self):\n",
    "        self.colorAMAF.clearit()\n",
    "        self.otherAMAF.clearit()\n",
    "        \n",
    "            \n",
    "    def MC_adaptive_move(self, root, color, start, time_limit, POLICY):\n",
    "        # Choose a move using Monte Carlo playouts.\n",
    "        # First check that there's not only one real choice.\n",
    "        # Use all time available.\n",
    "        # Use adaptive playouts.\n",
    "        # Play move with most visits at the root\n",
    "        # Only reinforce discretionary moves.\n",
    "        # if (POLICY): calculate and use heuristic move scores ('priors') in playouts\n",
    "        \n",
    "        self.clearAMAF()\n",
    "        showit = True\n",
    "        \n",
    "        CpuctRoot = 7.0 # exploration term for root\n",
    "        Cpuct = 4.0     # exploration term for all other nodes\n",
    "        otherColor = 2\n",
    "        if (color == 2):\n",
    "            otherColor = 1\n",
    "        \n",
    "        self.gm.b = root.copy()   # Set b in game_manager (This is the root board state)\n",
    "        # Get sensible moves \n",
    "        legalMoves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, color, otherColor)\n",
    "        # If the move is obvious, we return it along with the status\n",
    "        if (status == 2):\n",
    "            print(\"win\")\n",
    "        #    return(legalMoves[0], status)\n",
    "        if (len(legalMoves) == 1):\n",
    "            print(\"forced move\")\n",
    "            return(legalMoves[0], status)\n",
    "        if (len(legalMoves) == 0):\n",
    "            print(\"tie\")\n",
    "            return(-1, 0)   # tie\n",
    "        \n",
    "        # There must be 2 or more sensible moves to choose from\n",
    "        use_policy = POLICY\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # run MonteCarlo playouts for all 'sensible' moves\n",
    "        # \n",
    "        \n",
    "        rootMoves = legalMoves.copy()\n",
    "      \n",
    "        training_visits = np.zeros(7)\n",
    "        root_visits = np.zeros(len(rootMoves))\n",
    "        root_rewards = 0.0\n",
    "        if (use_policy):\n",
    "            root_scores = linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(rootMoves), color) \n",
    "        else:\n",
    "            root_scores = np.ones(len(rootMoves))\n",
    "            root_scores = root_scores / np.sum(root_scores)\n",
    "        # run the playouts\n",
    "        end = time.time()\n",
    "        reps = 0 \n",
    "        \n",
    "        # Run as many playouts as time permits (within reason).\n",
    "        while ((reps == 0) or ((end-start < time_limit) and (reps < 30000))):\n",
    "            # Run a single Monte carlo playout - with adaptive playouts using AMAF and priors\n",
    "            use_policy = POLICY\n",
    "            reps += 1\n",
    "            self.gm.b = root.copy() # reset board to root state\n",
    "            movesColor = [] # moves made by color this playout\n",
    "            movesOther = [] # moves made by otherColor this playout\n",
    "            # make a move at the root, a color move\n",
    "            if (use_policy):\n",
    "                j = self.colorAMAF.PUCT(CpuctRoot, rootMoves, root_scores)\n",
    "            else:\n",
    "                j = self.colorAMAF.PUCT_no_policy(CpuctRoot, rootMoves)\n",
    "            training_visits[j % 7] += 1 # keep track of starting column\n",
    "            #print(\"root move\", j)\n",
    "            self.gm.b[j] = color\n",
    "            movesColor.append(j)\n",
    "            cnt = 1\n",
    "            status = 1\n",
    "            while (status == 1):\n",
    "                # ~~~~~~~~~~~~~~~~~~~ otherColor moves ~~~~~~~~~~~~~~~\n",
    "                moves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, otherColor, color)\n",
    "                if (status == 2):\n",
    "                    result = otherColor\n",
    "                    x = moves[0]\n",
    "                    self.gm.b[x] = otherColor\n",
    "                    movesOther.append(x)\n",
    "                if (status == 0):\n",
    "                    result = 0\n",
    "                if (status == 1):\n",
    "                    if (len(moves) == 1):\n",
    "                        x = moves[0]\n",
    "                        #movesOther.append(x) #sort of a forced move so better not to use it\n",
    "                    else:\n",
    "                        if (use_policy):\n",
    "                            scores = linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(moves), otherColor) \n",
    "                            x = self.otherAMAF.PUCT(Cpuct, moves, scores)\n",
    "                        else:\n",
    "                            x = self.otherAMAF.PUCT_no_policy(Cpuct, moves)\n",
    "                        movesOther.append(x)    # use this move to reinforce RAVE\n",
    "                    self.gm.b[x] = otherColor\n",
    "                    cnt += 1\n",
    "                    # ~~~~~~~~~~~~~~~~~~~ color moves ~~~~~~~~~~~~~~~\n",
    "                    moves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, color, otherColor)\n",
    "                    if (status == 2):\n",
    "                        result = color\n",
    "                        x = moves[0]\n",
    "                        self.gm.b[x] = color\n",
    "                        movesColor.append(x)\n",
    "                    if (status == 0):\n",
    "                        result = 0\n",
    "                    if (status == 1):\n",
    "                        if (len(moves) == 1):\n",
    "                            x = moves[0]\n",
    "                            #movesColor.append(x) #sort of a forced move so better not to use it\n",
    "                        else:\n",
    "                            if (use_policy):\n",
    "                                scores = linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(moves), color) \n",
    "                                x = self.colorAMAF.PUCT(Cpuct, moves, scores)\n",
    "                            else:\n",
    "                                x = self.colorAMAF.PUCT_no_policy(Cpuct, moves)\n",
    "                            movesColor.append(x)\n",
    "                        self.gm.b[x] = color\n",
    "                        cnt += 1\n",
    "           \n",
    "            reward = 0\n",
    "            if (result == color):\n",
    "                reward = 2\n",
    "            elif (result == 0):\n",
    "                reward = 1\n",
    "            root_rewards += reward\n",
    "            # reinforce rave (AMAF)\n",
    "            self.colorAMAF.reinforce(movesColor, reward)\n",
    "            self.otherAMAF.reinforce(movesOther, 2 - reward)\n",
    "            end = time.time()\n",
    "            if (end-start > time_limit):\n",
    "                break\n",
    "            \n",
    "        #print(scores)\n",
    "        end = time.time()\n",
    "        #print(\"root_scores\", root_scores)\n",
    "        for k in range(len(rootMoves)):   # for display/diagnostics\n",
    "            x = rootMoves[k]\n",
    "            root_scores[k] = 100*self.colorAMAF.raveS[x] / max(1, self.colorAMAF.raveV[x])\n",
    "        #print(\"root_scores\", root_scores)\n",
    "        j = int(np.argmax(training_visits)) # the column\n",
    "        i = lowest_empty_row_jit(root, j) # the row\n",
    "        x = j + i * self.gm.c\n",
    "        root_rewards /= reps  # average reward for color\n",
    "        #print (root_visits, 'reps', reps, end-start, x % 7)\n",
    "        #print (training_visits, 'reps', reps, end-start, x % 7)\n",
    "        # end of Monte Carlo block\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        #b = root.copy() # reset board to root state\n",
    "        if (showit): \n",
    "            print (root_visits, \"root_rewards\", root_rewards, 'stone_count', np.sum(root != 0), 'playouts', reps, \"***\")\n",
    "            #net_move_scores   (b, rootMoves, color, otherColor, showit = True)\n",
    "        \n",
    "        status = 1 # ongoing game\n",
    "        # return column and status\n",
    "        return (x, status)  \n",
    "   \n",
    "brain = BRAIN() # The object that holds everything\n",
    "\n",
    "\n",
    "def mirror_top_to_bottom(B):\n",
    "   # Change board to my (upside down) internal coordinates \n",
    "    b = np.zeros((brain.gm.N), int)\n",
    "    for i in range(6): # rows of b\n",
    "        for j in range(7):\n",
    "            x = (i * 7) + j\n",
    "            mx = ((5 - i) * 7) + j # mirrored top to bottom\n",
    "            b[mx] = B[x]\n",
    "    return(b)    \n",
    "\n",
    "\n",
    "    \n",
    "def MCTS_wAdaptive_Playouts_(observation, configuration):\n",
    "    # calls brain to make a move \n",
    "    bail_early = False\n",
    "    start = time.time()\n",
    "    #The amount of thinking time per move. Add small margin.\n",
    "    #time_limit = configuration.actTimeout - 0.25\n",
    "    time_limit = 0.7\n",
    "    # My coordinates are upside down, so I must flip the input\n",
    "    b = mirror_top_to_bottom(np.array(observation.board))\n",
    "    \n",
    "    \n",
    "    stone_count = np.sum(b != 0)\n",
    "    if (stone_count <= 1): time_limit = time_limit / 2\n",
    "    color = observation.mark \n",
    "    \n",
    "    \n",
    "    if (stone_count <= 1): print('time_limit', time_limit)\n",
    "        \n",
    "        \n",
    "    rootMove, status = brain.MC_adaptive_move(b, color, start, time_limit, POLICY = True)\n",
    "    \n",
    "    \n",
    "    #x, status = brain.brain_linear_move(b, color) just uses heuristics, no search\n",
    "    if (bail_early and ((status == 2) or (stone_count > 40))): \n",
    "        print('About to win (or tie); cant have that, bailing')\n",
    "        return(-1)   # Cause an error so as to fail validation and not waste a submission slot.\n",
    "    \n",
    "    x = rootMove # x is the board position (one of 42)\n",
    "    x = x % brain.gm.c  # We must return the column\n",
    "    \n",
    "    \n",
    "    print('time_limit', time_limit, \"my duration\",time.time() - start, 'x', x)\n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951d2e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:09.182682Z",
     "iopub.status.busy": "2024-02-03T19:25:09.182159Z",
     "iopub.status.idle": "2024-02-03T19:25:10.788680Z",
     "shell.execute_reply": "2024-02-03T19:25:10.787032Z"
    },
    "papermill": {
     "duration": 1.629138,
     "end_time": "2024-02-03T19:25:10.791843",
     "exception": false,
     "start_time": "2024-02-03T19:25:09.162705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run MCTS_w_Adaptive_Playouts_glazed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb6fd2",
   "metadata": {
    "papermill": {
     "duration": 0.018917,
     "end_time": "2024-02-03T19:25:10.831981",
     "exception": false,
     "start_time": "2024-02-03T19:25:10.813064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [Monty Carlo Tree Search](https://www.kaggle.com/code/matant/monte-carlo-tree-search-connectx) - [*James McGuigan*](https://www.kaggle.com/jamesmcguigan)\n",
    "\n",
    "Monty Carlo Tree Search using an Object Oriented Tree with Numpy Bitboard Bitshifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643ad1b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:10.872015Z",
     "iopub.status.busy": "2024-02-03T19:25:10.871154Z",
     "iopub.status.idle": "2024-02-03T19:25:10.907794Z",
     "shell.execute_reply": "2024-02-03T19:25:10.906104Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.060715,
     "end_time": "2024-02-03T19:25:10.911149",
     "exception": false,
     "start_time": "2024-02-03T19:25:10.850434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MontyCarloTreeSearch_JamesMcGuigan.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MontyCarloTreeSearch_JamesMcGuigan.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "##### \n",
    "##### ../../kaggle_compile.py agents/MontyCarlo/MontyCarloPure.py\n",
    "##### \n",
    "##### 2020-08-26 16:45:21+01:00\n",
    "##### \n",
    "##### origin\tgit@github.com:JamesMcGuigan/ai-games.git (fetch)\n",
    "##### origin\tgit@github.com:JamesMcGuigan/ai-games.git (push)\n",
    "##### \n",
    "##### * master 247327a [ahead 6] ConnectX | reduce safety_time to 0.25s\n",
    "##### \n",
    "##### 247327afa97dfaa0c87ea36321e7be3deaa9d8d4\n",
    "##### \n",
    "\n",
    "#####\n",
    "##### START core/ConnectXBBNN.py\n",
    "#####\n",
    "\n",
    "# This is a functional implementation of ConnectX that has been optimized using both numpy and numba\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from numba import njit, int8, int64\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "# Hardcode for simplicity\n",
    "# observation   = {'mark': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "# configuration = {'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8}\n",
    "\n",
    "bitboard_type = numba.typeof(np.ndarray((2,), dtype=np.int64))\n",
    "Configuration = namedtuple('configuration', ['rows', 'columns', 'inarow'])\n",
    "configuration = Configuration(\n",
    "    rows=6,\n",
    "    columns=7,\n",
    "    inarow=4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Conversions\n",
    "\n",
    "def cast_configuration(configuration):\n",
    "    return Configuration(\n",
    "        rows    = configuration.rows,\n",
    "        columns = configuration.columns,\n",
    "        inarow  = configuration.inarow\n",
    "    )\n",
    "\n",
    "\n",
    "def is_bitboard(bitboard) -> bool:\n",
    "    if isinstance(bitboard, np.ndarray) and bitboard.dtype == np.int64 and bitboard.shape == (2,):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#@njit\n",
    "def list_to_bitboard(listboard: Union[np.ndarray,List[int]]) -> np.ndarray:\n",
    "    # bitboard[0] = played, is a square filled             | 0 = empty, 1 = filled\n",
    "    # bitboard[1] = player, who's token is this, if filled | 0 = empty, 1 = filled\n",
    "    bitboard_played = 0  # 42 bit number for if board square has been played\n",
    "    bitboard_player = 0  # 42 bit number for player 0=p1 1=p2\n",
    "    if isinstance(listboard, np.ndarray): listboard = listboard.flatten()\n",
    "    for n in range(len(listboard)):  # prange\n",
    "        if listboard[n] != 0:\n",
    "            bitboard_played |= (1 << n)        # is a square filled (0 = empty | 1 = filled)\n",
    "            if listboard[n] == 2:\n",
    "                bitboard_player |= (1 << n)    # mark as player 2 square, else assume p1=0 as default\n",
    "    bitboard = np.array([bitboard_played, bitboard_player], dtype=np.int64)\n",
    "    return bitboard\n",
    "\n",
    "\n",
    "@njit(int8[:,:](int64[:]))\n",
    "def bitboard_to_numpy2d(bitboard: np.ndarray) -> np.ndarray:\n",
    "    global configuration\n",
    "    rows    = configuration.rows\n",
    "    columns = configuration.columns\n",
    "    size    = rows * columns\n",
    "    output  = np.zeros((size,), dtype=np.int8)\n",
    "    for i in range(size):  # prange\n",
    "        is_played = (bitboard[0] >> i) & 1\n",
    "        if is_played:\n",
    "            player = (bitboard[1] >> i) & 1\n",
    "            output[i] = 1 if player == 0 else 2\n",
    "    return output.reshape((rows, columns))\n",
    "\n",
    "\n",
    "### Bitboard Operations\n",
    "\n",
    "@njit\n",
    "def empty_bitboard() -> np.ndarray:\n",
    "    return np.array([0, 0], dtype=np.int64)\n",
    "\n",
    "\n",
    "def bitboard_from_actions(actions: List[Union[int, Tuple[int]]]) -> np.ndarray:\n",
    "    bitboard  = empty_bitboard()\n",
    "    player_id = 1\n",
    "    for action in actions:\n",
    "        if isinstance(action, tuple): action, player_id = action\n",
    "        bitboard  = result_action(bitboard, action, player_id=player_id % 2)\n",
    "        player_id = next_player_id(player_id)\n",
    "    return bitboard\n",
    "\n",
    "\n",
    "@njit\n",
    "def hash_bitboard( bitboard: np.ndarray ) -> Tuple[int,int]:\n",
    "    \"\"\" Create a tupleised mirror hash, the minimum value of the bitboard and its mirrored reverse \"\"\"\n",
    "    if bitboard[0] == 0:\n",
    "        return ( bitboard[0], bitboard[1] )\n",
    "\n",
    "    global configuration\n",
    "    mirror_0 = mirror_bitstring(bitboard[0])\n",
    "    if bitboard[0] < mirror_0:\n",
    "        return ( bitboard[0], bitboard[1] )\n",
    "    else:\n",
    "        mirror_1 = mirror_bitstring(bitboard[1])\n",
    "        if bitboard[0] == mirror_0 and bitboard[1] <= mirror_1:\n",
    "            return ( bitboard[0], bitboard[1] )\n",
    "        else:\n",
    "            return ( mirror_0, mirror_1 )\n",
    "\n",
    "\n",
    "# Use string reverse to create mirror bit lookup table: mirror_bits[ 0100000 ] == 0000010\n",
    "mirror_bits = np.array([\n",
    "    int( \"\".join(reversed(f'{n:07b}')), 2 )\n",
    "    for n in range(2**configuration.columns)\n",
    "], dtype=np.int64)\n",
    "\n",
    "@njit\n",
    "def mirror_bitstring( bitstring: int ) -> int:\n",
    "    \"\"\" Return the mirror view of the board for hashing:  0100000 -> 0000010 \"\"\"\n",
    "    global configuration\n",
    "\n",
    "    if bitstring == 0:\n",
    "        return 0  # short-circuit for empty board\n",
    "\n",
    "    bitsize     = configuration.columns * configuration.rows        # total number of bits to process\n",
    "    unit_size   = configuration.columns                             # size of each row in bits\n",
    "    unit_mask   = (1 << unit_size) - 1                              # == 0b1111111 | 0x7f\n",
    "    offsets     = np.arange(0, bitsize, unit_size, dtype=np.int64)  # == [ 0, 7, 14, 21, 28, 35 ]\n",
    "\n",
    "    # row_masks   = unit_mask               << offsets  # create bitmasks for each row\n",
    "    # bits        = (bitstring & row_masks) >> offsets  # extract out the bits for each row\n",
    "    # stib        = mirror_bits[ bits ]     << offsets  # lookup mirror bits for each row and shift back into position\n",
    "    # output      = np.sum(stib)                        # np.sum() will bitwise AND the array assuming no overlapping bits\n",
    "\n",
    "    # This can technically be done as a one liner:\n",
    "    output = np.sum( mirror_bits[ (bitstring & (unit_mask << offsets)) >> offsets ] << offsets )\n",
    "\n",
    "    ### Old Loop Implementation\n",
    "    # output = 0\n",
    "    # for row in range(configuration.rows):\n",
    "    #     offset = row * configuration.columns\n",
    "    #     mask   = unit_mask          << offset\n",
    "    #     bits   = (bitstring & mask) >> offset\n",
    "    #     if bits == 0: continue\n",
    "    #     stib   = mirror_bits[ bits ]\n",
    "    #     output = output | (stib << offset)\n",
    "\n",
    "    return int(output)\n",
    "\n",
    "\n",
    "@njit\n",
    "def mirror_bitboard( bitboard: np.ndarray ) -> np.ndarray:\n",
    "    return np.array([\n",
    "        mirror_bitstring(bitboard[0]),\n",
    "        mirror_bitstring(bitboard[1]),\n",
    "    ], dtype=bitboard.dtype)\n",
    "\n",
    "\n",
    "\n",
    "### Player Id\n",
    "\n",
    "@njit\n",
    "def current_player_id( bitboard: np.ndarray ) -> int:\n",
    "    \"\"\" Returns next player to move: 1 = p1, 2 = p2 \"\"\"\n",
    "    move_number = get_move_number(bitboard)\n",
    "    next_player = 1 if move_number % 2 == 0 else 2  # player 1 has the first move on an empty board\n",
    "    return next_player\n",
    "\n",
    "def current_player_index( bitboard: np.ndarray ) -> int:\n",
    "    \"\"\" Returns next player to move: 0 = p1, 1 = p2 \"\"\"\n",
    "    move_number = get_move_number(bitboard)\n",
    "    next_player = 0 if move_number % 2 == 0 else 1  # player 1 has the first move on an empty board\n",
    "    return next_player\n",
    "\n",
    "\n",
    "@njit(int8(int8))\n",
    "def next_player_id(player_id: int) -> int:\n",
    "    # assert player_id in [1,2]\n",
    "    return 1 if player_id == 2 else 2\n",
    "\n",
    "\n",
    "\n",
    "### Coordinates\n",
    "\n",
    "@njit\n",
    "def index_to_coords(index: int) -> Tuple[int,int]:\n",
    "    global configuration\n",
    "    row    = index // configuration.columns\n",
    "    column = index - row * configuration.columns\n",
    "    return (row, column)\n",
    "\n",
    "\n",
    "@njit\n",
    "def coords_to_index(row: int, column: int) -> int:\n",
    "    global configuration\n",
    "    return column + row * configuration.columns\n",
    "\n",
    "\n",
    "\n",
    "### Moves\n",
    "\n",
    "@njit(int64[:](int8))\n",
    "def get_bitcount_mask(size: int = configuration.columns * configuration.rows) -> np.ndarray:\n",
    "    # return np.array([1 << index for index in range(0, size)], dtype=np.int64)\n",
    "    return 1 << np.arange(0, size, dtype=np.int64)\n",
    "\n",
    "# bitcount_mask = get_bitcount_mask()\n",
    "\n",
    "\n",
    "@njit(int8(int64[:]))\n",
    "def get_move_number(bitboard: np.ndarray) -> int:\n",
    "    global configuration\n",
    "    if bitboard[0] == 0: return 0\n",
    "    size          = configuration.columns * configuration.rows\n",
    "    mask_bitcount = get_bitcount_mask(size)\n",
    "    move_number   = np.count_nonzero(bitboard[0] & mask_bitcount)\n",
    "    return move_number\n",
    "\n",
    "\n",
    "mask_board       = (1 << configuration.columns * configuration.rows) - 1\n",
    "mask_legal_moves = (1 << configuration.columns) - 1\n",
    "\n",
    "@njit\n",
    "def has_no_illegal_moves( bitboard: np.ndarray ) -> int:\n",
    "    \"\"\"If any the squares on the top row have been played, then there are illegal moves\"\"\"\n",
    "    are_all_moves_legal = ((bitboard[0] & mask_legal_moves) == 0)\n",
    "    return 1 if are_all_moves_legal else 0\n",
    "\n",
    "\n",
    "@njit\n",
    "def has_no_more_moves(bitboard: np.ndarray) -> bool:\n",
    "    \"\"\"If all the squares on the top row have been played, then there are no more moves\"\"\"\n",
    "    return bitboard[0] & mask_legal_moves == mask_legal_moves\n",
    "\n",
    "\n",
    "_is_legal_move_mask  = ((1 << configuration.columns) - 1)\n",
    "_is_legal_move_cache = np.array([\n",
    "    [\n",
    "        int( (bits >> action) & 1 == 0 )\n",
    "        for action in range(configuration.columns)\n",
    "    ]\n",
    "    for bits in range(2**configuration.columns)\n",
    "], dtype=np.int8)\n",
    "\n",
    "@njit\n",
    "def is_legal_move(bitboard: np.ndarray, action: int) -> int:\n",
    "    bits = bitboard[0] & _is_legal_move_mask   # faster than: int( (bitboard[0] >> action) & 1 == 0 )\n",
    "    return _is_legal_move_cache[bits, action]  # NOTE: [bits,action] is faster than [bits][action]\n",
    "\n",
    "#@njit\n",
    "def get_legal_moves(bitboard: np.ndarray) -> np.ndarray:\n",
    "    # First 7 bytes represent the top row. Moves are legal if the sky is unplayed\n",
    "    global configuration\n",
    "    bits = bitboard[0] & _is_legal_move_mask  # faster than: int( (bitboard[0] >> action) & 1 == 0 )\n",
    "    if bits == 0:\n",
    "        return actions  # get_all_moves()\n",
    "    else:\n",
    "        return np.array([\n",
    "            action\n",
    "            for action in range(configuration.columns)\n",
    "            if _is_legal_move_cache[bits, action]\n",
    "        ], dtype=np.int8)\n",
    "\n",
    "\n",
    "actions = np.array([ action for action in range(configuration.columns) ], dtype=np.int64)\n",
    "@njit\n",
    "def get_all_moves() -> np.ndarray:\n",
    "    # First 7 bytes represent the top row. Moves are legal if the sky is unplayed\n",
    "    return actions\n",
    "    # global configuration\n",
    "    # return np.array([ action for action in range(configuration.columns) ])\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_random_move(bitboard: np.ndarray) -> int:\n",
    "    \"\"\" This is slightly quicker than random.choice(get_all_moves())\"\"\"\n",
    "    # assert not has_no_more_moves(bitboard)\n",
    "\n",
    "    global configuration\n",
    "    while True:\n",
    "        action = np.random.randint(0, configuration.columns)\n",
    "        if is_legal_move(bitboard, action):\n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "# Actions + Results\n",
    "\n",
    "@njit\n",
    "def get_next_index(bitboard: np.ndarray, action: int) -> int:\n",
    "    global configuration\n",
    "    # assert is_legal_move(bitboard, action)\n",
    "\n",
    "    # Start at the ground, and return first row that contains a 0\n",
    "    for row in range(configuration.rows-1, -1, -1):\n",
    "        index = action + (row * configuration.columns)\n",
    "        value = (bitboard[0] >> index) & 1\n",
    "        if value == 0:\n",
    "            return index\n",
    "    return action  # this should never happen - implies not is_legal_move(action)\n",
    "\n",
    "@njit\n",
    "def get_next_row(bitboard: np.ndarray, action: int) -> int:\n",
    "    global configuration\n",
    "    index = get_next_index(bitboard, action)\n",
    "    row   = index // configuration.columns\n",
    "    return row\n",
    "\n",
    "\n",
    "@njit\n",
    "def result_action(bitboard: np.ndarray, action: int, player_id: int) -> np.ndarray:\n",
    "    # assert is_legal_move(bitboard, action)\n",
    "    index    = get_next_index(bitboard, action)\n",
    "    mark     = 0 if player_id == 1 else 1\n",
    "    output = np.array([\n",
    "        bitboard[0] | 1    << index,\n",
    "        bitboard[1] | mark << index\n",
    "    ], dtype=bitboard.dtype)\n",
    "    return output\n",
    "\n",
    "\n",
    "### Simulations\n",
    "\n",
    "#@njit\n",
    "def run_random_simulation( bitboard: np.ndarray, player_id: int ) -> float:\n",
    "    \"\"\" Returns +1 = victory | 0.5 = draw | 0 = loss \"\"\"\n",
    "    move_number = get_move_number(bitboard)\n",
    "    next_player = 1 if move_number % 2 == 0 else 2  # player 1 has the first move on an empty board\n",
    "    while not is_gameover(bitboard):\n",
    "        actions     = get_legal_moves(bitboard)\n",
    "        action      = np.random.choice(actions)\n",
    "        bitboard    = result_action(bitboard, action, next_player)\n",
    "        next_player = next_player_id(next_player)\n",
    "        # print( bitboard_to_numpy2d(bitboard) )  # DEBUG\n",
    "    score = get_utility_zero_one(bitboard, player_id)\n",
    "    return score\n",
    "\n",
    "\n",
    "### Endgame\n",
    "\n",
    "@njit(int64[:]())\n",
    "def get_gameovers() -> np.ndarray:\n",
    "    \"\"\"Creates a list of all winning board positions, over 4 directions: horizontal, vertical and 2 diagonals\"\"\"\n",
    "    global configuration\n",
    "\n",
    "    rows    = configuration.rows\n",
    "    columns = configuration.columns\n",
    "    inarow  = configuration.inarow\n",
    "\n",
    "    gameovers = []\n",
    "\n",
    "    mask_horizontal  = 0\n",
    "    mask_vertical    = 0\n",
    "    mask_diagonal_dl = 0\n",
    "    mask_diagonal_ul = 0\n",
    "    for n in range(inarow):  # prange\n",
    "        mask_horizontal  |= 1 << n\n",
    "        mask_vertical    |= 1 << n * columns\n",
    "        mask_diagonal_dl |= 1 << n * columns + n\n",
    "        mask_diagonal_ul |= 1 << n * columns + (inarow - 1 - n)\n",
    "\n",
    "    row_inner = rows    - inarow\n",
    "    col_inner = columns - inarow\n",
    "    for row in range(rows):  # prange\n",
    "        for col in range(columns):  # prange\n",
    "            offset = col + row * columns\n",
    "            if col <= col_inner:\n",
    "                gameovers.append( mask_horizontal << offset )\n",
    "            if row <= row_inner:\n",
    "                gameovers.append( mask_vertical << offset )\n",
    "            if col <= col_inner and row <= row_inner:\n",
    "                gameovers.append( mask_diagonal_dl << offset )\n",
    "                gameovers.append( mask_diagonal_ul << offset )\n",
    "\n",
    "    _get_gameovers_cache = np.array(gameovers, dtype=np.int64)\n",
    "    return _get_gameovers_cache\n",
    "\n",
    "gameovers = get_gameovers()\n",
    "\n",
    "\n",
    "@njit\n",
    "def is_gameover(bitboard: np.ndarray) -> bool:\n",
    "    if has_no_more_moves(bitboard):  return True\n",
    "    if get_winner(bitboard) != 0:    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_winner(bitboard: np.ndarray) -> int:\n",
    "    \"\"\" Endgame get_winner: 0 for no get_winner, 1 = player 1, 2 = player 2\"\"\"\n",
    "    global gameovers\n",
    "    # gameovers = get_gameovers()\n",
    "    p2_wins = (bitboard[0] &  bitboard[1]) & gameovers == gameovers\n",
    "    if np.any(p2_wins): return 2\n",
    "    p1_wins = (bitboard[0] & ~bitboard[1]) & gameovers == gameovers\n",
    "    if np.any(p1_wins): return 1\n",
    "    return 0\n",
    "\n",
    "    # NOTE: above implementation is 2x faster than this original attempt\n",
    "    # gameovers_played = gameovers[ gameovers & bitboard[0] == gameovers ]  # exclude any unplayed squares\n",
    "    # if np.any(gameovers_played):                                          # have 4 tokens been played in a row yet\n",
    "    #     p1_wins = gameovers_played & ~bitboard[1] == gameovers_played\n",
    "    #     if np.any(p1_wins): return 1\n",
    "    #     p2_wins = gameovers_played &  bitboard[1] == gameovers_played\n",
    "    #     if np.any(p2_wins): return 2\n",
    "    # return 0\n",
    "\n",
    "\n",
    "### Utility Scores\n",
    "\n",
    "@njit\n",
    "def get_utility_one(bitboard: np.ndarray, player_id: int) -> int:\n",
    "    \"\"\" Like get_utility_inf but returns: 1 for victory, -1 for loss, 0 for draw \"\"\"\n",
    "    winning_player = get_winner(bitboard)\n",
    "    if winning_player == 0: return 0\n",
    "    return 1 if winning_player == player_id else -1\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_utility_zero_one(bitboard: np.ndarray, player_id: int) -> float:\n",
    "    \"\"\" Like get_utility_one but returns: 1 for victory, 0 for loss, 0.5 for draw \"\"\"\n",
    "    winning_player = get_winner(bitboard)\n",
    "    if winning_player == 0: return 0.5\n",
    "    return 1.0 if winning_player == player_id else 0.0\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_utility_inf(bitboard: np.ndarray, player_id: int) -> float:\n",
    "    \"\"\" Like get_utility_one but returns: +inf for victory, -inf for loss, 0 for draw \"\"\"\n",
    "    winning_player = get_winner(bitboard)\n",
    "    if winning_player == 0: return 0\n",
    "    return +np.inf if winning_player == player_id else -np.inf\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   core/ConnectXBBNN.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START util/base64_file.py\n",
    "#####\n",
    "\n",
    "import base64\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Union\n",
    "import humanize\n",
    "\n",
    "# _base64_file__test_base64_static_import = \"\"\"\n",
    "# H4sIAPx9LF8C/2tgri1k0IjgYGBgKCxNLS7JzM8rZIwtZNLwZvBm8mYEkjAI4jFB2KkRbED1iXnF\n",
    "# 5alFhczeWqV6AEGfwmBHAAAA\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "def base64_file_varname(filename: str) -> str:\n",
    "    # ../data/AntColonyTreeSearchNode.pickle.zip.base64 -> _base64_file__AntColonyTreeSearchNode__pickle__zip__base64\n",
    "    varname = re.sub(r'^.*/',   '',   filename)  # remove directories\n",
    "    varname = re.sub(r'[.\\W]+', '__', varname)   # convert dots and non-ascii to __\n",
    "    varname = f\"_base64_file__{varname}\"\n",
    "    return varname\n",
    "\n",
    "\n",
    "def base64_file_var_wrap(base64_data: Union[str,bytes], varname: str) -> str:\n",
    "    return f'{varname} = \"\"\"\\n{base64_data.strip()}\\n\"\"\"'                    # add varname = \"\"\"\\n\\n\"\"\" wrapper\n",
    "\n",
    "\n",
    "def base64_file_var_unwrap(base64_data: str) -> str:\n",
    "    output = base64_data.strip()\n",
    "    output = re.sub(r'^\\w+ = \"\"\"|\"\"\"$', '', output)  # remove varname = \"\"\" \"\"\" wrapper\n",
    "    output = output.strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def base64_file_encode(data: Any) -> str:\n",
    "    encoded = pickle.dumps(data)\n",
    "    encoded = gzip.compress(encoded)\n",
    "    encoded = base64.encodebytes(encoded).decode('utf8').strip()\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def base64_file_decode(encoded: str) -> Any:\n",
    "    data = base64.b64decode(encoded)\n",
    "    data = gzip.decompress(data)\n",
    "    data = pickle.loads(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def base64_file_save(data: Any, filename: str, vebose=True) -> float:\n",
    "    \"\"\"\n",
    "        Saves a base64 encoded version of data into filename, with a varname wrapper for importing via kaggle_compile.py\n",
    "        # Doesn't create/update global variable.\n",
    "        Returns filesize in bytes\n",
    "    \"\"\"\n",
    "    varname    = base64_file_varname(filename)\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, 'wb') as file:\n",
    "            encoded = base64_file_encode(data)\n",
    "            output  = base64_file_var_wrap(encoded, varname)\n",
    "            output  = output.encode('utf8')\n",
    "            file.write(output)\n",
    "            file.close()\n",
    "        if varname in globals(): globals()[varname] = encoded  # globals not shared between modules, but update for saftey\n",
    "\n",
    "        filesize = os.path.getsize(filename)\n",
    "        if vebose:\n",
    "            time_taken = time.perf_counter() - start_time\n",
    "            print(f\"base64_file_save(): {filename:40s} | {humanize.naturalsize(filesize)} in {time_taken:4.1f}s\")\n",
    "        return filesize\n",
    "    except Exception as exception:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def base64_file_load(filename: str, vebose=True) -> Union[Any,None]:\n",
    "    \"\"\"\n",
    "        Performs a lookup to see if the global variable for this file alread exists\n",
    "        If not, reads the base64 encoded file from filename, with an optional varname wrapper\n",
    "        # Doesn't create/update global variable.\n",
    "        Returns decoded data\n",
    "    \"\"\"\n",
    "    varname    = base64_file_varname(filename)\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        # Hard-coding PyTorch weights into a script - https://www.kaggle.com/c/connectx/discussion/126678\n",
    "        encoded = None\n",
    "\n",
    "        if varname in globals():\n",
    "            encoded = globals()[varname]\n",
    "\n",
    "        if encoded is None and os.path.exists(filename):\n",
    "            with open(filename, 'rb') as file:\n",
    "                encoded = file.read().decode('utf8')\n",
    "                encoded = base64_file_var_unwrap(encoded)\n",
    "                # globals()[varname] = encoded  # globals are not shared between modules\n",
    "\n",
    "        if encoded is not None:\n",
    "            data = base64_file_decode(encoded)\n",
    "\n",
    "            if vebose:\n",
    "                filesize = os.path.getsize(filename)\n",
    "                time_taken = time.perf_counter() - start_time\n",
    "                print(f\"base64_file_load(): {filename:40s} | {humanize.naturalsize(filesize)} in {time_taken:4.1f}s\")\n",
    "            return data\n",
    "    except Exception as exception:\n",
    "        print(f'base64_file_load({filename}): Exception:', exception)\n",
    "    return None\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   util/base64_file.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START agents/MontyCarlo/MontyCarloPure.py\n",
    "#####\n",
    "\n",
    "# This is a LinkedList implementation of MontyCarlo Tree Search\n",
    "# Inspired by https://www.kaggle.com/matant/monte-carlo-tree-search-connectx\n",
    "import atexit\n",
    "import time\n",
    "from struct import Struct\n",
    "from typing import Callable\n",
    "\n",
    "# from core.ConnectXBBNN import *\n",
    "# from util.base64_file import base64_file_load\n",
    "# from util.base64_file import base64_file_save\n",
    "\n",
    "Hyperparameters = namedtuple('hyperparameters', [])\n",
    "\n",
    "class MontyCarloNode:\n",
    "    persist   = True\n",
    "    save_node = {}                                                        # save_node[cls.__name__] = cls(empty_bitboard(), 1)\n",
    "    root_nodes: List[Union['MontyCarloNode', None]] = [None, None, None]  # root_nodes[observation.mark]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            bitboard:      np.ndarray,\n",
    "            player_id:     int,\n",
    "            parent:        Union['MontyCarloNode', None] = None,\n",
    "            parent_action: Union[int,None]       = None,\n",
    "            exploration:   float = 1.0,\n",
    "            **kwargs\n",
    "    ):\n",
    "        self.bitboard      = bitboard\n",
    "        self.player_id     = player_id\n",
    "        self.next_player   = 3 - player_id\n",
    "\n",
    "        self.exploration   = exploration\n",
    "        self.kwargs        = kwargs\n",
    "\n",
    "        # self.mirror_hash   = hash_bitboard(bitboard)  # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "        self.legal_moves   = get_legal_moves(bitboard)\n",
    "        self.is_gameover   = is_gameover(bitboard)\n",
    "        self.winner        = get_winner(bitboard) if self.is_gameover else 0\n",
    "        self.utility       = 1 if self.winner == self.player_id else 0  # Scores in range 0-1\n",
    "\n",
    "        self.parent        = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.is_expanded   = False\n",
    "        self.children: List[Union[MontyCarloNode, None]] = [None for action in get_all_moves()]  # include illegal moves to preserve indexing\n",
    "        self.total_score   = 0.0\n",
    "        self.total_visits  = 0\n",
    "        self.ucb1_score    = self.get_ucb1_score(self)\n",
    "\n",
    "\n",
    "\n",
    "    def __hash__(self):\n",
    "        return tuple(self.bitboard)\n",
    "        # return self.mirror_hash  # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "\n",
    "\n",
    "\n",
    "    ### Loading and Saving\n",
    "\n",
    "    @classmethod\n",
    "    def prune(cls, node: 'MontyCarloNode', min_visits=7, pruned_count=0, total_count=0):\n",
    "        for n, child in enumerate(node.children):\n",
    "            if child is None: continue\n",
    "            if child.total_visits < min_visits:\n",
    "                pruned_count    += child.total_visits  # excepting terminal states, this equals the number of grandchildren\n",
    "                total_count     += child.total_visits  # excepting terminal states, this equals the number of grandchildren\n",
    "                node.children[n] = None\n",
    "                node.is_expanded = False  # Use def expand(self) to reinitalize state\n",
    "            else:\n",
    "                total_count += 1\n",
    "                pruned_count, total_count = cls.prune(child, min_visits, pruned_count, total_count)\n",
    "        return pruned_count, total_count\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def filename(cls):\n",
    "        return f\"data/{cls.__name__}_base64.py\"\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        filename = cls.filename()\n",
    "        loaded   = base64_file_load(filename)\n",
    "        if loaded is not None:\n",
    "            cls.save_node[cls.__name__] = loaded\n",
    "            return loaded\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls) -> Union[str,None]:\n",
    "        if cls.persist == True and cls.save_node.get(cls.__name__, None) is not None:\n",
    "            save_node    = cls.save_node[cls.__name__]\n",
    "\n",
    "            start_time   = time.perf_counter()\n",
    "            pruned_count, total_count = cls.prune(save_node)  # This reduces a 47MB base64 file down to 5Mb\n",
    "            print(f'{cls.__name__}.save() - pruned {pruned_count:.0f}/{total_count:.0f} nodes leaving {total_count-pruned_count:.0f} in {time.perf_counter() - start_time:.2f}s')\n",
    "\n",
    "            filename = cls.filename()\n",
    "            filesize = base64_file_save(save_node, filename)\n",
    "            return filename\n",
    "        return None\n",
    "\n",
    "    ### Constructors and Lookups\n",
    "\n",
    "    def create_child( self, action: int ) -> 'MontyCarloNode':\n",
    "        result = result_action(self.bitboard, action, self.player_id)\n",
    "        child  = None  # self.find_mirror_child(result, depth=1)  # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "        if child is None:\n",
    "            child = self.__class__(\n",
    "                bitboard      = result,\n",
    "                player_id     = next_player_id(self.player_id),\n",
    "                parent        = self,\n",
    "                parent_action = action,\n",
    "                exploration   = self.exploration,\n",
    "                **self.kwargs\n",
    "            )\n",
    "        self.children[action] = child\n",
    "        self.is_expanded      = self._is_expanded()\n",
    "        if self.is_expanded:\n",
    "            self.on_expanded()\n",
    "        return child\n",
    "\n",
    "\n",
    "    def find_child( self, bitboard: np.array, depth=2 ) -> Union['MontyCarloNode', None]:\n",
    "        # assert 0 <= depth <= 2\n",
    "\n",
    "        if depth >= 0:\n",
    "            if np.all( self.bitboard == bitboard ):\n",
    "                return self\n",
    "        if depth >= 1:\n",
    "            for child in self.children:\n",
    "                if child is None: continue\n",
    "                if np.all( child.bitboard == bitboard ):\n",
    "                    return child\n",
    "        if depth >= 2:\n",
    "            # Avoid recursion to prevent duplicate calls to hash_bitboard()\n",
    "            for child in self.children:\n",
    "                if child is None: continue\n",
    "                for grandchild in child.children:\n",
    "                    if grandchild is None: continue\n",
    "                    if np.all( grandchild.bitboard == bitboard ):\n",
    "                        return grandchild\n",
    "        return None\n",
    "\n",
    "    # # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "    # def find_mirror_child( self, bitboard: np.array, depth=2 ) -> Union['MontyCarloNode', None]:\n",
    "    #     # assert 0 <= depth <= 2\n",
    "    #     mirror_hash = hash_bitboard(bitboard)\n",
    "    #\n",
    "    #     if depth >= 0:\n",
    "    #         if self.mirror_hash == mirror_hash:\n",
    "    #             return self\n",
    "    #     if depth >= 1:\n",
    "    #         for child in self.children:\n",
    "    #             if child is None: continue\n",
    "    #             if child.mirror_hash == mirror_hash:\n",
    "    #                 return child\n",
    "    #     if depth >= 2:\n",
    "    #         # Avoid recursion to prevent duplicate calls to hash_bitboard()\n",
    "    #         for child in self.children:\n",
    "    #             if child is None: continue\n",
    "    #             for grandchild in child.children:\n",
    "    #                 if grandchild is None: continue\n",
    "    #                 if grandchild.mirror_hash == mirror_hash:\n",
    "    #                     return grandchild\n",
    "    #     return None\n",
    "\n",
    "\n",
    "\n",
    "    ### Properties\n",
    "\n",
    "    def _is_expanded(self) -> bool:\n",
    "        is_expanded = True\n",
    "        for action in self.legal_moves:\n",
    "            if self.children[action] is None:\n",
    "                is_expanded = False\n",
    "                break\n",
    "        return is_expanded\n",
    "\n",
    "\n",
    "    def get_unexpanded(self) -> List[int]:\n",
    "        return [\n",
    "            action\n",
    "            for action in self.legal_moves\n",
    "            if self.children[action] is None\n",
    "        ]\n",
    "\n",
    "\n",
    "    ### Action Selection\n",
    "\n",
    "    def get_best_action(self) -> int:\n",
    "        scores = [\n",
    "            self.children[action].total_score\n",
    "            if self.children[action] is not None else 0\n",
    "            for action in self.legal_moves\n",
    "        ]\n",
    "        index  = np.argmax(scores)\n",
    "        action = self.legal_moves[index]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_exploration_action(self) -> int:\n",
    "        scores = [\n",
    "            self.children[action].ucb1_score\n",
    "            if self.children[action] is not None else 0\n",
    "            for action in self.legal_moves\n",
    "        ]\n",
    "        index  = np.argmax(scores)\n",
    "        action = self.legal_moves[index]\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "    ### Scores\n",
    "\n",
    "    def get_ucb1_score(self, node: 'MontyCarloNode') -> float:\n",
    "        if node is None or node.total_visits == 0:\n",
    "            return np.inf\n",
    "        else:\n",
    "            score = node.total_score / node.total_visits\n",
    "            if node.parent is not None and node.parent.total_visits > 0:\n",
    "                score += (\n",
    "                    node.exploration * np.sqrt(2)\n",
    "                    * np.log(node.parent.total_visits) / node.total_visits\n",
    "                )\n",
    "            return score\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def opponents_score(score: float):\n",
    "        # assert 0 <= score <= 1\n",
    "        return 1 - score\n",
    "\n",
    "\n",
    "\n",
    "    ### Training and Backpropagation\n",
    "\n",
    "    def single_run(self):\n",
    "        if self.is_gameover:\n",
    "            self.backpropagate(self.utility)\n",
    "        elif not self.is_expanded:\n",
    "            child = self.expand()\n",
    "            score = child.simulate()    # score from the perspective of the other player\n",
    "            child.backpropagate(score)\n",
    "        else:\n",
    "            # Recurse down tree, until a gameover or not expanded node is found\n",
    "            action = self.get_exploration_action()\n",
    "            child  = self.children[action]\n",
    "            child.single_run()\n",
    "\n",
    "\n",
    "    def expand(self) -> 'MontyCarloNode':\n",
    "        # assert not self.is_gameover\n",
    "        # assert not self.is_expanded\n",
    "\n",
    "        unexpanded = self.get_unexpanded()\n",
    "        # assert len(unexpanded)\n",
    "\n",
    "        action     = np.random.choice(unexpanded)\n",
    "        child      = self.create_child(action)\n",
    "        return child\n",
    "\n",
    "    def on_expanded(self) -> None:\n",
    "        # Callback placeholder for any subclass hooks\n",
    "        pass\n",
    "\n",
    "    def simulate(self) -> float:\n",
    "        return run_random_simulation(self.bitboard, self.player_id)\n",
    "\n",
    "\n",
    "    def backpropagate(self, score: float):\n",
    "        # child.simulate()  returns score for the player 2\n",
    "        # child.total_score is accessed via the parent node, so score on this node is from the perspective of player 1\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            score = self.opponents_score(score)\n",
    "            node.total_score  += score\n",
    "            node.total_visits += 1\n",
    "            node = node.parent      # when we reach the root: node.parent == None which terminates\n",
    "\n",
    "        # get_ucb1_score() gets called 4x less often if we cache the value after backpropagation\n",
    "        # get_ucb1_score() depends on parent.total_visits, so needs to be called after updating scores\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            node.ucb1_score = node.get_ucb1_score(node)\n",
    "            node = node.parent      # when we reach the root: node.parent == None which terminates\n",
    "\n",
    "\n",
    "\n",
    "    ### Agent\n",
    "    @classmethod\n",
    "    def agent(cls, **kwargs) -> Callable[[Struct, Struct],int]:\n",
    "        def kaggle_agent( observation: Struct, _configuration_: Struct ):\n",
    "            first_move_time = 0\n",
    "            safety_time     = kwargs.get('safety_time', 0.25)\n",
    "            start_time      = time.perf_counter()\n",
    "            # configuration   = cast_configuration(_configuration_)\n",
    "\n",
    "            player_id     = int(observation.mark)\n",
    "            listboard     = np.array(observation.board, dtype=np.int8)\n",
    "            bitboard      = list_to_bitboard(listboard)\n",
    "            move_number   = get_move_number(bitboard)\n",
    "            is_first_move = int(move_number < 2)\n",
    "          # endtime       = start_time + _configuration_.timeout - safety_time - (first_move_time * is_first_move)\n",
    "            endtime       = start_time +            1.25          - safety_time - (first_move_time * is_first_move)\n",
    "            \n",
    "            if cls.persist == True and cls.save_node.get(cls.__name__, None) is None:\n",
    "                atexit.register(cls.save)\n",
    "                cls.save_node[cls.__name__] = cls.load() or cls(empty_bitboard(), player_id=1)\n",
    "                cls.root_nodes[1] = cls.root_nodes[2] = cls.save_node[cls.__name__]  # implement shared state\n",
    "\n",
    "            root_node = cls.root_nodes[player_id]\n",
    "            if root_node is None or root_node.find_child(bitboard, depth=2) is None:\n",
    "                root_node = cls.root_nodes[player_id] = cls(\n",
    "                    bitboard      = bitboard,\n",
    "                    player_id     = player_id,\n",
    "                    parent        = None,\n",
    "                    # configuration = configuration,\n",
    "                    **kwargs\n",
    "                )\n",
    "            else:\n",
    "                root_node = cls.root_nodes[player_id] = cls.root_nodes[player_id].find_child(bitboard)\n",
    "            # assert root_node is not None\n",
    "\n",
    "            count = 0\n",
    "            while time.perf_counter() < endtime:\n",
    "                count += 1\n",
    "                root_node.single_run()\n",
    "\n",
    "            action     = root_node.get_best_action()\n",
    "            time_taken = time.perf_counter() - start_time\n",
    "            print(f'{cls.__name__}: p{player_id} action = {action} after {count} simulations in {time_taken:.3f}s')\n",
    "            return int(action)\n",
    "\n",
    "        kaggle_agent.__name__ = cls.__name__\n",
    "        return kaggle_agent\n",
    "\n",
    "def MontyCarloPure(**kwargs):\n",
    "    # observation   = {'mark': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    # configuration = {'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8}\n",
    "    def MontyCarloPure(observation: Struct, configuration: Struct) -> int:\n",
    "        return MontyCarloNode.agent(**kwargs)(observation, configuration)\n",
    "    return MontyCarloPure\n",
    "\n",
    "def MontyCarlo_JamesMcGuigan(observation, configuration):\n",
    "    return MontyCarloPure()(observation, configuration)\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   agents/MontyCarlo/MontyCarloPure.py\n",
    "#####\n",
    "\n",
    "##### \n",
    "##### ../../kaggle_compile.py agents/MontyCarlo/MontyCarloPure.py\n",
    "##### \n",
    "##### 2020-08-26 16:45:21+01:00\n",
    "##### \n",
    "##### origin\tgit@github.com:JamesMcGuigan/ai-games.git (fetch)\n",
    "##### origin\tgit@github.com:JamesMcGuigan/ai-games.git (push)\n",
    "##### \n",
    "##### * master 247327a [ahead 6] ConnectX | reduce safety_time to 0.25s\n",
    "##### \n",
    "##### 247327afa97dfaa0c87ea36321e7be3deaa9d8d4\n",
    "##### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d165fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:10.951743Z",
     "iopub.status.busy": "2024-02-03T19:25:10.951257Z",
     "iopub.status.idle": "2024-02-03T19:25:15.170296Z",
     "shell.execute_reply": "2024-02-03T19:25:15.168256Z"
    },
    "papermill": {
     "duration": 4.244537,
     "end_time": "2024-02-03T19:25:15.173577",
     "exception": false,
     "start_time": "2024-02-03T19:25:10.929040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run MontyCarloTreeSearch_JamesMcGuigan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b739c",
   "metadata": {
    "papermill": {
     "duration": 0.016294,
     "end_time": "2024-02-03T19:25:15.206962",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.190668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [MCTS Bitboard + Bitsquares Heuristic](https://www.kaggle.com/code/jamesmcguigan/connectx-mcts-bitboard-bitsquares-heuristic) - [*James McGuigan*](https://www.kaggle.com/jamesmcguigan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "525cc864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.243613Z",
     "iopub.status.busy": "2024-02-03T19:25:15.243211Z",
     "iopub.status.idle": "2024-02-03T19:25:15.291778Z",
     "shell.execute_reply": "2024-02-03T19:25:15.290371Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.071545,
     "end_time": "2024-02-03T19:25:15.294994",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.223449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MCTS_Bitboard_Bitsquares_Heuristic_JamesMcGuigan.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MCTS_Bitboard_Bitsquares_Heuristic_JamesMcGuigan.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "##### \n",
    "##### /ai-games/kaggle_compile.py agents/MontyCarlo/MontyCarloBitsquares.py\n",
    "##### \n",
    "##### 2024-01-28 08:30:57+00:00\n",
    "##### \n",
    "##### origin\thttps://github.com/JamesMcGuigan/ai-games.git (fetch)\n",
    "##### origin\thttps://github.com/JamesMcGuigan/ai-games.git (push)\n",
    "##### \n",
    "##### * master eac436d Revert: Rock Paper Scissors | Random Seed Search | tweak unit tests\n",
    "##### \n",
    "##### eac436d23e03624c2245838138e9608cb13d2f1f\n",
    "##### \n",
    "\n",
    "#####\n",
    "##### START core/ConnectXBBNN.py\n",
    "#####\n",
    "\n",
    "# This is a functional implementation of ConnectX that has been optimized using both numpy and numba\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "# Hardcode for simplicity\n",
    "# observation   = {'mark': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "# configuration = {'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8}\n",
    "\n",
    "bitboard_type = numba.typeof(np.ndarray((2,), dtype=np.int64))\n",
    "Configuration = namedtuple('configuration', ['rows', 'columns', 'inarow'])\n",
    "configuration = Configuration(\n",
    "    rows=6,\n",
    "    columns=7,\n",
    "    inarow=4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Conversions\n",
    "\n",
    "def cast_configuration(configuration):\n",
    "    return Configuration(\n",
    "        rows    = configuration.rows,\n",
    "        columns = configuration.columns,\n",
    "        inarow  = configuration.inarow\n",
    "    )\n",
    "\n",
    "\n",
    "def is_bitboard(bitboard) -> bool:\n",
    "    if isinstance(bitboard, np.ndarray) and bitboard.dtype == np.int64 and bitboard.shape == (2,):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#@njit\n",
    "def list_to_bitboard(listboard: Union[np.ndarray,List[int]]) -> np.ndarray:\n",
    "    # bitboard[0] = played, is a square filled             | 0 = empty, 1 = filled\n",
    "    # bitboard[1] = player, who's token is this, if filled | 0 = empty, 1 = filled\n",
    "    bitboard_played = 0  # 42 bit number for if board square has been played\n",
    "    bitboard_player = 0  # 42 bit number for player 0=p1 1=p2\n",
    "    if isinstance(listboard, np.ndarray): listboard = listboard.flatten()\n",
    "    for n in range(len(listboard)):  # prange\n",
    "        if listboard[n] != 0:\n",
    "            bitboard_played |= (1 << n)        # is a square filled (0 = empty | 1 = filled)\n",
    "            if listboard[n] == 2:\n",
    "                bitboard_player |= (1 << n)    # mark as player 2 square, else assume p1=0 as default\n",
    "    bitboard = np.array([bitboard_played, bitboard_player], dtype=np.int64)\n",
    "    return bitboard\n",
    "\n",
    "\n",
    "#@njit(int8[:,:](int64[:]))\n",
    "def bitboard_to_numpy2d(bitboard: np.ndarray) -> np.ndarray:\n",
    "    global configuration\n",
    "    rows    = configuration.rows\n",
    "    columns = configuration.columns\n",
    "    size    = rows * columns\n",
    "    output  = np.zeros((size,), dtype=np.int8)\n",
    "    for i in range(size):  # prange\n",
    "        is_played = (bitboard[0] >> i) & 1\n",
    "        if is_played:\n",
    "            player = (bitboard[1] >> i) & 1\n",
    "            output[i] = 1 if player == 0 else 2\n",
    "    return output.reshape((rows, columns))\n",
    "\n",
    "\n",
    "### Bitboard Operations\n",
    "\n",
    "#@njit\n",
    "def empty_bitboard() -> np.ndarray:\n",
    "    return np.array([0, 0], dtype=np.int64)\n",
    "\n",
    "\n",
    "def bitboard_from_actions(actions: List[Union[int, Tuple[int]]]) -> np.ndarray:\n",
    "    bitboard  = empty_bitboard()\n",
    "    player_id = 1\n",
    "    for action in actions:\n",
    "        if isinstance(action, tuple): action, player_id = action\n",
    "        bitboard  = result_action(bitboard, action, player_id=player_id % 2)\n",
    "        player_id = next_player_id(player_id)\n",
    "    return bitboard\n",
    "\n",
    "\n",
    "#@njit\n",
    "def hash_bitboard( bitboard: np.ndarray ) -> Tuple[int,int]:\n",
    "    \"\"\" Create a tupleised mirror hash, the minimum value of the bitboard and its mirrored reverse \"\"\"\n",
    "    if bitboard[0] == 0:\n",
    "        return ( bitboard[0], bitboard[1] )\n",
    "\n",
    "    global configuration\n",
    "    mirror_0 = mirror_bitstring(bitboard[0])\n",
    "    if bitboard[0] < mirror_0:\n",
    "        return ( bitboard[0], bitboard[1] )\n",
    "    else:\n",
    "        mirror_1 = mirror_bitstring(bitboard[1])\n",
    "        if bitboard[0] == mirror_0 and bitboard[1] <= mirror_1:\n",
    "            return ( bitboard[0], bitboard[1] )\n",
    "        else:\n",
    "            return ( mirror_0, mirror_1 )\n",
    "\n",
    "\n",
    "# Use string reverse to create mirror bit lookup table: mirror_bits[ 0100000 ] == 0000010\n",
    "mirror_bits = np.array([\n",
    "    int( \"\".join(reversed(f'{n:07b}')), 2 )\n",
    "    for n in range(2**configuration.columns)\n",
    "], dtype=np.int64)\n",
    "\n",
    "#@njit\n",
    "def mirror_bitstring( bitstring: int ) -> int:\n",
    "    \"\"\" Return the mirror view of the board for hashing:  0100000 -> 0000010 \"\"\"\n",
    "    global configuration\n",
    "\n",
    "    if bitstring == 0:\n",
    "        return 0  # short-circuit for empty board\n",
    "\n",
    "    bitsize     = configuration.columns * configuration.rows        # total number of bits to process\n",
    "    unit_size   = configuration.columns                             # size of each row in bits\n",
    "    unit_mask   = (1 << unit_size) - 1                              # == 0b1111111 | 0x7f\n",
    "    offsets     = np.arange(0, bitsize, unit_size, dtype=np.int64)  # == [ 0, 7, 14, 21, 28, 35 ]\n",
    "\n",
    "    # row_masks   = unit_mask               << offsets  # create bitmasks for each row\n",
    "    # bits        = (bitstring & row_masks) >> offsets  # extract out the bits for each row\n",
    "    # stib        = mirror_bits[ bits ]     << offsets  # lookup mirror bits for each row and shift back into position\n",
    "    # output      = np.sum(stib)                        # np.sum() will bitwise AND the array assuming no overlapping bits\n",
    "\n",
    "    # This can technically be done as a one liner:\n",
    "    output = np.sum( mirror_bits[ (bitstring & (unit_mask << offsets)) >> offsets ] << offsets )\n",
    "\n",
    "    ### Old Loop Implementation\n",
    "    # output = 0\n",
    "    # for row in range(configuration.rows):\n",
    "    #     offset = row * configuration.columns\n",
    "    #     mask   = unit_mask          << offset\n",
    "    #     bits   = (bitstring & mask) >> offset\n",
    "    #     if bits == 0: continue\n",
    "    #     stib   = mirror_bits[ bits ]\n",
    "    #     output = output | (stib << offset)\n",
    "\n",
    "    return int(output)\n",
    "\n",
    "\n",
    "#@njit\n",
    "def mirror_bitboard( bitboard: np.ndarray ) -> np.ndarray:\n",
    "    return np.array([\n",
    "        mirror_bitstring(bitboard[0]),\n",
    "        mirror_bitstring(bitboard[1]),\n",
    "    ], dtype=bitboard.dtype)\n",
    "\n",
    "\n",
    "\n",
    "### Player Id\n",
    "\n",
    "#@njit\n",
    "def current_player_id( bitboard: np.ndarray ) -> int:\n",
    "    \"\"\" Returns next player to move: 1 = p1, 2 = p2 \"\"\"\n",
    "    move_number = get_move_number(bitboard)\n",
    "    next_player = 1 if move_number % 2 == 0 else 2  # player 1 has the first move on an empty board\n",
    "    return next_player\n",
    "\n",
    "def current_player_index( bitboard: np.ndarray ) -> int:\n",
    "    \"\"\" Returns next player to move: 0 = p1, 1 = p2 \"\"\"\n",
    "    move_number = get_move_number(bitboard)\n",
    "    next_player = 0 if move_number % 2 == 0 else 1  # player 1 has the first move on an empty board\n",
    "    return next_player\n",
    "\n",
    "\n",
    "#@njit(int8(int8))\n",
    "def next_player_id(player_id: int) -> int:\n",
    "    # assert player_id in [1,2]\n",
    "    return 1 if player_id == 2 else 2\n",
    "\n",
    "\n",
    "\n",
    "### Coordinates\n",
    "\n",
    "#@njit\n",
    "def index_to_coords(index: int) -> Tuple[int,int]:\n",
    "    global configuration\n",
    "    row    = index // configuration.columns\n",
    "    column = index - row * configuration.columns\n",
    "    return (row, column)\n",
    "\n",
    "\n",
    "#@njit\n",
    "def coords_to_index(row: int, column: int) -> int:\n",
    "    global configuration\n",
    "    return column + row * configuration.columns\n",
    "\n",
    "\n",
    "\n",
    "### Moves\n",
    "\n",
    "#@njit(int64[:](int8))\n",
    "def get_bitcount_mask(size: int = configuration.columns * configuration.rows) -> np.ndarray:\n",
    "    # return np.array([1 << index for index in range(0, size)], dtype=np.int64)\n",
    "    return 1 << np.arange(0, size, dtype=np.int64)\n",
    "\n",
    "bitcount_mask = get_bitcount_mask()\n",
    "\n",
    "\n",
    "#@njit(int8(int64[:]))\n",
    "def get_move_number(bitboard: np.ndarray) -> int:\n",
    "    global configuration\n",
    "    if bitboard[0] == 0: return 0\n",
    "    size          = configuration.columns * configuration.rows\n",
    "    mask_bitcount = get_bitcount_mask(size)\n",
    "    move_number   = np.count_nonzero(bitboard[0] & mask_bitcount)\n",
    "    return move_number\n",
    "\n",
    "\n",
    "mask_board       = (1 << configuration.columns * configuration.rows) - 1\n",
    "mask_legal_moves = (1 << configuration.columns) - 1\n",
    "\n",
    "#@njit\n",
    "def has_no_illegal_moves( bitboard: np.ndarray ) -> int:\n",
    "    \"\"\"If any the squares on the top row have been played, then there are illegal moves\"\"\"\n",
    "    are_all_moves_legal = ((bitboard[0] & mask_legal_moves) == 0)\n",
    "    return 1 if are_all_moves_legal else 0\n",
    "\n",
    "\n",
    "#@njit\n",
    "def has_no_more_moves(bitboard: np.ndarray) -> bool:\n",
    "    \"\"\"If all the squares on the top row have been played, then there are no more moves\"\"\"\n",
    "    return bitboard[0] & mask_legal_moves == mask_legal_moves\n",
    "\n",
    "\n",
    "_is_legal_move_mask  = ((1 << configuration.columns) - 1)\n",
    "_is_legal_move_cache = np.array([\n",
    "    [\n",
    "        int( (bits >> action) & 1 == 0 )\n",
    "        for action in range(configuration.columns)\n",
    "    ]\n",
    "    for bits in range(2**configuration.columns)\n",
    "], dtype=np.int8)\n",
    "\n",
    "#@njit\n",
    "def is_legal_move(bitboard: np.ndarray, action: int) -> int:\n",
    "    bits = bitboard[0] & _is_legal_move_mask   # faster than: int( (bitboard[0] >> action) & 1 == 0 )\n",
    "    return _is_legal_move_cache[bits, action]  # NOTE: [bits,action] is faster than [bits][action]\n",
    "\n",
    "#@njit\n",
    "def get_legal_moves(bitboard: np.ndarray) -> np.ndarray:\n",
    "    # First 7 bytes represent the top row. Moves are legal if the sky is unplayed\n",
    "    global configuration\n",
    "    bits = bitboard[0] & _is_legal_move_mask  # faster than: int( (bitboard[0] >> action) & 1 == 0 )\n",
    "    if bits == 0:\n",
    "        return actions  # get_all_moves()\n",
    "    else:\n",
    "        return np.array([\n",
    "            action\n",
    "            for action in range(configuration.columns)\n",
    "            if _is_legal_move_cache[bits, action]\n",
    "        ], dtype=np.int8)\n",
    "\n",
    "\n",
    "actions = np.array([ action for action in range(configuration.columns) ], dtype=np.int64)\n",
    "#@njit\n",
    "def get_all_moves() -> np.ndarray:\n",
    "    # First 7 bytes represent the top row. Moves are legal if the sky is unplayed\n",
    "    return actions\n",
    "    # global configuration\n",
    "    # return np.array([ action for action in range(configuration.columns) ])\n",
    "\n",
    "\n",
    "#@njit\n",
    "def get_random_move(bitboard: np.ndarray) -> int:\n",
    "    \"\"\" This is slightly quicker than random.choice(get_all_moves())\"\"\"\n",
    "    # assert not has_no_more_moves(bitboard)\n",
    "\n",
    "    global configuration\n",
    "    while True:\n",
    "        action = np.random.randint(0, configuration.columns)\n",
    "        if is_legal_move(bitboard, action):\n",
    "            return action\n",
    "\n",
    "#@njit\n",
    "def get_random_draw_move(bitboard: np.ndarray) -> int:\n",
    "    # Get a random move, but deliberately don't play any winning moves to generate has_no_move_moves() endgames\n",
    "    # Statistically, only 1 in 7 games generated this way are draws\n",
    "    actions   = get_legal_moves(bitboard)\n",
    "    player_id = current_player_id(bitboard)\n",
    "    while len(actions):\n",
    "        action = np.random.choice(actions)\n",
    "        result = result_action(bitboard, action, player_id)\n",
    "        if get_winner(result):\n",
    "            actions = np.delete(actions, np.where(actions == action))\n",
    "        else:\n",
    "            return action\n",
    "    return np.random.choice(get_legal_moves(bitboard))  # winning is unavoidable\n",
    "\n",
    "\n",
    "# Actions + Results\n",
    "\n",
    "#@njit\n",
    "def get_next_index(bitboard: np.ndarray, action: int) -> int:\n",
    "    global configuration\n",
    "    # assert is_legal_move(bitboard, action)\n",
    "\n",
    "    # Start at the ground, and return first row that contains a 0\n",
    "    for row in range(configuration.rows-1, -1, -1):\n",
    "        index = action + (row * configuration.columns)\n",
    "        value = (bitboard[0] >> index) & 1\n",
    "        if value == 0:\n",
    "            return index\n",
    "    return action  # this should never happen - implies not is_legal_move(action)\n",
    "\n",
    "#@njit\n",
    "def get_next_row(bitboard: np.ndarray, action: int) -> int:\n",
    "    global configuration\n",
    "    index = get_next_index(bitboard, action)\n",
    "    row   = index // configuration.columns\n",
    "    return row\n",
    "\n",
    "\n",
    "#@njit\n",
    "def result_action(bitboard: np.ndarray, action: int, player_id: int) -> np.ndarray:\n",
    "    # assert is_legal_move(bitboard, action)\n",
    "    index    = get_next_index(bitboard, action)\n",
    "    mark     = 0 if player_id == 1 else 1\n",
    "    output = np.array([\n",
    "        bitboard[0] | 1    << index,\n",
    "        bitboard[1] | mark << index\n",
    "    ], dtype=bitboard.dtype)\n",
    "    return output\n",
    "\n",
    "\n",
    "### Simulations\n",
    "\n",
    "#@njit\n",
    "def run_random_simulation( bitboard: np.ndarray, player_id: int ) -> float:\n",
    "    \"\"\" Returns +1 = victory | 0.5 = draw | 0 = loss \"\"\"\n",
    "    move_number = get_move_number(bitboard)\n",
    "    next_player = 1 if move_number % 2 == 0 else 2  # player 1 has the first move on an empty board\n",
    "    while not is_gameover(bitboard):\n",
    "        actions     = get_legal_moves(bitboard)\n",
    "        action      = np.random.choice(actions)\n",
    "        bitboard    = result_action(bitboard, action, next_player)\n",
    "        next_player = next_player_id(next_player)\n",
    "        # print( bitboard_to_numpy2d(bitboard) )  # DEBUG\n",
    "    score = get_utility_zero_one(bitboard, player_id)\n",
    "    return score\n",
    "\n",
    "\n",
    "### Endgame\n",
    "\n",
    "#@njit(int64[:]())\n",
    "def get_gameovers() -> np.ndarray:\n",
    "    \"\"\"Creates a list of all winning board positions, over 4 directions: horizontal, vertical and 2 diagonals\"\"\"\n",
    "    global configuration\n",
    "\n",
    "    rows    = configuration.rows\n",
    "    columns = configuration.columns\n",
    "    inarow  = configuration.inarow\n",
    "\n",
    "    gameovers = []\n",
    "\n",
    "    mask_horizontal  = 0\n",
    "    mask_vertical    = 0\n",
    "    mask_diagonal_dl = 0\n",
    "    mask_diagonal_ul = 0\n",
    "    for n in range(inarow):  # prange\n",
    "        mask_horizontal  |= 1 << n\n",
    "        mask_vertical    |= 1 << n * columns\n",
    "        mask_diagonal_dl |= 1 << n * columns + n\n",
    "        mask_diagonal_ul |= 1 << n * columns + (inarow - 1 - n)\n",
    "\n",
    "    row_inner = rows    - inarow\n",
    "    col_inner = columns - inarow\n",
    "    for row in range(rows):  # prange\n",
    "        for col in range(columns):  # prange\n",
    "            offset = col + row * columns\n",
    "            if col <= col_inner:\n",
    "                gameovers.append( mask_horizontal << offset )\n",
    "            if row <= row_inner:\n",
    "                gameovers.append( mask_vertical << offset )\n",
    "            if col <= col_inner and row <= row_inner:\n",
    "                gameovers.append( mask_diagonal_dl << offset )\n",
    "                gameovers.append( mask_diagonal_ul << offset )\n",
    "\n",
    "    _get_gameovers_cache = np.array(gameovers, dtype=np.int64)\n",
    "    return _get_gameovers_cache\n",
    "\n",
    "gameovers = get_gameovers()\n",
    "\n",
    "\n",
    "#@njit\n",
    "def is_gameover(bitboard: np.ndarray) -> bool:\n",
    "    if has_no_more_moves(bitboard):  return True\n",
    "    if get_winner(bitboard) != 0:    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#@njit\n",
    "def get_winner(bitboard: np.ndarray) -> int:\n",
    "    \"\"\" Endgame get_winner: 0 for no get_winner, 1 = player 1, 2 = player 2\"\"\"\n",
    "    global gameovers\n",
    "    # gameovers = get_gameovers()\n",
    "    p2_wins = (bitboard[0] &  bitboard[1]) & gameovers == gameovers\n",
    "    if np.any(p2_wins): return 2\n",
    "    p1_wins = (bitboard[0] & ~bitboard[1]) & gameovers == gameovers\n",
    "    if np.any(p1_wins): return 1\n",
    "    return 0\n",
    "\n",
    "    # NOTE: above implementation is 2x faster than this original attempt\n",
    "    # gameovers_played = gameovers[ gameovers & bitboard[0] == gameovers ]  # exclude any unplayed squares\n",
    "    # if np.any(gameovers_played):                                          # have 4 tokens been played in a row yet\n",
    "    #     p1_wins = gameovers_played & ~bitboard[1] == gameovers_played\n",
    "    #     if np.any(p1_wins): return 1\n",
    "    #     p2_wins = gameovers_played &  bitboard[1] == gameovers_played\n",
    "    #     if np.any(p2_wins): return 2\n",
    "    # return 0\n",
    "\n",
    "\n",
    "### Utility Scores\n",
    "\n",
    "#@njit\n",
    "def get_utility_one(bitboard: np.ndarray, player_id: int) -> int:\n",
    "    \"\"\" Like get_utility_inf but returns: 1 for victory, -1 for loss, 0 for draw \"\"\"\n",
    "    winning_player = get_winner(bitboard)\n",
    "    if winning_player == 0: return 0\n",
    "    return 1 if winning_player == player_id else -1\n",
    "\n",
    "\n",
    "#@njit\n",
    "def get_utility_zero_one(bitboard: np.ndarray, player_id: int) -> float:\n",
    "    \"\"\" Like get_utility_one but returns: 1 for victory, 0 for loss, 0.5 for draw \"\"\"\n",
    "    winning_player = get_winner(bitboard)\n",
    "    if winning_player == 0: return 0.5\n",
    "    return 1.0 if winning_player == player_id else 0.0\n",
    "\n",
    "\n",
    "#@njit\n",
    "def get_utility_inf(bitboard: np.ndarray, player_id: int) -> float:\n",
    "    \"\"\" Like get_utility_one but returns: +inf for victory, -inf for loss, 0 for draw \"\"\"\n",
    "    winning_player = get_winner(bitboard)\n",
    "    if winning_player == 0: return 0\n",
    "    return +np.inf if winning_player == player_id else -np.inf\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   core/ConnectXBBNN.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START util/sigmoid.py\n",
    "#####\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(self, value: float):\n",
    "    # self.sigmoid_width == 2 means a heuristic score of +-2 will return +-0.73\n",
    "    # 1 / (1 + math.exp(-(+np.inf))) == 1.0\n",
    "    # 1 / (1 + math.exp(-(4.0)))     == 0.99\n",
    "    # 1 / (1 + math.exp(-(4.0)))     == 0.98\n",
    "    # 1 / (1 + math.exp(-(3.0)))     == 0.95\n",
    "    # 1 / (1 + math.exp(-(2.0)))     == 0.88\n",
    "    # 1 / (1 + math.exp(-(1.0)))     == 0.73\n",
    "    # 1 / (1 + math.exp(-(0.5)))     == 0.62\n",
    "    # 1 / (1 + math.exp(-(0.0)))     == 0.5\n",
    "    # 1 / (1 + math.exp(-(-np.inf))) == 0.0\n",
    "    return 1 / (1 + np.exp(-value))\n",
    "\n",
    "\n",
    "def scaled_sigmoid(value: float, sigmoid_width: float, sigmoid_height: float = 1.0) -> float:\n",
    "    # sigmoid_width  == 2 means a heuristic score of +-2 will return +-0.73\n",
    "    # sigmoid_height is used to distinguish between gameover() == +-1 and heuristic values\n",
    "    return sigmoid_height * (1 / (1 + np.exp(-value / sigmoid_width))) if sigmoid_width else 0\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   util/sigmoid.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START util/base64_file.py\n",
    "#####\n",
    "\n",
    "import base64\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any\n",
    "from typing import Union\n",
    "\n",
    "import dill\n",
    "import humanize\n",
    "\n",
    "\n",
    "# _base64_file__test_base64_static_import = \"\"\"\n",
    "# H4sIAPx9LF8C/2tgri1k0IjgYGBgKCxNLS7JzM8rZIwtZNLwZvBm8mYEkjAI4jFB2KkRbED1iXnF\n",
    "# 5alFhczeWqV6AEGfwmBHAAAA\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "def base64_file_varname(filename: str) -> str:\n",
    "    # ../data/AntColonyTreeSearchNode.dill.zip.base64 -> _base64_file__AntColonyTreeSearchNode__dill__zip__base64\n",
    "    varname = re.sub(r'^.*/',   '',   filename)  # remove directories\n",
    "    varname = re.sub(r'[.\\W]+', '__', varname)   # convert dots and non-ascii to __\n",
    "    varname = f\"_base64_file__{varname}\"\n",
    "    return varname\n",
    "\n",
    "\n",
    "def base64_file_var_wrap(base64_data: Union[str,bytes], varname: str) -> str:\n",
    "    return f'{varname} = \"\"\"\\n{base64_data.strip()}\\n\"\"\"'                    # add varname = \"\"\"\\n\\n\"\"\" wrapper\n",
    "\n",
    "\n",
    "def base64_file_var_unwrap(base64_data: str) -> str:\n",
    "    output = base64_data.strip()\n",
    "    output = re.sub(r'^\\w+ = \"\"\"|\"\"\"$', '', output)  # remove varname = \"\"\" \"\"\" wrapper\n",
    "    output = output.strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def base64_file_encode(data: Any) -> str:\n",
    "    encoded = dill.dumps(data)\n",
    "    encoded = gzip.compress(encoded)\n",
    "    encoded = base64.encodebytes(encoded).decode('utf8').strip()\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def base64_file_decode(encoded: str) -> Any:\n",
    "    data = base64.b64decode(encoded)\n",
    "    data = gzip.decompress(data)\n",
    "    data = dill.loads(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def base64_file_save(data: Any, filename: str, vebose=True) -> float:\n",
    "    \"\"\"\n",
    "        Saves a base64 encoded version of data into filename, with a varname wrapper for importing via kaggle_compile.py\n",
    "        # Doesn't create/update global variable.\n",
    "        Returns filesize in bytes\n",
    "    \"\"\"\n",
    "    varname    = base64_file_varname(filename)\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        with open(filename, 'wb') as file:\n",
    "            encoded = base64_file_encode(data)\n",
    "            output  = base64_file_var_wrap(encoded, varname)\n",
    "            output  = output.encode('utf8')\n",
    "            file.write(output)\n",
    "            file.close()\n",
    "        if varname in globals(): globals()[varname] = encoded  # globals not shared between modules, but update for saftey\n",
    "\n",
    "        filesize = os.path.getsize(filename)\n",
    "        if vebose:\n",
    "            time_taken = time.perf_counter() - start_time\n",
    "            print(f\"base64_file_save(): {filename:40s} | {humanize.naturalsize(filesize)} in {time_taken:4.1f}s\")\n",
    "        return filesize\n",
    "    except Exception as exception:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def base64_file_load(filename: str, vebose=True) -> Union[Any,None]:\n",
    "    \"\"\"\n",
    "        Performs a lookup to see if the global variable for this file alread exists\n",
    "        If not, reads the base64 encoded file from filename, with an optional varname wrapper\n",
    "        # Doesn't create/update global variable.\n",
    "        Returns decoded data\n",
    "    \"\"\"\n",
    "    varname    = base64_file_varname(filename)\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        # Hard-coding PyTorch weights into a script - https://www.kaggle.com/c/connectx/discussion/126678\n",
    "        encoded = None\n",
    "\n",
    "        if varname in globals():\n",
    "            encoded = globals()[varname]\n",
    "\n",
    "        if encoded is None and os.path.exists(filename):\n",
    "            with open(filename, 'rb') as file:\n",
    "                encoded = file.read().decode('utf8')\n",
    "                encoded = base64_file_var_unwrap(encoded)\n",
    "                # globals()[varname] = encoded  # globals are not shared between modules\n",
    "\n",
    "        if encoded is not None:\n",
    "            data = base64_file_decode(encoded)\n",
    "\n",
    "            if vebose:\n",
    "                filesize = os.path.getsize(filename)\n",
    "                time_taken = time.perf_counter() - start_time\n",
    "                print(f\"base64_file_load(): {filename:40s} | {humanize.naturalsize(filesize)} in {time_taken:4.1f}s\")\n",
    "            return data\n",
    "    except Exception as exception:\n",
    "        print(f'base64_file_load({filename}): Exception:', exception)\n",
    "    return None\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   util/base64_file.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START agents/MontyCarlo/MontyCarloPure.py\n",
    "#####\n",
    "\n",
    "# This is a LinkedList implementation of MontyCarlo Tree Search\n",
    "# Inspired by https://www.kaggle.com/matant/monte-carlo-tree-search-connectx\n",
    "import atexit\n",
    "import time\n",
    "from struct import Struct\n",
    "from typing import Callable\n",
    "\n",
    "# from core.ConnectXBBNN import *\n",
    "# from util.base64_file import base64_file_load\n",
    "# from util.base64_file import base64_file_save\n",
    "\n",
    "Hyperparameters = namedtuple('hyperparameters', [])\n",
    "\n",
    "class MontyCarloNode:\n",
    "    persist   = True\n",
    "    save_node = None\n",
    "    root_nodes: List[Union['MontyCarloNode', None]] = [None, None, None]  # root_nodes[observation.mark]\n",
    "\n",
    "    # This ensures we have unique instances of cls.save_node, cls.root_nodes in case multiple subclasses are running simultaniously\n",
    "    @classmethod\n",
    "    def init_class(cls):\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        # create a new cls.save_node and cls.root_nodes for each class\n",
    "        for parentclass in cls.__mro__:  # https://stackoverflow.com/questions/2611892/how-to-get-the-parents-of-a-python-class\n",
    "            if cls is parentclass: continue\n",
    "            if ( cls.save_node  is getattr(parentclass, 'save_node',  None)\n",
    "              or cls.root_nodes is getattr(parentclass, 'root_nodes', None)\n",
    "            ):\n",
    "                cls.save_node  = None\n",
    "                cls.root_nodes = [None, None, None]\n",
    "                break\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            bitboard:      np.ndarray,\n",
    "            player_id:     int,\n",
    "            parent:        Union['MontyCarloNode', None] = None,\n",
    "            parent_action: Union[int,None]       = None,\n",
    "            exploration:   float = 1.0,\n",
    "            **kwargs\n",
    "    ):\n",
    "        self.bitboard      = bitboard\n",
    "        self.player_id     = player_id\n",
    "        self.next_player   = 3 - player_id\n",
    "\n",
    "        self.exploration   = exploration\n",
    "        self.kwargs        = kwargs\n",
    "\n",
    "        # self.mirror_hash   = hash_bitboard(bitboard)  # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "        self.legal_moves   = get_legal_moves(bitboard)\n",
    "        self.is_gameover   = is_gameover(bitboard)\n",
    "        self.winner        = get_winner(bitboard) if self.is_gameover else 0\n",
    "        self.utility       = 1 if self.winner == self.player_id else 0  # Scores in range 0-1\n",
    "\n",
    "        self.parent        = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.is_expanded   = False\n",
    "        self.children: List[Union[MontyCarloNode, None]] = [None for action in get_all_moves()]  # include illegal moves to preserve indexing\n",
    "        self.total_score   = 0.0\n",
    "        self.total_visits  = 0\n",
    "        self.ucb1_score    = self.get_ucb1_score(self)\n",
    "\n",
    "\n",
    "\n",
    "    def __hash__(self):\n",
    "        return tuple(self.bitboard)\n",
    "        # return self.mirror_hash  # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "\n",
    "\n",
    "\n",
    "    ### Loading and Saving\n",
    "\n",
    "    @classmethod\n",
    "    def prune(cls, node: 'MontyCarloNode', min_visits=7, pruned_count=0, total_count=0):\n",
    "        for n, child in enumerate(node.children):\n",
    "            if child is None: continue\n",
    "            if child.total_visits < min_visits:\n",
    "                pruned_count    += child.total_visits  # excepting terminal states, this equals the number of grandchildren\n",
    "                total_count     += child.total_visits  # excepting terminal states, this equals the number of grandchildren\n",
    "                node.children[n] = None\n",
    "                node.is_expanded = False  # Use def expand(self) to reinitalize state\n",
    "            else:\n",
    "                total_count += 1\n",
    "                pruned_count, total_count = cls.prune(child, min_visits, pruned_count, total_count)\n",
    "        return pruned_count, total_count\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def filename(cls):\n",
    "        return f\"data/{cls.__name__}_base64.py\"\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        filename = cls.filename()\n",
    "        loaded   = base64_file_load(filename)\n",
    "        if loaded is not None:\n",
    "            cls.save_node = loaded\n",
    "            return loaded\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls) -> Union[str,None]:\n",
    "        if cls.persist == True and cls.save_node is not None:\n",
    "            save_node    = cls.save_node\n",
    "\n",
    "            start_time   = time.perf_counter()\n",
    "            pruned_count, total_count = cls.prune(save_node)  # This reduces a 47MB base64 file down to 5Mb\n",
    "            print(f'{cls.__name__}.save() - pruned {pruned_count:.0f}/{total_count:.0f} nodes '\n",
    "                  + f'leaving {total_count-pruned_count:.0f} in {time.perf_counter() - start_time:.2f}s - ')\n",
    "\n",
    "            filename = cls.filename()\n",
    "            filesize = base64_file_save(save_node, filename)\n",
    "            return filename\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    ### Constructors and Lookups\n",
    "\n",
    "    def create_child( self, action: int ) -> 'MontyCarloNode':\n",
    "        result = result_action(self.bitboard, action, self.player_id)\n",
    "        child  = None  # self.find_mirror_child(result, depth=1)  # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "        if child is None:\n",
    "            child = self.__class__(\n",
    "                bitboard      = result,\n",
    "                player_id     = next_player_id(self.player_id),\n",
    "                parent        = self,\n",
    "                parent_action = action,\n",
    "                exploration   = self.exploration,\n",
    "                **self.kwargs\n",
    "            )\n",
    "        self.children[action] = child\n",
    "        self.is_expanded      = self._is_expanded()\n",
    "        if self.is_expanded:\n",
    "            self.on_expanded()\n",
    "        return child\n",
    "\n",
    "\n",
    "    def find_child( self, bitboard: np.array, depth=2 ) -> Union['MontyCarloNode', None]:\n",
    "        # assert 0 <= depth <= 2\n",
    "\n",
    "        if depth >= 0:\n",
    "            if np.all( self.bitboard == bitboard ):\n",
    "                return self\n",
    "        if depth >= 1:\n",
    "            for child in self.children:\n",
    "                if child is None: continue\n",
    "                if np.all( child.bitboard == bitboard ):\n",
    "                    return child\n",
    "        if depth >= 2:\n",
    "            # Avoid recursion to prevent duplicate calls to hash_bitboard()\n",
    "            for child in self.children:\n",
    "                if child is None: continue\n",
    "                for grandchild in child.children:\n",
    "                    if grandchild is None: continue\n",
    "                    if np.all( grandchild.bitboard == bitboard ):\n",
    "                        return grandchild\n",
    "        return None\n",
    "\n",
    "    # # BUG: using mirror hashes causes get_best_action() to return invalid moves\n",
    "    # def find_mirror_child( self, bitboard: np.array, depth=2 ) -> Union['MontyCarloNode', None]:\n",
    "    #     # assert 0 <= depth <= 2\n",
    "    #     mirror_hash = hash_bitboard(bitboard)\n",
    "    #\n",
    "    #     if depth >= 0:\n",
    "    #         if self.mirror_hash == mirror_hash:\n",
    "    #             return self\n",
    "    #     if depth >= 1:\n",
    "    #         for child in self.children:\n",
    "    #             if child is None: continue\n",
    "    #             if child.mirror_hash == mirror_hash:\n",
    "    #                 return child\n",
    "    #     if depth >= 2:\n",
    "    #         # Avoid recursion to prevent duplicate calls to hash_bitboard()\n",
    "    #         for child in self.children:\n",
    "    #             if child is None: continue\n",
    "    #             for grandchild in child.children:\n",
    "    #                 if grandchild is None: continue\n",
    "    #                 if grandchild.mirror_hash == mirror_hash:\n",
    "    #                     return grandchild\n",
    "    #     return None\n",
    "\n",
    "\n",
    "\n",
    "    ### Properties\n",
    "\n",
    "    def _is_expanded(self) -> bool:\n",
    "        is_expanded = True\n",
    "        for action in self.legal_moves:\n",
    "            if self.children[action] is None:\n",
    "                is_expanded = False\n",
    "                break\n",
    "        return is_expanded\n",
    "\n",
    "\n",
    "    def get_unexpanded(self) -> List[int]:\n",
    "        return [\n",
    "            action\n",
    "            for action in self.legal_moves\n",
    "            if self.children[action] is None\n",
    "        ]\n",
    "\n",
    "\n",
    "    ### Action Selection\n",
    "\n",
    "    def get_best_action(self) -> int:\n",
    "        scores = [\n",
    "            self.children[action].total_score\n",
    "            if self.children[action] is not None else 0\n",
    "            for action in self.legal_moves\n",
    "        ]\n",
    "        index  = np.argmax(scores)\n",
    "        action = self.legal_moves[index]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_exploration_action(self) -> int:\n",
    "        scores = [\n",
    "            self.children[action].ucb1_score\n",
    "            if self.children[action] is not None else 0\n",
    "            for action in self.legal_moves\n",
    "        ]\n",
    "        index  = np.argmax(scores)\n",
    "        action = self.legal_moves[index]\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "    ### Scores\n",
    "\n",
    "    def get_ucb1_score(self, node: 'MontyCarloNode') -> float:\n",
    "        if node is None or node.total_visits == 0:\n",
    "            return np.inf\n",
    "        else:\n",
    "            score = node.total_score / node.total_visits\n",
    "            if node.parent is not None and node.parent.total_visits > 0:\n",
    "                score += (\n",
    "                    node.exploration * np.sqrt(2)\n",
    "                    * np.log(node.parent.total_visits) / node.total_visits\n",
    "                )\n",
    "            return score\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def opponents_score(score: float):\n",
    "        # assert 0 <= score <= 1\n",
    "        return 1 - score\n",
    "\n",
    "\n",
    "\n",
    "    ### Training and Backpropagation\n",
    "\n",
    "    def single_run(self):\n",
    "        if self.is_gameover:\n",
    "            self.backpropagate(self.utility)\n",
    "        elif not self.is_expanded:\n",
    "            child = self.expand()\n",
    "            score = child.simulate()    # score from the perspective of the other player\n",
    "            child.backpropagate(score)\n",
    "        else:\n",
    "            # Recurse down tree, until a gameover or not expanded node is found\n",
    "            action = self.get_exploration_action()\n",
    "            child  = self.children[action]\n",
    "            child.single_run()\n",
    "\n",
    "\n",
    "    def expand(self) -> 'MontyCarloNode':\n",
    "        # assert not self.is_gameover\n",
    "        # assert not self.is_expanded\n",
    "\n",
    "        unexpanded = self.get_unexpanded()\n",
    "        # assert len(unexpanded)\n",
    "\n",
    "        action     = np.random.choice(unexpanded)\n",
    "        child      = self.create_child(action)\n",
    "        return child\n",
    "\n",
    "    def on_expanded(self) -> None:\n",
    "        # Callback placeholder for any subclass hooks\n",
    "        pass\n",
    "\n",
    "    def simulate(self) -> float:\n",
    "        return run_random_simulation(self.bitboard, self.player_id)\n",
    "\n",
    "\n",
    "    def backpropagate(self, score: float):\n",
    "        # child.simulate()  returns score for the player 2\n",
    "        # child.total_score is accessed via the parent node, so score on this node is from the perspective of player 1\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            score = self.opponents_score(score)\n",
    "            node.total_score  += score\n",
    "            node.total_visits += 1\n",
    "            node = node.parent      # when we reach the root: node.parent == None which terminates\n",
    "\n",
    "        # get_ucb1_score() gets called 4x less often if we cache the value after backpropagation\n",
    "        # get_ucb1_score() depends on parent.total_visits, so needs to be called after updating scores\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            node.ucb1_score = node.get_ucb1_score(node)\n",
    "            node = node.parent      # when we reach the root: node.parent == None which terminates\n",
    "\n",
    "\n",
    "\n",
    "    ### Agent\n",
    "    @classmethod\n",
    "    def agent(cls, **kwargs) -> Callable[[Struct, Struct],int]:\n",
    "        def kaggle_agent( observation: Struct, _configuration_: Struct ):\n",
    "            first_move_time = 0\n",
    "            safety_time     = kwargs.get('safety_time', 0.25)\n",
    "            start_time      = time.perf_counter()\n",
    "            # configuration   = cast_configuration(_configuration_)\n",
    "\n",
    "            player_id     = int(observation.mark)\n",
    "            listboard     = np.array(observation.board, dtype=np.int8)\n",
    "            bitboard      = list_to_bitboard(listboard)\n",
    "            move_number   = get_move_number(bitboard)\n",
    "            is_first_move = int(move_number < 2)\n",
    "          # endtime       = start_time + _configuration_.timeout - safety_time - (first_move_time * is_first_move)\n",
    "            endtime       = start_time +           1.25          - safety_time - (first_move_time * is_first_move)\n",
    "            cls.init_class()  # ensure unique instance of cls.save_node between subclasses\n",
    "            if cls.persist == True and cls.save_node is None:\n",
    "                atexit.register(cls.save)\n",
    "                cls.save_node = cls.load() or cls(empty_bitboard(), player_id=1)\n",
    "                cls.root_nodes[1] = cls.root_nodes[2] = cls.save_node  # implement shared state\n",
    "\n",
    "            root_node = cls.root_nodes[player_id]\n",
    "            if root_node is None or root_node.find_child(bitboard, depth=2) is None:\n",
    "                root_node = cls.root_nodes[player_id] = cls(\n",
    "                    bitboard      = bitboard,\n",
    "                    player_id     = player_id,\n",
    "                    parent        = None,\n",
    "                    # configuration = configuration,\n",
    "                    **kwargs\n",
    "                )\n",
    "            else:\n",
    "                root_node = cls.root_nodes[player_id] = cls.root_nodes[player_id].find_child(bitboard)\n",
    "            # assert root_node is not None\n",
    "\n",
    "            count = 0\n",
    "            while time.perf_counter() < endtime:\n",
    "                count += 1\n",
    "                root_node.single_run()\n",
    "\n",
    "            action     = root_node.get_best_action()\n",
    "            time_taken = time.perf_counter() - start_time\n",
    "            print(f'{cls.__name__}: p{player_id} action = {action} after {count} simulations in {time_taken:.3f}s')\n",
    "            return int(action)\n",
    "\n",
    "        kaggle_agent.__name__ = cls.__name__\n",
    "        return kaggle_agent\n",
    "\n",
    "def MontyCarloPure(**kwargs):\n",
    "    # observation   = {'mark': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    # configuration = {'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8}\n",
    "    def MontyCarloPure(observation: Struct, configuration: Struct) -> int:\n",
    "        return MontyCarloNode.agent(**kwargs)(observation, configuration)\n",
    "    return MontyCarloPure\n",
    "\n",
    "def MontyCarloPureKaggle(observation, configuration):\n",
    "    return MontyCarloPure()(observation, configuration)\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   agents/MontyCarlo/MontyCarloPure.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START heuristics/BitboardGameoversHeuristic.py\n",
    "#####\n",
    "\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# from core.ConnectXBBNN import *\n",
    "# Hyperparameters\n",
    "# from util.sigmoid import scaled_sigmoid\n",
    "\n",
    "single_square_score = 0.1  # Mostly ignore single squares, that can make lines in 8 directions\n",
    "double_attack_score = 0.5  # 0.5 == 100% winrate vs AlphaBetaAgent\n",
    "\n",
    "min_score =  math.inf  # min_score: -32.3\n",
    "max_score = -math.inf  # max_score:  26.4\n",
    "\n",
    "\n",
    "# Profiler (Macbook Pro 2011): This vectorized implementation is actually slower than unvectorized\n",
    "# bitboard_gameovers_heuristic\t                call_count=9469\t    time=1558\town_time=1141\n",
    "# bitboard_gameovers_heuristic_unvectorized\t    call_count=9469\t    time=1879\town_time=1862  ( 20% slower)\n",
    "# bitboard_gameovers_heuristic_slow\t            call_count=9469\t    time=3419\town_time=1893  (220% slower)\n",
    "def bitboard_gameovers_heuristic( bitboard: np.ndarray, player_id: int ) -> float:\n",
    "    \"\"\" For all possible connect4 gameover positions,\n",
    "        check if a player has at least one token in position and that the opponent is not blocking\n",
    "        return difference in score\n",
    "\n",
    "        Winrates:\n",
    "         55% vs AlphaBetaAgent - original heuristic\n",
    "         70% vs AlphaBetaAgent - + np.log2(p1_can_play) % 1 == 0\n",
    "         60% vs AlphaBetaAgent - + double_attack_score=1   without np.log2() (mostly draws)\n",
    "         80% vs AlphaBetaAgent - + double_attack_score=1   with np.log2() (mostly wins)\n",
    "         80% vs AlphaBetaAgent - + double_attack_score=2   with np.log2() (mostly wins)\n",
    "         70% vs AlphaBetaAgent - + double_attack_score=4   with np.log2() (mostly wins)\n",
    "         80% vs AlphaBetaAgent - + double_attack_score=8   with np.log2() (mostly wins)\n",
    "        100% vs AlphaBetaAgent - + double_attack_score=0.5 with np.log2() (mostly wins)\n",
    "    \"\"\"\n",
    "\n",
    "    p1_score = 0.0\n",
    "    p2_score = 0.0\n",
    "\n",
    "    invert_mask = sys.maxsize\n",
    "    p1_tokens   = bitboard[0] & (bitboard[1] ^ invert_mask)\n",
    "    p2_tokens   = bitboard[0] & (bitboard[1])\n",
    "\n",
    "    for n in range(1):  # allow short circuit break statement\n",
    "        p1_bitmasks = p1_tokens & gameovers\n",
    "        p2_bitmasks = p2_tokens & gameovers\n",
    "        p1_wins     = p1_bitmasks == gameovers\n",
    "        p2_wins     = p2_bitmasks == gameovers\n",
    "\n",
    "        # If we have 4 in a row, then immediately return infinity\n",
    "        if np.any(p1_wins):\n",
    "            p1_score = np.inf\n",
    "            break\n",
    "        if np.any(p2_wins):\n",
    "            p2_score = np.inf\n",
    "            break\n",
    "\n",
    "        # Exclude any gameovers that contain moves from both players\n",
    "        # np.log2() % 1 == 0 will be true for any bitmask containing only a single bit\n",
    "        # p1_lines is a shorted array only containing matching gameover masks\n",
    "        overlaps            = (p1_bitmasks != 0) & (p2_bitmasks != 0)\n",
    "        p1_is_valid         = (p1_bitmasks != 0) & ~overlaps\n",
    "        p2_is_valid         = (p2_bitmasks != 0) & ~overlaps\n",
    "        p1_is_single_square = p1_is_valid & ( np.log2(p1_bitmasks, where=p1_is_valid ) % 1 == 0 )\n",
    "        p2_is_single_square = p2_is_valid & ( np.log2(p2_bitmasks, where=p2_is_valid ) % 1 == 0 )\n",
    "\n",
    "        p1_lines            = gameovers[ p1_is_valid & ~p1_is_single_square ]\n",
    "        p2_lines            = gameovers[ p2_is_valid & ~p2_is_single_square ]\n",
    "\n",
    "        # Offer a reduced score for any bitmask containing only a single bit\n",
    "        p1_score += p1_lines.size + np.count_nonzero(p1_is_single_square) * single_square_score\n",
    "        p2_score += p2_lines.size + np.count_nonzero(p2_is_single_square) * single_square_score\n",
    "\n",
    "        # NOTE: Turns out that trying to be clever and creating a square matrix using np.roll() is actually slower!\n",
    "        if p1_lines.size >= 2:\n",
    "            for n1 in range(p1_lines.size):\n",
    "                for n2 in range(n1+1, p1_lines.size):\n",
    "                    gameover1 = p1_lines[n1]\n",
    "                    gameover2 = p1_lines[n2]\n",
    "                    overlap   = gameover1 & gameover2\n",
    "                    if overlap == 0:              continue  # Ignore no overlap\n",
    "                    if np.log2(overlap) % 1 != 0: continue  # Only count double_attacks with a single overlap square\n",
    "                    p1_score += double_attack_score\n",
    "        if p2_lines.size >= 2:\n",
    "            for n1 in range(p2_lines.size):\n",
    "                for n2 in range(n1+1, p2_lines.size):\n",
    "                    gameover1 = p2_lines[n1]\n",
    "                    gameover2 = p2_lines[n2]\n",
    "                    overlap   = gameover1 & gameover2\n",
    "                    if overlap == 0:              continue  # Ignore no overlap\n",
    "                    if np.log2(overlap) % 1 != 0: continue  # Only count double_attacks with a single overlap square\n",
    "                    p2_score += double_attack_score\n",
    "\n",
    "    score = (p1_score - p2_score) if player_id == 1 else (p2_score - p1_score)\n",
    "    # # assert np.math.isclose( score, bitboard_gameovers_heuristic_unvectorized(bitboard, player_id, gameovers), abs_tol=0.01), f'{score} != {bitboard_gameovers_heuristic_unvectorized(bitboard, player_id, gameovers)}'\n",
    "\n",
    "    # global min_score, max_score\n",
    "    # if score < min_score: min_score = score; print(f'min_score: {min_score}')  # min_score: -32.3\n",
    "    # if score > max_score: max_score = score; print(f'max_score: {max_score}')  # max_score:  26.4\n",
    "    return score\n",
    "\n",
    "\n",
    "def bitboard_gameovers_heuristic_sigmoid(sigmoid_width = 6.0, sigmoid_height = 1.0):\n",
    "    def _bitboard_gameovers_heuristic_sigmoid( bitboard: np.ndarray, player_id: int ) -> float:\n",
    "        score = bitboard_gameovers_heuristic(bitboard, player_id)\n",
    "        score = scaled_sigmoid(score, sigmoid_width, sigmoid_height)\n",
    "        return score\n",
    "    return _bitboard_gameovers_heuristic_sigmoid\n",
    "\n",
    "\n",
    "### Previous implementations of the above code\n",
    "\n",
    "# @njit\n",
    "def bitboard_gameovers_heuristic_unvectorized( bitboard: np.ndarray, player_id: int, gameovers: np.ndarray = get_gameovers() ) -> float:\n",
    "    invert_mask = sys.maxsize\n",
    "    p1_tokens   = bitboard[0] & (bitboard[1] ^ invert_mask)\n",
    "    p2_tokens   = bitboard[0] & (bitboard[1])\n",
    "\n",
    "    p1_score = 0.0\n",
    "    p2_score = 0.0\n",
    "    p1_gameovers = []\n",
    "    p2_gameovers = []\n",
    "    for gameover in gameovers:\n",
    "        p1_can_play = p1_tokens & gameover\n",
    "        p2_can_play = p2_tokens & gameover\n",
    "        if p1_can_play and not p2_can_play:\n",
    "            if   p1_can_play == gameover:       p1_score += np.inf                # Connect 4\n",
    "            elif np.log2(p1_can_play) % 1 == 0: p1_score += single_square_score   # Mostly ignore single square lines\n",
    "            else:                               p1_score += 1; p1_gameovers.append(gameover);\n",
    "        elif p2_can_play and not p1_can_play:\n",
    "            if   p2_can_play == gameover:       p2_score += np.inf                # Connect 4\n",
    "            elif np.log2(p2_can_play) % 1 == 0: p2_score += single_square_score   # Mostly ignore single square lines\n",
    "            else:                               p2_score += 1; p2_gameovers.append(gameover);\n",
    "\n",
    "\n",
    "    for n1 in range(len(p1_gameovers)):\n",
    "        for n2 in range(n1+1, len(p1_gameovers)):\n",
    "            gameover1 = p1_gameovers[n1]\n",
    "            gameover2 = p1_gameovers[n2]\n",
    "            overlap = gameover1 & gameover2\n",
    "            if gameover1 == gameover2:    continue  # Ignore self\n",
    "            if overlap == 0:              continue  # Ignore no overlap\n",
    "            if np.log2(overlap) % 1 != 0: continue  # Only count double_attacks with a single overlap square\n",
    "            p1_score += double_attack_score\n",
    "    for n1 in range(len(p2_gameovers)):\n",
    "        for n2 in range(n1+1, len(p2_gameovers)):\n",
    "            gameover1 = p2_gameovers[n1]\n",
    "            gameover2 = p2_gameovers[n2]\n",
    "            overlap = gameover1 & gameover2\n",
    "            if gameover1 == gameover2:    continue  # Ignore self\n",
    "            if overlap == 0:              continue  # Ignore no overlap\n",
    "            if np.log2(overlap) % 1 != 0: continue  # Only count double_attacks with a single overlap square\n",
    "            p2_score += double_attack_score\n",
    "\n",
    "    if player_id == 1:\n",
    "        return p1_score - p2_score\n",
    "    else:\n",
    "        return p2_score - p1_score\n",
    "\n",
    "\n",
    "\n",
    "# Profiler (Macbook Pro 2011): This vectorized implementation is actually slower than unvectorized\n",
    "# bitboard_gameovers_heuristic\t                call_count=9469\t    time=1558\town_time=1141\n",
    "# bitboard_gameovers_heuristic_unvectorized\t    call_count=9469\t    time=1879\town_time=1862  ( 20% slower)\n",
    "# bitboard_gameovers_heuristic_slow\t            call_count=9469\t    time=3419\town_time=1893  (220% slower)\n",
    "def bitboard_gameovers_heuristic_slow( bitboard: np.ndarray, player_id: int, gameovers: np.ndarray = get_gameovers() ) -> float:\n",
    "\n",
    "    # Hyperparameters\n",
    "    single_square_score = 0.1  # Mostly ignore single squares, that can make lines in 8 directions\n",
    "    double_attack_score = 0.5  # 0.5 == 100% winrate vs AlphaBetaAgent\n",
    "\n",
    "    scores = np.array([0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "    invert_mask = sys.maxsize\n",
    "    tokens = np.array([\n",
    "        bitboard[0] & (bitboard[1] ^ invert_mask),\n",
    "        bitboard[0] & (bitboard[1]),\n",
    "    ])\n",
    "    for _ in range(1):  # allow short circuit break statement\n",
    "        bitmasks = np.array([ token & gameovers for token in tokens ])\n",
    "        wins     = bitmasks == gameovers\n",
    "        if np.any(wins):\n",
    "            if np.any(wins[0]): scores[0] = np.inf\n",
    "            if np.any(wins[1]): scores[1] = np.inf\n",
    "            break\n",
    "\n",
    "        is_overlap = (bitmasks[0] != 0) & (bitmasks[1] != 0)\n",
    "        is_valid   = (bitmasks    != 0) & ~is_overlap\n",
    "        if not np.any(is_valid):\n",
    "            break\n",
    "\n",
    "        is_single_square = is_valid & ( np.log2(bitmasks, where=is_valid ) % 1 == 0 )\n",
    "        is_multi_line    = is_valid & ~is_single_square\n",
    "        scores += np.count_nonzero(is_multi_line, axis=-1) + np.count_nonzero(is_single_square, axis=-1) * single_square_score\n",
    "        pass\n",
    "\n",
    "        lines = [\n",
    "            gameovers[ is_multi_line[0] ],\n",
    "            gameovers[ is_multi_line[1] ]\n",
    "        ]\n",
    "        for player in range(len(lines)):\n",
    "            player_lines = lines[player]\n",
    "            if len(player_lines) <= 1: continue\n",
    "            for n in range(player_lines.size):\n",
    "                gameover = player_lines[n]\n",
    "                overlaps = gameover & player_lines[n+1:]\n",
    "                overlaps = overlaps[ (overlaps != 0) & (overlaps != gameover) ]  # Ignore empty and self\n",
    "                double_attacks = np.count_nonzero(np.log2(overlaps) % 1 == 0)    # Only count double_attacks with a single overlap square\n",
    "                scores[player] += double_attacks * double_attack_score\n",
    "\n",
    "    score = (scores[0] - scores[1]) if player_id == 1 else (scores[1] - scores[0])\n",
    "    # # assert np.math.isclose( score, bitboard_gameovers_heuristic_unvectorized(bitboard, player_id, gameovers), abs_tol=0.01), f'{score} != {bitboard_gameovers_heuristic_unvectorized(bitboard, player_id, gameovers)}'\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   heuristics/BitboardGameoversHeuristic.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START agents/MontyCarlo/MontyCarloHeuristic.py\n",
    "#####\n",
    "\n",
    "# This is a LinkedList implementation of MontyCarlo Tree Search\n",
    "# but using bitboard_gameovers_heuristic() instead of run_random_simulation()\n",
    "# Inspired by https://www.kaggle.com/matant/monte-carlo-tree-search-connectx\n",
    "from struct import Struct\n",
    "from typing import Callable\n",
    "from typing import Dict\n",
    "\n",
    "# from agents.MontyCarlo.MontyCarloPure import MontyCarloNode\n",
    "# from core.ConnectXBBNN import *\n",
    "# from heuristics.BitboardGameoversHeuristic import bitboard_gameovers_heuristic_sigmoid\n",
    "\n",
    "Hyperparameters = namedtuple('hyperparameters', [])\n",
    "\n",
    "class MontyCarloHeuristicNode(MontyCarloNode):\n",
    "    heuristic_fn   =  bitboard_gameovers_heuristic_sigmoid\n",
    "    heuristic_args = {}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            bitboard:      np.ndarray,\n",
    "            player_id:     int,\n",
    "            parent:        Union['MontyCarloNode', None] = None,\n",
    "            parent_action: Union[int,None]              = None,\n",
    "            exploration:   float = 1.0,\n",
    "            heuristic_fn:  Callable = None,\n",
    "            heuristic_args: Dict    = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            bitboard        = bitboard,\n",
    "            player_id       = player_id,\n",
    "            parent          = parent,\n",
    "            parent_action   = parent_action,\n",
    "            exploration     = exploration,\n",
    "            heuristic_fn    = heuristic_fn,\n",
    "            heuristic_args  = heuristic_args,\n",
    "            **kwargs\n",
    "        )\n",
    "        # self.kwargs[] needs to be defined in order to pass arguments down to child nodes\n",
    "        self.kwargs['heuristic_fn']   = self.heuristic_fn   = heuristic_fn   or self.__class__.heuristic_fn\n",
    "        self.kwargs['heuristic_args'] = self.heuristic_args = heuristic_args or self.__class__.heuristic_args\n",
    "\n",
    "    def simulate(self) -> float:\n",
    "        self.heuristic = getattr(self, 'heuristic', self.heuristic_fn(**(self.heuristic_args or {})))\n",
    "        score = self.heuristic(self.bitboard, self.player_id)\n",
    "        return score\n",
    "\n",
    "    # BUGFIX: pickle() throws exceptions for functions as local properties\n",
    "    @classmethod\n",
    "    def prune(cls, node: 'MontyCarloNode', *args, **kwargs):\n",
    "        node.heuristic = None  # remove from pickle, will be restored by simulate()\n",
    "        return super().prune(node, *args, **kwargs)\n",
    "\n",
    "\n",
    "def MontyCarloHeuristic(**kwargs):\n",
    "    # observation   = {'mark': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    # configuration = {'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8}\n",
    "    def MontyCarloHeuristic(observation: Struct, configuration: Struct) -> int:\n",
    "        return MontyCarloHeuristicNode.agent(**kwargs)(observation, configuration)\n",
    "    return MontyCarloHeuristic\n",
    "\n",
    "def MontyCarloHeuristicKaggle(observation, configuration):\n",
    "    return MontyCarloHeuristic()(observation, configuration)\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   agents/MontyCarlo/MontyCarloHeuristic.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START heuristics/BitsquaresHeuristic.py\n",
    "#####\n",
    "\n",
    "# from core.ConnectXBBNN import *\n",
    "\n",
    "\n",
    "# Scores vs bitboard_gameovers_heuristic():\n",
    "#   62% winrate @ reward_power=1\n",
    "#   78% winrate @ reward_power=1.25\n",
    "#   81% winrate @ reward_power=1.5\n",
    "#   94% winrate @ reward_power=1.75\n",
    "#   48% winrate @ reward_power=2\n",
    "#   64% winrate @ reward_power=2.5\n",
    "#   69% winrate @ reward_power=3\n",
    "#   25% winrate @ reward_power=4\n",
    "# from util.sigmoid import scaled_sigmoid\n",
    "\n",
    "\n",
    "def bitsquares_heuristic(reward_power=1.75):\n",
    "    def _bitsquares_heuristic(bitboard: np.ndarray, player_id: int, playable_lines = None):\n",
    "        playable_lines = playable_lines or get_playable_lines_by_length(bitboard)\n",
    "        scores = [ 0, 0 ]\n",
    "        for player in [0,1]:\n",
    "            for n in range(1, configuration.inarow+1):\n",
    "                scores[player] += len(playable_lines[player][n]) * (n ** reward_power)\n",
    "\n",
    "        score = (scores[0] - scores[1]) if player_id == 1 else (scores[1] - scores[0])\n",
    "        return score\n",
    "    return _bitsquares_heuristic\n",
    "\n",
    "\n",
    "# Scores for: *sigmoid_width=1.75**: 1-in-a-row = 1 | 2-in-a-row = 3.4 | 3-in-a-row = 6.8\n",
    "def bitsquares_heuristic_sigmoid(reward_power=1.75, sigmoid_width=7.0, sigmoid_height=1.0):\n",
    "    heuristic = bitsquares_heuristic(reward_power=reward_power)\n",
    "    def _bitsquares_heuristic_sigmoid(bitboard: np.ndarray, player_id: int, playable_lines = None) -> float:\n",
    "        score = heuristic(bitboard=bitboard, player_id=player_id, playable_lines=playable_lines)\n",
    "        score = scaled_sigmoid(score, sigmoid_width, sigmoid_height)\n",
    "        return score\n",
    "    return _bitsquares_heuristic_sigmoid\n",
    "\n",
    "\n",
    "### Utility Functions\n",
    "\n",
    "def get_playable_lines(bitboard: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    played_squares = bitboard[0]\n",
    "    empty_squares  = mask_board  & ~bitboard[0]\n",
    "    p1_tokens      = bitboard[0] & ~bitboard[1]\n",
    "    p2_tokens      = bitboard[0] &  bitboard[1]\n",
    "\n",
    "    # find gameovers masks that could be filled with empty squares\n",
    "    p1_playable_lines = gameovers[:] & (p1_tokens & ~p2_tokens | empty_squares)\n",
    "    p2_playable_lines = gameovers[:] & (p2_tokens & ~p1_tokens | empty_squares)\n",
    "    p1_is_valid = (p1_playable_lines[:] == gameovers[:]) & (p1_playable_lines[:] & played_squares != 0)\n",
    "    p2_is_valid = (p2_playable_lines[:] == gameovers[:]) & (p2_playable_lines[:] & played_squares != 0)\n",
    "\n",
    "    p1_playable_lines = p1_playable_lines[p1_is_valid]\n",
    "    p2_playable_lines = p2_playable_lines[p2_is_valid]\n",
    "    return p1_playable_lines, p2_playable_lines\n",
    "\n",
    "\n",
    "def get_playable_lines_by_length(bitboard: np.ndarray) -> List[List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Returns outputs[player][length] = gameovers[bitcount == length]\n",
    "    \"\"\"\n",
    "    outputs        = [ [], [] ]\n",
    "    player_tokens  = [ bitboard[0] & ~bitboard[1], bitboard[0] &  bitboard[1] ]\n",
    "    playable_lines = get_playable_lines(bitboard)\n",
    "    played_bits    = [ player_tokens[0] & playable_lines[0], player_tokens[1] & playable_lines[1] ]\n",
    "    inarow         = configuration.inarow\n",
    "\n",
    "    for player in [0,1]:\n",
    "        is_gameover  = (playable_lines[player][:] == played_bits[player][:])\n",
    "        is_singlebit = np.log2(played_bits[player][:]) % 1 == 0\n",
    "        is_multibit  = ~is_gameover[:] & ~is_singlebit[:]\n",
    "        bitcounts    = np.array([\n",
    "            np.count_nonzero(\n",
    "                bitcount_mask[:] & playable_lines[player][n] & player_tokens[player]\n",
    "            ) if is_multibit[n]\n",
    "            # else 1      if is_singlebit[n]\n",
    "            # else inarow if is_gameover[n]\n",
    "            else 0\n",
    "            for n in range(len(playable_lines[player]))\n",
    "        ])\n",
    "\n",
    "        outputs[player]         = [ np.array([], dtype=np.int64) for _ in range(inarow + 1) ]\n",
    "        outputs[player][1]      = playable_lines[player][ is_singlebit ]\n",
    "        outputs[player][inarow] = playable_lines[player][ is_gameover  ]\n",
    "        for n in range(2, inarow):\n",
    "            outputs[player][n]  = playable_lines[player][ bitcounts == n ]\n",
    "    return outputs\n",
    "\n",
    "#####\n",
    "##### END   heuristics/BitsquaresHeuristic.py\n",
    "#####\n",
    "\n",
    "#####\n",
    "##### START agents/MontyCarlo/MontyCarloBitsquares.py\n",
    "#####\n",
    "\n",
    "# This is a LinkedList implementation of MontyCarlo Tree Search\n",
    "# but using bitboard_gameovers_heuristic() instead of run_random_simulation()\n",
    "# Inspired by https://www.kaggle.com/matant/monte-carlo-tree-search-connectx\n",
    "from struct import Struct\n",
    "\n",
    "# from agents.MontyCarlo.MontyCarloHeuristic import MontyCarloHeuristicNode\n",
    "# from core.ConnectXBBNN import *\n",
    "# from heuristics.BitsquaresHeuristic import bitsquares_heuristic_sigmoid\n",
    "\n",
    "Hyperparameters = namedtuple('hyperparameters', [])\n",
    "\n",
    "class MontyCarloBitsquaresNode(MontyCarloHeuristicNode):\n",
    "    heuristic_fn   = bitsquares_heuristic_sigmoid\n",
    "    heuristic_args = {}  # reward_power=1.75, sigmoid_width=7.0, sigmoid_height=1.0\n",
    "\n",
    "class MontyCarloBitsquaresNode2(MontyCarloBitsquaresNode):\n",
    "    pass\n",
    "\n",
    "\n",
    "def MontyCarloBitsquares(**kwargs):\n",
    "    # observation   = {'mark': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    # configuration = {'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8}\n",
    "    def MontyCarloBitsquares(observation: Struct, configuration: Struct) -> int:\n",
    "        return MontyCarloBitsquaresNode.agent(**kwargs)(observation, configuration)\n",
    "    return MontyCarloBitsquares\n",
    "\n",
    "def MontyCarlo_BtS_JMcGuigan(observation, configuration):\n",
    "    return MontyCarloBitsquares()(observation, configuration)\n",
    "\n",
    "\n",
    "#####\n",
    "##### END   agents/MontyCarlo/MontyCarloBitsquares.py\n",
    "#####\n",
    "\n",
    "##### \n",
    "##### /ai-games/kaggle_compile.py agents/MontyCarlo/MontyCarloBitsquares.py\n",
    "##### \n",
    "##### 2024-01-28 08:30:57+00:00\n",
    "##### \n",
    "##### origin\thttps://github.com/JamesMcGuigan/ai-games.git (fetch)\n",
    "##### origin\thttps://github.com/JamesMcGuigan/ai-games.git (push)\n",
    "##### \n",
    "##### * master eac436d Revert: Rock Paper Scissors | Random Seed Search | tweak unit tests\n",
    "##### \n",
    "##### eac436d23e03624c2245838138e9608cb13d2f1f\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984af441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.334785Z",
     "iopub.status.busy": "2024-02-03T19:25:15.334339Z",
     "iopub.status.idle": "2024-02-03T19:25:15.356018Z",
     "shell.execute_reply": "2024-02-03T19:25:15.354835Z"
    },
    "papermill": {
     "duration": 0.045451,
     "end_time": "2024-02-03T19:25:15.359068",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.313617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run MCTS_Bitboard_Bitsquares_Heuristic_JamesMcGuigan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d969c5",
   "metadata": {
    "papermill": {
     "duration": 0.017501,
     "end_time": "2024-02-03T19:25:15.395717",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.378216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [The Fast mini Max **II**](https://www.kaggle.com/code/martynovandrey/the-fast-mini-max-ii) - [*Martynov Andrey*](https://www.kaggle.com/martynovandrey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1ae34a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.436312Z",
     "iopub.status.busy": "2024-02-03T19:25:15.435876Z",
     "iopub.status.idle": "2024-02-03T19:25:15.460691Z",
     "shell.execute_reply": "2024-02-03T19:25:15.459204Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.049647,
     "end_time": "2024-02-03T19:25:15.463914",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.414267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agent_The_Fast_mini_Max_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent_The_Fast_mini_Max_2.py\n",
    "\n",
    "def agent_TheFast_mini_Max_2(obs, config):\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import time\n",
    "\n",
    "    # Precalculate some constants for better performance\n",
    "    W = config.inarow\n",
    "    R = config.rows\n",
    "    C = config.columns\n",
    "    Rp1 = R + 1\n",
    "    Rm1 = R - 1\n",
    "    RmW = R - W\n",
    "    RmWp1 = R - W + 1\n",
    "    Cp1 = C + 1\n",
    "    Cm1 = C - 1\n",
    "    Rp1Cp1 = Rp1 * Cp1\n",
    "    Ct2 = C * 2\n",
    "    Cp1t2 = Cp1 * 2\n",
    "    Cm1t2 = Cm1 * 2\n",
    "    Wmask = 2**W - 1\n",
    "    DELAY = 0.5\n",
    "   \n",
    "    # the order of calculation the scores of a move, nearest to the center first, \n",
    "    # in the case of 7 columns it's [3, 2, 4, 1, 5, 0, 6]\n",
    "    ORDER = [C//2 - i//2 - 1 if i%2 else C//2 + i//2 for i in range(C)]\n",
    "    \n",
    "    # Some utility functions\n",
    "    \n",
    "    def step_number(grid):\n",
    "        # returns step of the game\n",
    "        empty = 0\n",
    "        for i in range(R):\n",
    "            for j in range(C):\n",
    "                if grid[i][j] == 0:\n",
    "                    empty += 1\n",
    "        return R*C - empty\n",
    "\n",
    "    def is_first_player(step):\n",
    "        return step%2 == 0\n",
    "    \n",
    "    def int_to_bin(x):\n",
    "        # 64-bit integer to 64-letter string\n",
    "        return bin(x)[2:].zfill(64)\n",
    "    \n",
    "    \n",
    "    class State:\n",
    "        '''\n",
    "        Store a board as dict of two integers\n",
    "        pos[1] - position of ones\n",
    "        pos[2] - position of twos\n",
    "        '''\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.pos = dict()\n",
    "\n",
    "        def valid_moves(self):\n",
    "            mask = self.pos[1] | self.pos[2]\n",
    "            return [i for i in ORDER if not (mask >> Rm1+C*i) & 1]         \n",
    "\n",
    "        def from_grid(grid):\n",
    "            # converts 2-D array of 0, 1, 2 (\"grid\") to state\n",
    "            state = State()\n",
    "            matrix = np.array([0 for i in range(Rp1Cp1)]).reshape(Rp1, Cp1)\n",
    "            for r in range(R):\n",
    "                for c in range(C):\n",
    "                    matrix[r+1, c] = grid[r, c]\n",
    "            position1, position2 = '', ''\n",
    "            for c in range(C, -1, -1):\n",
    "                for r in range(0, Rp1):\n",
    "                    position1 += ['0', '1'][matrix[r,c] == 1]\n",
    "                    position2 += ['0', '1'][matrix[r,c] == 2]\n",
    "            state.pos[1] = int(position1, 2)\n",
    "            state.pos[2] = int(position2, 2)\n",
    "            return state\n",
    "\n",
    "        def to_grid(self):\n",
    "            position1 = int_to_bin(self.pos[1])\n",
    "            position2 = int_to_bin(self.pos[2])\n",
    "            matrix = np.array([0 for i in range(Rp1Cp1)]).reshape(Rp1, Cp1)\n",
    "            for c in range(0, Cp1):\n",
    "                for r in range(R, -1, -1):\n",
    "                    if position1[-1] == '1':\n",
    "                        matrix[r,c] = 1\n",
    "                    position1 = position1[:-1]\n",
    "                    if position2[-1] == '1':\n",
    "                        matrix[r,c] = 2\n",
    "                    position2 = position2[:-1]\n",
    "            return matrix[1:, :-1]\n",
    "\n",
    "        def connected_four(self, mark):\n",
    "            # checks if the state has for in a row of mark\n",
    "            position = self.pos[mark]\n",
    "            # Horizontal check\n",
    "            m = position & (position >> C)\n",
    "            if m & (m >> Ct2): return True\n",
    "            # Diagonal \\\n",
    "            m = position & (position >> Cm1)\n",
    "            if m & (m >> Cm1t2): return True\n",
    "            # Diagonal /\n",
    "            m = position & (position >> Cp1)\n",
    "            if m & (m >> Cp1t2): return True\n",
    "            # Vertical\n",
    "            m = position & (position >> 1)\n",
    "            if m & (m >> 2): return True\n",
    "            # Nothing found\n",
    "            return False\n",
    "\n",
    "        def make_move(self, col, mark):\n",
    "            # returns state after playing piece mark in column col\n",
    "            mask = self.pos[1] | self.pos[2]\n",
    "            new_state = State()\n",
    "            new_mask = mask | (mask + (1 << (col*C)))\n",
    "            new_state.pos[mark] = self.pos[3-mark] ^ new_mask\n",
    "            new_state.pos[3-mark] = self.pos[3-mark]\n",
    "            return new_state\n",
    "\n",
    "        def doublewin(self, mark):\n",
    "            # helper for heuristic, checks if player has\n",
    "            # two winning moves simultaneously or two in a row in the \n",
    "            # same column\n",
    "            win = 0\n",
    "            for col in self.valid_moves():\n",
    "                next_state = self.make_move(col, mark)\n",
    "                if next_state.connected_four(mark):\n",
    "                    next_state_opp =  self.make_move(col, 3-mark)\n",
    "                    if next_state_opp.make_move(col, mark).connected_four(mark):\n",
    "                        return True # win with this column first or second move, unstoppable\n",
    "                    win += 1\n",
    "                    if win == 2:\n",
    "                        return True # more than one winning move\n",
    "            return False \n",
    "        \n",
    "\n",
    "        def num23(self, mark):\n",
    "            MASKS = [{4: 60, 3: [28, 44, 52, 56], 2: [12, 20, 24, 36, 40, 48]}, {4: 30, 3: [14, 22, 26, 28], 2: [6, 10, 12, 18, 20, 24]}, {4: 15, 3: [7, 11, 13, 14], 2: [3, 5, 6, 9, 10, 12]}, {4: 7680, 3: [3584, 5632, 6656, 7168], 2: [1536, 2560, 3072, 4608, 5120, 6144]}, {4: 3840, 3: [1792, 2816, 3328, 3584], 2: [768, 1280, 1536, 2304, 2560, 3072]}, {4: 1920, 3: [896, 1408, 1664, 1792], 2: [384, 640, 768, 1152, 1280, 1536]}, {4: 983040, 3: [458752, 720896, 851968, 917504], 2: [196608, 327680, 393216, 589824, 655360, 786432]}, {4: 491520, 3: [229376, 360448, 425984, 458752], 2: [98304, 163840, 196608, 294912, 327680, 393216]}, {4: 245760, 3: [114688, 180224, 212992, 229376], 2: [49152, 81920, 98304, 147456, 163840, 196608]}, {4: 125829120, 3: [58720256, 92274688, 109051904, 117440512], 2: [25165824, 41943040, 50331648, 75497472, 83886080, 100663296]}, {4: 62914560, 3: [29360128, 46137344, 54525952, 58720256], 2: [12582912, 20971520, 25165824, 37748736, 41943040, 50331648]}, {4: 31457280, 3: [14680064, 23068672, 27262976, 29360128], 2: [6291456, 10485760, 12582912, 18874368, 20971520, 25165824]}, {4: 16106127360, 3: [7516192768, 11811160064, 13958643712, 15032385536], 2: [3221225472, 5368709120, 6442450944, 9663676416, 10737418240, 12884901888]}, {4: 8053063680, 3: [3758096384, 5905580032, 6979321856, 7516192768], 2: [1610612736, 2684354560, 3221225472, 4831838208, 5368709120, 6442450944]}, {4: 4026531840, 3: [1879048192, 2952790016, 3489660928, 3758096384], 2: [805306368, 1342177280, 1610612736, 2415919104, 2684354560, 3221225472]}, {4: 2061584302080, 3: [962072674304, 1511828488192, 1786706395136, 1924145348608], 2: [412316860416, 687194767360, 824633720832, 1236950581248, 1374389534720, 1649267441664]}, {4: 1030792151040, 3: [481036337152, 755914244096, 893353197568, 962072674304], 2: [206158430208, 343597383680, 412316860416, 618475290624, 687194767360, 824633720832]}, {4: 515396075520, 3: [240518168576, 377957122048, 446676598784, 481036337152], 2: [103079215104, 171798691840, 206158430208, 309237645312, 343597383680, 412316860416]}, {4: 263882790666240, 3: [123145302310912, 193514046488576, 228698418577408, 246290604621824], 2: [52776558133248, 87960930222080, 105553116266496, 158329674399744, 175921860444160, 211106232532992]}, {4: 131941395333120, 3: [61572651155456, 96757023244288, 114349209288704, 123145302310912], 2: [26388279066624, 43980465111040, 52776558133248, 79164837199872, 87960930222080, 105553116266496]}, {4: 65970697666560, 3: [30786325577728, 48378511622144, 57174604644352, 61572651155456], 2: [13194139533312, 21990232555520, 26388279066624, 39582418599936, 43980465111040, 52776558133248]}, {4: 67637280, 3: [67637248, 67633184, 67112992, 528416], 2: [67633152, 67112960, 528384, 67108896, 524320, 4128]}, {4: 8657571840, 3: [8657567744, 8657047552, 8590462976, 67637248], 2: [8657043456, 8590458880, 67633152, 8589938688, 67112960, 528384]}, {4: 1108169195520, 3: [1108168671232, 1108102086656, 1099579260928, 8657567744], 2: [1108101562368, 1099578736640, 8657043456, 1099512152064, 8590458880, 67633152]}, {4: 141845657026560, 3: [141845589917696, 141837067091968, 140746145398784, 1108168671232], 2: [141836999983104, 140746078289920, 1108101562368, 140737555464192, 1099578736640, 8657043456]}, {4: 33818640, 3: [33818624, 33816592, 33556496, 264208], 2: [33816576, 33556480, 264192, 33554448, 262160, 2064]}, {4: 4328785920, 3: [4328783872, 4328523776, 4295231488, 33818624], 2: [4328521728, 4295229440, 33816576, 4294969344, 33556480, 264192]}, {4: 554084597760, 3: [554084335616, 554051043328, 549789630464, 4328783872], 2: [554050781184, 549789368320, 4328521728, 549756076032, 4295229440, 33816576]}, {4: 70922828513280, 3: [70922794958848, 70918533545984, 70373072699392, 554084335616], 2: [70918499991552, 70373039144960, 554050781184, 70368777732096, 549789368320, 4328521728]}, {4: 16909320, 3: [16909312, 16908296, 16778248, 132104], 2: [16908288, 16778240, 132096, 16777224, 131080, 1032]}, {4: 2164392960, 3: [2164391936, 2164261888, 2147615744, 16909312], 2: [2164260864, 2147614720, 16908288, 2147484672, 16778240, 132096]}, {4: 277042298880, 3: [277042167808, 277025521664, 274894815232, 2164391936], 2: [277025390592, 274894684160, 2164260864, 274878038016, 2147614720, 16908288]}, {4: 35461414256640, 3: [35461397479424, 35459266772992, 35186536349696, 277042167808], 2: [35459249995776, 35186519572480, 277025390592, 35184388866048, 274894684160, 2164260864]}, {4: 8454660, 3: [8454656, 8454148, 8389124, 66052], 2: [8454144, 8389120, 66048, 8388612, 65540, 516]}, {4: 1082196480, 3: [1082195968, 1082130944, 1073807872, 8454656], 2: [1082130432, 1073807360, 8454144, 1073742336, 8389120, 66048]}, {4: 138521149440, 3: [138521083904, 138512760832, 137447407616, 1082195968], 2: [138512695296, 137447342080, 1082130432, 137439019008, 1073807360, 8454144]}, {4: 17730707128320, 3: [17730698739712, 17729633386496, 17593268174848, 138521083904], 2: [17729624997888, 17593259786240, 138512695296, 17592194433024, 137447342080, 1082130432]}, {4: 4227330, 3: [4227328, 4227074, 4194562, 33026], 2: [4227072, 4194560, 33024, 4194306, 32770, 258]}, {4: 541098240, 3: [541097984, 541065472, 536903936, 4227328], 2: [541065216, 536903680, 4227072, 536871168, 4194560, 33024]}, {4: 69260574720, 3: [69260541952, 69256380416, 68723703808, 541097984], 2: [69256347648, 68723671040, 541065216, 68719509504, 536903680, 4227072]}, {4: 8865353564160, 3: [8865349369856, 8864816693248, 8796634087424, 69260541952], 2: [8864812498944, 8796629893120, 69256347648, 8796097216512, 68723671040, 541065216]}, {4: 2113665, 3: [2113664, 2113537, 2097281, 16513], 2: [2113536, 2097280, 16512, 2097153, 16385, 129]}, {4: 270549120, 3: [270548992, 270532736, 268451968, 2113664], 2: [270532608, 268451840, 2113536, 268435584, 2097280, 16512]}, {4: 34630287360, 3: [34630270976, 34628190208, 34361851904, 270548992], 2: [34628173824, 34361835520, 270532608, 34359754752, 268451840, 2113536]}, {4: 4432676782080, 3: [4432674684928, 4432408346624, 4398317043712, 34630270976], 2: [4432406249472, 4398314946560, 34628173824, 4398048608256, 34361835520, 270532608]}, {4: 8521760, 3: [8521728, 8519712, 8390688, 133152], 2: [8519680, 8390656, 133120, 8388640, 131104, 2080]}, {4: 1090785280, 3: [1090781184, 1090523136, 1074008064, 17043456], 2: [1090519040, 1074003968, 17039360, 1073745920, 16781312, 266240]}, {4: 139620515840, 3: [139619991552, 139586961408, 137473032192, 2181562368], 2: [139586437120, 137472507904, 2181038080, 137439477760, 2148007936, 34078720]}, {4: 17871426027520, 3: [17871358918656, 17867131060224, 17596548120576, 279239983104], 2: [17867063951360, 17596481011712, 279172874240, 17592253153280, 274945015808, 4362076160]}, {4: 4260880, 3: [4260864, 4259856, 4195344, 66576], 2: [4259840, 4195328, 66560, 4194320, 65552, 1040]}, {4: 545392640, 3: [545390592, 545261568, 537004032, 8521728], 2: [545259520, 537001984, 8519680, 536872960, 8390656, 133120]}, {4: 69810257920, 3: [69809995776, 69793480704, 68736516096, 1090781184], 2: [69793218560, 68736253952, 1090519040, 68719738880, 1074003968, 17039360]}, {4: 8935713013760, 3: [8935679459328, 8933565530112, 8798274060288, 139619991552], 2: [8933531975680, 8798240505856, 139586437120, 8796126576640, 137472507904, 2181038080]}, {4: 2130440, 3: [2130432, 2129928, 2097672, 33288], 2: [2129920, 2097664, 33280, 2097160, 32776, 520]}, {4: 272696320, 3: [272695296, 272630784, 268502016, 4260864], 2: [272629760, 268500992, 4259840, 268436480, 4195328, 66560]}, {4: 34905128960, 3: [34904997888, 34896740352, 34368258048, 545390592], 2: [34896609280, 34368126976, 545259520, 34359869440, 537001984, 8519680]}, {4: 4467856506880, 3: [4467839729664, 4466782765056, 4399137030144, 69809995776], 2: [4466765987840, 4399120252928, 69793218560, 4398063288320, 68736253952, 1090519040]}, {4: 16843009, 3: [16843008, 16842753, 16777473, 65793], 2: [16842752, 16777472, 65792, 16777217, 65537, 257]}, {4: 2155905152, 3: [2155905024, 2155872384, 2147516544, 8421504], 2: [2155872256, 2147516416, 8421376, 2147483776, 8388736, 32896]}, {4: 275955859456, 3: [275955843072, 275951665152, 274882117632, 1077952512], 2: [275951648768, 274882101248, 1077936128, 274877923328, 1073758208, 4210688]}, {4: 35322350010368, 3: [35322347913216, 35321813139456, 35184911056896, 137977921536], 2: [35321811042304, 35184908959744, 137975824384, 35184374185984, 137441050624, 538968064]}, {4: 33686018, 3: [33686016, 33685506, 33554946, 131586], 2: [33685504, 33554944, 131584, 33554434, 131074, 514]}, {4: 4311810304, 3: [4311810048, 4311744768, 4295033088, 16843008], 2: [4311744512, 4295032832, 16842752, 4294967552, 16777472, 65792]}, {4: 551911718912, 3: [551911686144, 551903330304, 549764235264, 2155905024], 2: [551903297536, 549764202496, 2155872256, 549755846656, 2147516416, 8421376]}, {4: 70644700020736, 3: [70644695826432, 70643626278912, 70369822113792, 275955843072], 2: [70643622084608, 70369817919488, 275951648768, 70368748371968, 274882101248, 1077936128]}, {4: 67372036, 3: [67372032, 67371012, 67109892, 263172], 2: [67371008, 67109888, 263168, 67108868, 262148, 1028]}, {4: 8623620608, 3: [8623620096, 8623489536, 8590066176, 33686016], 2: [8623489024, 8590065664, 33685504, 8589935104, 33554944, 131584]}, {4: 1103823437824, 3: [1103823372288, 1103806660608, 1099528470528, 4311810048], 2: [1103806595072, 1099528404992, 4311744512, 1099511693312, 4295032832, 16842752]}, {4: 141289400041472, 3: [141289391652864, 141287252557824, 140739644227584, 551911686144], 2: [141287244169216, 140739635838976, 551903297536, 140737496743936, 549764202496, 2155872256]}]\n",
    "            n12, n13, n22, n23 = 0, 0, 0, 0\n",
    "            s1, s2 = self.pos[mark], self.pos[3-mark]\n",
    "            for m in MASKS:\n",
    "                w1 = s1 & m[4]\n",
    "                w2 = s2 & m[4]\n",
    "                if not w2 and w1:\n",
    "                    for m3 in m[3]:\n",
    "                        if w1 == m3:\n",
    "                            n13 += 1\n",
    "                    for m2 in m[2]:\n",
    "                        if w1 == m2:\n",
    "                            n12 += 1\n",
    "                if not w1 and w2:\n",
    "                    for m3 in m[3]:\n",
    "                        if w2 == m3:\n",
    "                            n23 += 1\n",
    "                    for m2 in m[2]:\n",
    "                        if w2 == m2:\n",
    "                            n22 += 1\n",
    "            return  n12, n13, n22, n23\n",
    "\n",
    "        def count_even_odd(self, mark):\n",
    "            # helper for heuristic\n",
    "            # calculate number of marks in even and odd rows\n",
    "            odd, even = 0, 0\n",
    "            for row in range(R):\n",
    "                rd2 = row%2     \n",
    "                for col in range(C):\n",
    "                    if (self.pos[mark] >> (C*col+row)) & 1:\n",
    "                        if rd2 == 0:\n",
    "                            odd += 1\n",
    "                        else:\n",
    "                            even += 1\n",
    "            return even, odd \n",
    "\n",
    "        def heuristic(self, mark, maximizingPlayer):\n",
    "            # the heuristic uses numbers of 2-marks and 3 -marks windows\n",
    "            # numbers of marks in the even and odd rows\n",
    "            # and if player or opponent has \"doublewin\"\n",
    "\n",
    "            num_twos, num_threes, num_twos_opp, num_threes_opp = self.num23(mark)\n",
    "            score = 900000*num_threes - 900000*num_threes_opp + 30000*num_twos - 30000*num_twos_opp\n",
    "\n",
    "            num_evens, num_odds = self.count_even_odd(mark)\n",
    "            if first: \n",
    "                even_odd_rate = num_odds - num_evens\n",
    "            else:\n",
    "                even_odd_rate = num_evens - num_odds\n",
    "            score += 100 * even_odd_rate\n",
    "\n",
    "#             if num_threes > 1:\n",
    "#                 if self.doublewin(mark) : score = 1e8\n",
    "#             if num_threes_opp > 1:\n",
    "#                 if self.doublewin(3-mark) : score = -1e8\n",
    "            return score \n",
    "\n",
    "        def score_move(self, col, mark, nsteps):\n",
    "            next_state = self.make_move(col, mark)\n",
    "            return minimax(next_state, nsteps-1, -np.Inf, np.Inf, False, mark)\n",
    "       \n",
    "    # Minimax implementation\n",
    "    def minimax(state, depth, a, b, maximizingPlayer, mark):\n",
    "        \n",
    "        if time.time() > FINISH: # timeout\n",
    "            return 0\n",
    "\n",
    "        if state.connected_four(mark): # player win\n",
    "            return np.inf\n",
    "        if state.connected_four(3-mark): # opponent win\n",
    "            return -np.inf\n",
    "        if state.pos[1] | state.pos[2] == 279258638311359: # fullboard mask, tie\n",
    "            return 0\n",
    "\n",
    "        if depth == 0: # leaf node\n",
    "            key = (state.pos[1], state.pos[2]) # the key for store state in the dict\n",
    "            if key in STATES: # heuristic for the state is in tne dict\n",
    "                return STATES[key] # get it from the dict\n",
    "            # new state, calculate heuristic\n",
    "            h = state.heuristic(mark, maximizingPlayer=maximizingPlayer)\n",
    "            STATES[key] = h # and store it in the dict\n",
    "            return h\n",
    " \n",
    "        # MiniMax with alpha-beta pruning\n",
    "    \n",
    "        valid_moves = state.valid_moves()\n",
    "       \n",
    "        if maximizingPlayer:\n",
    "            value = -np.Inf\n",
    "            for col in valid_moves:\n",
    "                child = state.make_move(col, mark)\n",
    "                value = max(value, minimax(child, depth-1, a, b, False, mark))\n",
    "                if value >= b:\n",
    "                    break\n",
    "                a = max(a, value)\n",
    "            return value\n",
    "        else:\n",
    "            value = np.Inf\n",
    "            for col in valid_moves:\n",
    "                child = state.make_move(col, 3-mark)\n",
    "                value = min(value, minimax(child, depth-1, a, b, True, mark))\n",
    "                if value <= a:\n",
    "                    break\n",
    "                b = min(b, value)\n",
    "            return value \n",
    "    \n",
    "    # The agent begins to work\n",
    "    \n",
    "    START = time.time()\n",
    "    FINISH = START + DELAY\n",
    "    \n",
    "    # the dict to store heuristics, a la transposition table\n",
    "    STATES = {}  \n",
    "    \n",
    "    # the depth to start with\n",
    "    N_STEPS = 4\n",
    "    \n",
    "    # Convert the board to a 2D grid and then the grig  to state\n",
    "    grid = np.asarray(obs.board).reshape(R, C)\n",
    "    state = State.from_grid(grid)\n",
    "    \n",
    "    # the best move for the first move is \"3\", let's play it\n",
    "    step = step_number(grid)\n",
    "    if step < 2:\n",
    "        return 3\n",
    "    \n",
    "    # remember if the agent is the first player, for heuristic\n",
    "    first = is_first_player(step)\n",
    "\n",
    "    valid_moves = state.valid_moves()\n",
    "    # check if we have winning move\n",
    "    for col in valid_moves:\n",
    "        next_state = state.make_move(col, obs.mark)\n",
    "        if next_state.connected_four(obs.mark):\n",
    "            # yes, we have, let's play it\n",
    "            return col\n",
    "        \n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    # the agent is rather fast, so it starts with six steps to look ahead\n",
    "    scores = dict(zip(valid_moves, [state.score_move(col, obs.mark, N_STEPS) for col in valid_moves]))\n",
    "\n",
    "    # Repeat with one step more depth\n",
    "    while  N_STEPS < 42 - step:\n",
    "        N_STEPS += 1\n",
    "\n",
    "        # no need to calculate scores for the moves we already know are the way to loss\n",
    "        for vm in valid_moves:\n",
    "            if scores[vm] == -np.inf:\n",
    "                valid_moves.remove(vm)\n",
    "        if len(valid_moves) == 0: # we loss anyway, surrender :(\n",
    "            break\n",
    "            \n",
    "        scores_2 = {}\n",
    "        for col in valid_moves:\n",
    "            score = state.score_move(col, obs.mark, N_STEPS)\n",
    "            if score == np.inf:\n",
    "                # we have the move to win with, let's play it\n",
    "                return col\n",
    "            scores_2[col] = score\n",
    "\n",
    "        if time.time() > FINISH or np.amax(list(scores_2.values())) <= -np.inf:\n",
    "            # timeout, the search with the next depth isn't complete\n",
    "            # leave the previous scores as is\n",
    "            break\n",
    "        # update the scores withe the result of the next depth search\n",
    "        scores = scores_2.copy()\n",
    "    \n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "\n",
    "    # Select the best as nearest to the center of the board from the maximizing columns\n",
    "    if len(max_cols) >0:\n",
    "        if 3 in max_cols:\n",
    "            return 3\n",
    "        if 2 in max_cols and 4 in max_cols:\n",
    "            return random.choice([2, 4])\n",
    "        if 2 in max_cols:\n",
    "            return 2\n",
    "        if 4 in max_cols:\n",
    "            return 4\n",
    "        \n",
    "        if 1 in max_cols and 5 in max_cols:\n",
    "            return random.choice([1, 5])\n",
    "        if 1 in max_cols:\n",
    "            return 1\n",
    "        if 5 in max_cols:\n",
    "            return 5\n",
    "        \n",
    "        if 0 in max_cols and 6 in max_cols:\n",
    "            return random.choice([0, 6])\n",
    "        if 0 in max_cols:\n",
    "            return 0\n",
    "        if 6 in max_cols:\n",
    "            return 6\n",
    "        \n",
    "        col = random.choice(max_cols)\n",
    "        return col\n",
    "    col =  random.choice(max_cols)\n",
    "    return col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0942c1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.505419Z",
     "iopub.status.busy": "2024-02-03T19:25:15.504980Z",
     "iopub.status.idle": "2024-02-03T19:25:15.517105Z",
     "shell.execute_reply": "2024-02-03T19:25:15.515875Z"
    },
    "papermill": {
     "duration": 0.035569,
     "end_time": "2024-02-03T19:25:15.520247",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.484678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run agent_The_Fast_mini_Max_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b69bac",
   "metadata": {
    "papermill": {
     "duration": 0.018051,
     "end_time": "2024-02-03T19:25:15.557060",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.539009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [The Fast mini Max **IV**](https://www.kaggle.com/code/martynovandrey/the-fast-mini-max-iv) - [*Martynov Andrey*](https://www.kaggle.com/martynovandrey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f1529a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.597322Z",
     "iopub.status.busy": "2024-02-03T19:25:15.596780Z",
     "iopub.status.idle": "2024-02-03T19:25:15.620385Z",
     "shell.execute_reply": "2024-02-03T19:25:15.619405Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048128,
     "end_time": "2024-02-03T19:25:15.623040",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.574912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agent_The_Fast_mini_Max_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent_The_Fast_mini_Max_4.py\n",
    "\n",
    "def agent_TheFast_mini_Max_4(obs, config):\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import time\n",
    "\n",
    "    # Precalculate some constants for better performance\n",
    "    W = config.inarow\n",
    "    R = config.rows\n",
    "    C = config.columns\n",
    "    Rp1 = R + 1\n",
    "    Rm1 = R - 1\n",
    "    RmW = R - W\n",
    "    RmWp1 = R - W + 1\n",
    "    Cp1 = C + 1\n",
    "    Cm1 = C - 1\n",
    "    Rp1Cp1 = Rp1 * Cp1\n",
    "    Ct2 = C * 2\n",
    "    Cp1t2 = Cp1 * 2\n",
    "    Cm1t2 = Cm1 * 2\n",
    "    Wmask = 2**W - 1\n",
    "    RC = R*C\n",
    "    DELAY = 0.5\n",
    "                \n",
    "    # the order of calculation the scores of a move, nearest to the center first, \n",
    "    # in the case of 7 columns it's [3, 2, 4, 1, 5, 0, 6]\n",
    "    ORDER = [C//2 - i//2 - 1 if i%2 else C//2 + i//2 for i in range(C)]\n",
    "    \n",
    "    # Some utility functions\n",
    "    \n",
    "    def step_number(grid):\n",
    "        # returns step of the game\n",
    "        empty = 0\n",
    "        for i in range(R):\n",
    "            for j in range(C):\n",
    "                if grid[i][j] == 0:\n",
    "                    empty += 1\n",
    "        return R*C - empty\n",
    "\n",
    "    def is_first_player(step):\n",
    "        return step%2 == 0\n",
    "    \n",
    "    def int_to_bin(x):\n",
    "        # 64-bit integer to 64-letter string\n",
    "        return bin(x)[2:].zfill(64)\n",
    "    \n",
    "    def grid_to_matrix(grid):\n",
    "        matrix = np.array([0 for i in range(Rp1Cp1)]).reshape(Rp1, Cp1)\n",
    "        for r in range(R):\n",
    "            for c in range(C):\n",
    "                matrix[r+1, c] = grid[r, c]\n",
    "        return matrix\n",
    "\n",
    "    def grid_to_state(grid):\n",
    "        matrix = grid_to_matrix(grid)\n",
    "        position1, position2 = '', ''\n",
    "        for c in range(C, -1, -1):\n",
    "            for r in range(0, Rp1):\n",
    "                if matrix[r,c] == 1:\n",
    "                    position1 += '1'\n",
    "                else:\n",
    "                    position1 += '0'\n",
    "                if matrix[r,c] == 2:\n",
    "                    position2 += '1'\n",
    "                else:\n",
    "                    position2 += '0'\n",
    "        return {1:int(position1, 2), 2:int(position2, 2)}\n",
    "\n",
    "    def state_to_grid(state):\n",
    "        position1 = state[1]\n",
    "        position2 = state[2]\n",
    "        matrix = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        for c in range(0, Cp1):\n",
    "            for r in range(R, -1, -1):\n",
    "                if position1 & 1:\n",
    "                    matrix[r,c] = 1\n",
    "                position1 = position1 >> 1\n",
    "                if position2 & 1:\n",
    "                    matrix[r,c] = 2\n",
    "                position2 = position2 >> 1\n",
    "        return matrix[1:, :-1]\n",
    "\n",
    "    def make_move(state, col, mark):\n",
    "        mask = state[1] | state[2]\n",
    "        new_state = state.copy()\n",
    "        new_state[mark] = state[3-mark] ^ (mask | (mask + (1 << (col*7))))\n",
    "        return new_state\n",
    "\n",
    "    def connected_four(position):\n",
    "        # Horizontal check\n",
    "        m = position & (position >> 7)\n",
    "        if m & (m >> 14): return True\n",
    "        # Diagonal \\\n",
    "        m = position & (position >> 6)\n",
    "        if m & (m >> 12): return True\n",
    "        # Diagonal /\n",
    "        m = position & (position >> 8)\n",
    "        if m & (m >> 16): return True\n",
    "        # Vertical\n",
    "        m = position & (position >> 1)\n",
    "        if m & (m >> 2): return True\n",
    "        # Nothing found\n",
    "        return False\n",
    "\n",
    "    def int_to_bin(x):\n",
    "        return bin(x)[2:].zfill(64)\n",
    "\n",
    "    def valid_moves_list(state):\n",
    "        m = state[1] | state[2]\n",
    "        return [i for i in [3,2,4,1,5,0,6] if not (m >> 5+7*i) & 1]\n",
    "   \n",
    "    MASKS = [{4: 60, 3: [28, 44, 52, 56], 2: [12, 20, 24, 36, 40, 48]}, {4: 30, 3: [14, 22, 26, 28], 2: [6, 10, 12, 18, 20, 24]}, {4: 15, 3: [7, 11, 13, 14], 2: [3, 5, 6, 9, 10, 12]}, {4: 7680, 3: [3584, 5632, 6656, 7168], 2: [1536, 2560, 3072, 4608, 5120, 6144]}, {4: 3840, 3: [1792, 2816, 3328, 3584], 2: [768, 1280, 1536, 2304, 2560, 3072]}, {4: 1920, 3: [896, 1408, 1664, 1792], 2: [384, 640, 768, 1152, 1280, 1536]}, {4: 983040, 3: [458752, 720896, 851968, 917504], 2: [196608, 327680, 393216, 589824, 655360, 786432]}, {4: 491520, 3: [229376, 360448, 425984, 458752], 2: [98304, 163840, 196608, 294912, 327680, 393216]}, {4: 245760, 3: [114688, 180224, 212992, 229376], 2: [49152, 81920, 98304, 147456, 163840, 196608]}, {4: 125829120, 3: [58720256, 92274688, 109051904, 117440512], 2: [25165824, 41943040, 50331648, 75497472, 83886080, 100663296]}, {4: 62914560, 3: [29360128, 46137344, 54525952, 58720256], 2: [12582912, 20971520, 25165824, 37748736, 41943040, 50331648]}, {4: 31457280, 3: [14680064, 23068672, 27262976, 29360128], 2: [6291456, 10485760, 12582912, 18874368, 20971520, 25165824]}, {4: 16106127360, 3: [7516192768, 11811160064, 13958643712, 15032385536], 2: [3221225472, 5368709120, 6442450944, 9663676416, 10737418240, 12884901888]}, {4: 8053063680, 3: [3758096384, 5905580032, 6979321856, 7516192768], 2: [1610612736, 2684354560, 3221225472, 4831838208, 5368709120, 6442450944]}, {4: 4026531840, 3: [1879048192, 2952790016, 3489660928, 3758096384], 2: [805306368, 1342177280, 1610612736, 2415919104, 2684354560, 3221225472]}, {4: 2061584302080, 3: [962072674304, 1511828488192, 1786706395136, 1924145348608], 2: [412316860416, 687194767360, 824633720832, 1236950581248, 1374389534720, 1649267441664]}, {4: 1030792151040, 3: [481036337152, 755914244096, 893353197568, 962072674304], 2: [206158430208, 343597383680, 412316860416, 618475290624, 687194767360, 824633720832]}, {4: 515396075520, 3: [240518168576, 377957122048, 446676598784, 481036337152], 2: [103079215104, 171798691840, 206158430208, 309237645312, 343597383680, 412316860416]}, {4: 263882790666240, 3: [123145302310912, 193514046488576, 228698418577408, 246290604621824], 2: [52776558133248, 87960930222080, 105553116266496, 158329674399744, 175921860444160, 211106232532992]}, {4: 131941395333120, 3: [61572651155456, 96757023244288, 114349209288704, 123145302310912], 2: [26388279066624, 43980465111040, 52776558133248, 79164837199872, 87960930222080, 105553116266496]}, {4: 65970697666560, 3: [30786325577728, 48378511622144, 57174604644352, 61572651155456], 2: [13194139533312, 21990232555520, 26388279066624, 39582418599936, 43980465111040, 52776558133248]}, {4: 67637280, 3: [67637248, 67633184, 67112992, 528416], 2: [67633152, 67112960, 528384, 67108896, 524320, 4128]}, {4: 8657571840, 3: [8657567744, 8657047552, 8590462976, 67637248], 2: [8657043456, 8590458880, 67633152, 8589938688, 67112960, 528384]}, {4: 1108169195520, 3: [1108168671232, 1108102086656, 1099579260928, 8657567744], 2: [1108101562368, 1099578736640, 8657043456, 1099512152064, 8590458880, 67633152]}, {4: 141845657026560, 3: [141845589917696, 141837067091968, 140746145398784, 1108168671232], 2: [141836999983104, 140746078289920, 1108101562368, 140737555464192, 1099578736640, 8657043456]}, {4: 33818640, 3: [33818624, 33816592, 33556496, 264208], 2: [33816576, 33556480, 264192, 33554448, 262160, 2064]}, {4: 4328785920, 3: [4328783872, 4328523776, 4295231488, 33818624], 2: [4328521728, 4295229440, 33816576, 4294969344, 33556480, 264192]}, {4: 554084597760, 3: [554084335616, 554051043328, 549789630464, 4328783872], 2: [554050781184, 549789368320, 4328521728, 549756076032, 4295229440, 33816576]}, {4: 70922828513280, 3: [70922794958848, 70918533545984, 70373072699392, 554084335616], 2: [70918499991552, 70373039144960, 554050781184, 70368777732096, 549789368320, 4328521728]}, {4: 16909320, 3: [16909312, 16908296, 16778248, 132104], 2: [16908288, 16778240, 132096, 16777224, 131080, 1032]}, {4: 2164392960, 3: [2164391936, 2164261888, 2147615744, 16909312], 2: [2164260864, 2147614720, 16908288, 2147484672, 16778240, 132096]}, {4: 277042298880, 3: [277042167808, 277025521664, 274894815232, 2164391936], 2: [277025390592, 274894684160, 2164260864, 274878038016, 2147614720, 16908288]}, {4: 35461414256640, 3: [35461397479424, 35459266772992, 35186536349696, 277042167808], 2: [35459249995776, 35186519572480, 277025390592, 35184388866048, 274894684160, 2164260864]}, {4: 8454660, 3: [8454656, 8454148, 8389124, 66052], 2: [8454144, 8389120, 66048, 8388612, 65540, 516]}, {4: 1082196480, 3: [1082195968, 1082130944, 1073807872, 8454656], 2: [1082130432, 1073807360, 8454144, 1073742336, 8389120, 66048]}, {4: 138521149440, 3: [138521083904, 138512760832, 137447407616, 1082195968], 2: [138512695296, 137447342080, 1082130432, 137439019008, 1073807360, 8454144]}, {4: 17730707128320, 3: [17730698739712, 17729633386496, 17593268174848, 138521083904], 2: [17729624997888, 17593259786240, 138512695296, 17592194433024, 137447342080, 1082130432]}, {4: 4227330, 3: [4227328, 4227074, 4194562, 33026], 2: [4227072, 4194560, 33024, 4194306, 32770, 258]}, {4: 541098240, 3: [541097984, 541065472, 536903936, 4227328], 2: [541065216, 536903680, 4227072, 536871168, 4194560, 33024]}, {4: 69260574720, 3: [69260541952, 69256380416, 68723703808, 541097984], 2: [69256347648, 68723671040, 541065216, 68719509504, 536903680, 4227072]}, {4: 8865353564160, 3: [8865349369856, 8864816693248, 8796634087424, 69260541952], 2: [8864812498944, 8796629893120, 69256347648, 8796097216512, 68723671040, 541065216]}, {4: 2113665, 3: [2113664, 2113537, 2097281, 16513], 2: [2113536, 2097280, 16512, 2097153, 16385, 129]}, {4: 270549120, 3: [270548992, 270532736, 268451968, 2113664], 2: [270532608, 268451840, 2113536, 268435584, 2097280, 16512]}, {4: 34630287360, 3: [34630270976, 34628190208, 34361851904, 270548992], 2: [34628173824, 34361835520, 270532608, 34359754752, 268451840, 2113536]}, {4: 4432676782080, 3: [4432674684928, 4432408346624, 4398317043712, 34630270976], 2: [4432406249472, 4398314946560, 34628173824, 4398048608256, 34361835520, 270532608]}, {4: 8521760, 3: [8521728, 8519712, 8390688, 133152], 2: [8519680, 8390656, 133120, 8388640, 131104, 2080]}, {4: 1090785280, 3: [1090781184, 1090523136, 1074008064, 17043456], 2: [1090519040, 1074003968, 17039360, 1073745920, 16781312, 266240]}, {4: 139620515840, 3: [139619991552, 139586961408, 137473032192, 2181562368], 2: [139586437120, 137472507904, 2181038080, 137439477760, 2148007936, 34078720]}, {4: 17871426027520, 3: [17871358918656, 17867131060224, 17596548120576, 279239983104], 2: [17867063951360, 17596481011712, 279172874240, 17592253153280, 274945015808, 4362076160]}, {4: 4260880, 3: [4260864, 4259856, 4195344, 66576], 2: [4259840, 4195328, 66560, 4194320, 65552, 1040]}, {4: 545392640, 3: [545390592, 545261568, 537004032, 8521728], 2: [545259520, 537001984, 8519680, 536872960, 8390656, 133120]}, {4: 69810257920, 3: [69809995776, 69793480704, 68736516096, 1090781184], 2: [69793218560, 68736253952, 1090519040, 68719738880, 1074003968, 17039360]}, {4: 8935713013760, 3: [8935679459328, 8933565530112, 8798274060288, 139619991552], 2: [8933531975680, 8798240505856, 139586437120, 8796126576640, 137472507904, 2181038080]}, {4: 2130440, 3: [2130432, 2129928, 2097672, 33288], 2: [2129920, 2097664, 33280, 2097160, 32776, 520]}, {4: 272696320, 3: [272695296, 272630784, 268502016, 4260864], 2: [272629760, 268500992, 4259840, 268436480, 4195328, 66560]}, {4: 34905128960, 3: [34904997888, 34896740352, 34368258048, 545390592], 2: [34896609280, 34368126976, 545259520, 34359869440, 537001984, 8519680]}, {4: 4467856506880, 3: [4467839729664, 4466782765056, 4399137030144, 69809995776], 2: [4466765987840, 4399120252928, 69793218560, 4398063288320, 68736253952, 1090519040]}, {4: 16843009, 3: [16843008, 16842753, 16777473, 65793], 2: [16842752, 16777472, 65792, 16777217, 65537, 257]}, {4: 2155905152, 3: [2155905024, 2155872384, 2147516544, 8421504], 2: [2155872256, 2147516416, 8421376, 2147483776, 8388736, 32896]}, {4: 275955859456, 3: [275955843072, 275951665152, 274882117632, 1077952512], 2: [275951648768, 274882101248, 1077936128, 274877923328, 1073758208, 4210688]}, {4: 35322350010368, 3: [35322347913216, 35321813139456, 35184911056896, 137977921536], 2: [35321811042304, 35184908959744, 137975824384, 35184374185984, 137441050624, 538968064]}, {4: 33686018, 3: [33686016, 33685506, 33554946, 131586], 2: [33685504, 33554944, 131584, 33554434, 131074, 514]}, {4: 4311810304, 3: [4311810048, 4311744768, 4295033088, 16843008], 2: [4311744512, 4295032832, 16842752, 4294967552, 16777472, 65792]}, {4: 551911718912, 3: [551911686144, 551903330304, 549764235264, 2155905024], 2: [551903297536, 549764202496, 2155872256, 549755846656, 2147516416, 8421376]}, {4: 70644700020736, 3: [70644695826432, 70643626278912, 70369822113792, 275955843072], 2: [70643622084608, 70369817919488, 275951648768, 70368748371968, 274882101248, 1077936128]}, {4: 67372036, 3: [67372032, 67371012, 67109892, 263172], 2: [67371008, 67109888, 263168, 67108868, 262148, 1028]}, {4: 8623620608, 3: [8623620096, 8623489536, 8590066176, 33686016], 2: [8623489024, 8590065664, 33685504, 8589935104, 33554944, 131584]}, {4: 1103823437824, 3: [1103823372288, 1103806660608, 1099528470528, 4311810048], 2: [1103806595072, 1099528404992, 4311744512, 1099511693312, 4295032832, 16842752]}, {4: 141289400041472, 3: [141289391652864, 141287252557824, 140739644227584, 551911686144], 2: [141287244169216, 140739635838976, 551903297536, 140737496743936, 549764202496, 2155872256]}]\n",
    "    def num23(state, mark):\n",
    "        n12, n13, n22, n23 = 0, 0, 0, 0\n",
    "        s1, s2 = state[mark], state[3-mark]\n",
    "        for m in MASKS:\n",
    "            w1 = s1 & m[4]\n",
    "            w2 = s2 & m[4]\n",
    "            if not w2 and w1:\n",
    "                for m3 in m[3]:\n",
    "                    if w1 == m3:\n",
    "                        n13 += 1\n",
    "                for m2 in m[2]:\n",
    "                    if w1 == m2:\n",
    "                        n12 += 1\n",
    "            if not w1 and w2:\n",
    "                for m3 in m[3]:\n",
    "                    if w2 == m3:\n",
    "                        n23 += 1\n",
    "                for m2 in m[2]:\n",
    "                    if w2 == m2:\n",
    "                        n22 += 1\n",
    "        return  n12, n13, n22, n23\n",
    "\n",
    "    def doublewin(state, mark):\n",
    "        win = 0\n",
    "        for col in valid_moves_list(state):\n",
    "            next_state = make_move(state,col, mark)\n",
    "            if connected_four(next_state[mark]):\n",
    "                next_state_opp = make_move(state, col, 3-mark)\n",
    "                if connected_four(make_move(next_state_opp, col, mark)[mark]):\n",
    "                    return True # win with this column first or second move, unstoppable\n",
    "                win += 1\n",
    "                if win == 2:\n",
    "                    return True # more than one winning move\n",
    "        return False\n",
    "    \n",
    "    def count_even_odd(state, mark):\n",
    "        # helper for heuristic\n",
    "        # calculate number of marks in even and odd rows\n",
    "        odd, even = 0, 0\n",
    "        for row in range(R):\n",
    "            rd2 = row%2     \n",
    "            for col in range(C):\n",
    "                if (state[mark] >> (C*col+row)) & 1:\n",
    "                    if rd2 == 0:\n",
    "                        odd += 1\n",
    "                    else:\n",
    "                        even += 1\n",
    "        return even, odd \n",
    "\n",
    "    def heuristic(state, mark):\n",
    "        # the heuristic uses numbers of 2-marks and 3 -marks windows\n",
    "        # numbers of marks in the even and odd rows\n",
    "        # and if player or opponent has \"doublewin\"\n",
    "\n",
    "        num_twos, num_threes, num_twos_opp, num_threes_opp = num23(state, mark)\n",
    "        score = 900000*num_threes - 900000*num_threes_opp + 30000*num_twos - 30000*num_twos_opp\n",
    "\n",
    "        num_evens, num_odds = count_even_odd(state, mark)\n",
    "        if first: \n",
    "            even_odd_rate = num_odds - num_evens\n",
    "        else:\n",
    "            even_odd_rate = num_evens - num_odds\n",
    "        score += 100 * even_odd_rate\n",
    "\n",
    "        if num_threes > 1:\n",
    "            if doublewin(state, mark) : score = 1e8\n",
    "        if num_threes_opp > 1:\n",
    "            if doublewin(state, 3-mark) : score = -1e8\n",
    "        return score \n",
    "\n",
    "    def score_move(state, col, mark, nsteps):\n",
    "        next_state = make_move(state, col, mark)\n",
    "        return minimax(next_state, nsteps-1, -np.Inf, np.Inf, False, mark)\n",
    "       \n",
    "    # Minimax implementation\n",
    "    def minimax(state, depth, a, b, maximizingPlayer, mark):\n",
    "        if time.time() > FINISH: # timeout\n",
    "            return 0\n",
    "        if connected_four(state[mark]): # player win\n",
    "            return np.inf\n",
    "        if connected_four(state[3-mark]): # opponent win\n",
    "            return -np.inf\n",
    "        if state[1] | state[2] == 279258638311359: # fullboard mask, tie\n",
    "            return 0\n",
    "\n",
    "        if depth == 0: # leaf node\n",
    "            key = (state[1], state[2]) # the key for store state in the dict\n",
    "            if key in STATES: # heuristic for the state is in tne dict\n",
    "                return STATES[key] # get it from the dict\n",
    "            # new state, calculate heuristic\n",
    "            h = heuristic(state, mark)\n",
    "            STATES[key] = h # and store it in the dict\n",
    "            return h\n",
    " \n",
    "        # MiniMax with alpha-beta pruning\n",
    "    \n",
    "        valid_moves = valid_moves_list(state)\n",
    "       \n",
    "        if maximizingPlayer:\n",
    "            value = -np.Inf\n",
    "            for col in valid_moves:\n",
    "                child = make_move(state, col, mark)\n",
    "                value = max(value, minimax(child, depth-1, a, b, False, mark))\n",
    "                if value >= b:\n",
    "                    break\n",
    "                a = max(a, value)\n",
    "            return value\n",
    "        else:\n",
    "            value = np.Inf\n",
    "            for col in valid_moves:\n",
    "                child = make_move(state, col, 3-mark)\n",
    "                value = min(value, minimax(child, depth-1, a, b, True, mark))\n",
    "                if value <= a:\n",
    "                    break\n",
    "                b = min(b, value)\n",
    "            return value \n",
    "    \n",
    "    # The agent begins to work\n",
    "    \n",
    "    START = time.time()\n",
    "    FINISH = START + DELAY\n",
    "    \n",
    "    # the dict to store heuristics, a la transposition table\n",
    "    STATES = {}  \n",
    "    \n",
    "    # the depth to start with\n",
    "    N_STEPS = 5\n",
    "    \n",
    "    # Convert the board to a 2D grid and then the grig  to state\n",
    "    grid = np.asarray(obs.board).reshape(R, C)\n",
    "    state = grid_to_state(grid)\n",
    "    \n",
    "    # the best move for the first move is \"3\", let's play it\n",
    "    step = step_number(grid)\n",
    "    if step < 2:\n",
    "        return 3\n",
    "    \n",
    "    # remember if the agent is the first player, for heuristic\n",
    "    first = is_first_player(step)\n",
    "\n",
    "    valid_moves = valid_moves_list(state)\n",
    "    # check if we have winning move\n",
    "    for col in valid_moves:\n",
    "        next_state = make_move(state, col, obs.mark)\n",
    "        if connected_four(next_state[obs.mark]):\n",
    "            # yes, we have, let's play it\n",
    "            return col\n",
    "        \n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    # the agent is rather fast, so it starts with N_STEPS steps to look ahead\n",
    "    scores = dict(zip(valid_moves, [score_move(state, col, obs.mark, N_STEPS) for col in valid_moves]))\n",
    "\n",
    "    # Repeat with one step more depth\n",
    "    while  N_STEPS < 42 - step:\n",
    "        N_STEPS += 1\n",
    "\n",
    "        # no need to calculate scores for the moves we already know are the way to loss\n",
    "        for vm in valid_moves:\n",
    "            if scores[vm] == -np.inf:\n",
    "                valid_moves.remove(vm)\n",
    "        if len(valid_moves) == 0: # we loss anyway, surrender :(\n",
    "            break\n",
    "            \n",
    "        scores_2 = {}\n",
    "        for col in valid_moves:\n",
    "            score = score_move(state, col, obs.mark, N_STEPS)\n",
    "            if score == np.inf:\n",
    "                # we have the move to win with, let's play it\n",
    "                return col\n",
    "            scores_2[col] = score\n",
    "\n",
    "        if time.time() > FINISH or np.amax(list(scores_2.values())) <= -np.inf:\n",
    "            # timeout, the search with the next depth isn't complete\n",
    "            # leave the previous scores as is\n",
    "            break\n",
    "        # update the scores withe the result of the next depth search\n",
    "        scores = scores_2.copy()\n",
    "    \n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "\n",
    "    # Select the best as nearest to the center of the board from the maximizing columns\n",
    "    if len(max_cols) >0:\n",
    "        if 3 in max_cols:\n",
    "            return 3\n",
    "        if 2 in max_cols and 4 in max_cols:\n",
    "            return random.choice([2, 4])\n",
    "        if 2 in max_cols:\n",
    "            return 2\n",
    "        if 4 in max_cols:\n",
    "            return 4\n",
    "        \n",
    "        if 1 in max_cols and 5 in max_cols:\n",
    "            return random.choice([1, 5])\n",
    "        if 1 in max_cols:\n",
    "            return 1\n",
    "        if 5 in max_cols:\n",
    "            return 5\n",
    "        \n",
    "        if 0 in max_cols and 6 in max_cols:\n",
    "            return random.choice([0, 6])\n",
    "        if 0 in max_cols:\n",
    "            return 0\n",
    "        if 6 in max_cols:\n",
    "            return 6\n",
    "        \n",
    "        col = random.choice(max_cols)\n",
    "        return col\n",
    "    col =  random.choice(max_cols)\n",
    "    return col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61136af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.661017Z",
     "iopub.status.busy": "2024-02-03T19:25:15.660593Z",
     "iopub.status.idle": "2024-02-03T19:25:15.672771Z",
     "shell.execute_reply": "2024-02-03T19:25:15.671419Z"
    },
    "papermill": {
     "duration": 0.035085,
     "end_time": "2024-02-03T19:25:15.676412",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.641327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run agent_The_Fast_mini_Max_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35a975",
   "metadata": {
    "papermill": {
     "duration": 0.017447,
     "end_time": "2024-02-03T19:25:15.712419",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.694972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### [debugger](https://www.kaggle.com/vyacheslavbolotin/debugger-c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e387b172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.751594Z",
     "iopub.status.busy": "2024-02-03T19:25:15.751206Z",
     "iopub.status.idle": "2024-02-03T19:25:15.808747Z",
     "shell.execute_reply": "2024-02-03T19:25:15.807829Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.080661,
     "end_time": "2024-02-03T19:25:15.811707",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.731046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def debugger_c4(list_of_agents_to_trace):\n",
    "    map4q = '''\n",
    "         01,02,03,04; 31,32,33,34;                 32,33,34,35; 33,34,35,36; \n",
    "           40,41,42,43; 41,42,43,44;             42,43,44,45; 43,44,45,46; \n",
    "             50,51,52,53; 51,52,53,54;         52,53,54,55; 30,31,32,33;\n",
    "               04,14,24,34; 14,24,34,44;     24,34,44,54; 05,15,25,35; \n",
    "                 06,16,26,36; 03,12,21,30; 15,25,35,45; 25,35,45,55; \n",
    "                   16,26,36,46; 26,36,46,56;          23,34,45,56;\n",
    "                     04,13,22,31; 13,22,31,40;      03,14,25,36; \n",
    "                       05,14,23,32; 14,23,32,41;  20,30,40,50; \n",
    "                         06,15,24,33; 15,24,33,42;\n",
    "                           24,33,42,51; 23,32,41,50;\n",
    "                             03,13,23,33; 13,23,33,43; \n",
    "                               16,25,34,43; 25,34,43,52; \n",
    "                                 11,22,33,44; 11,21,31,41;\n",
    "                     01,12,23,34;  02,13,24,35; 12,23,34,45; \n",
    "                   00,11,22,33;      26,35,44,53; 23,33,43,53;              \n",
    "                 02,12,22,32;          21,32,43,54; 22,33,44,55; \n",
    "               00,10,20,30; 22,23,24,25; 21,31,41,51; 13,24,35,46;\n",
    "             01,11,21,31; 10,20,30,40;     12,22,32,42; 22,32,42,52;\n",
    "           00,01,02,03; 10,21,32,43;         02,03,04,05; 03,04,05,06;\n",
    "         10,11,12,13; 11,12,13,14;             12,13,14,15; 13,14,15,16;\n",
    "       20,21,22,23; 21,22,23,24;                 23,24,25,26; 20,31,42,53; 53,54,55,56'''\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    start_games_at_away_first_agent = False # True # => The first agent from the list will start his game either “from home” or “away”,\n",
    "    only_observation_first_agent    = False # True # => Everyone will play two games with the first: one “at home”, the other “away”, or everyone will play with everyone\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    class Obs:\n",
    "        def __init__(self, board, mark):\n",
    "            self.board = board\n",
    "            self.mark = mark\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    class Config:\n",
    "        def __init__(self, rows, columns, inarow):\n",
    "            self.rows = rows\n",
    "            self.columns = columns\n",
    "            self.inarow = inarow\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    import time\n",
    "    import numpy as np\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    Rows, Cols = 6, 7\n",
    "    config = Config(Rows, Cols, 4)\n",
    "    AT2 = (np.array([a for a in range(42)])).reshape(Rows, Cols)\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def yx(s): return AT2[int(s[0])][int(s[1])]\n",
    "    Ss = [x.split(\",\") for x in [x for x in map4q.replace(\" \", \"\").replace(\"\\n\", \"\").split(\";\")]]\n",
    "    iMap4q = [list(map(yx, s)) for s in Ss]\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def gyx(s): return [int(s[0]), int(s[1])]\n",
    "    gSs = [x.split(\",\") for x in [x for x in map4q.replace(\" \", \"\").replace(\"\\n\", \"\").split(\";\")]]\n",
    "    gMap4q = [list(map(gyx, s)) for s in gSs]\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def inject(col, mark, board):\n",
    "        i_krai = config.columns * (config.rows - 1) + col\n",
    "        for i in range(i_krai, -1, -7):\n",
    "            if board[i] == 0:\n",
    "                new_board = board.copy()\n",
    "                new_board[i] = mark\n",
    "                return new_board\n",
    "        return board\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def win_4o_in(board):\n",
    "        grid = np.asarray(board).reshape(6,7)\n",
    "        for i in gMap4q:\n",
    "            v = grid[i[0][0],i[0][1]] * grid[i[1][0],i[1][1]] * grid[i[2][0],i[2][1]] * grid[i[3][0],i[3][1]]\n",
    "            if v == 1 or v == 16: return True\n",
    "        return False\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def tsum(reagent):\n",
    "        home = sum(reagent[\"games\"][0]) if len(reagent[\"games\"][0])>0 else 0\n",
    "        away = sum(reagent[\"games\"][1]) if len(reagent[\"games\"][1])>0 else 0\n",
    "        return home + away\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def ttime(reagent, gt):\n",
    "        time = reagent[\"time\"][0]  if len(reagent[\"time\"])>0 else 0\n",
    "        return [round(time+gt, 2)]\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def tmoves(reagent, ms):\n",
    "        moves = reagent[\"moves\"][0] if len(reagent[\"moves\"])>0 else 0\n",
    "        return [moves+ms]\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def tper1move(reagent):\n",
    "        time = reagent[\"time\"][0]   if len(reagent[\"time\"]) > 0 else 0\n",
    "        moves = reagent[\"moves\"][0] if len(reagent[\"moves\"])> 0 else 0\n",
    "        return [round(time / moves, 1)]\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def print_to_clip(board, ims, i, attack, defense, stime1, stime2, s1, s2):\n",
    "        print(\"1.{0} [{1}]   2.{2} [{3}]\".format(name(attack), s1, name(defense), s2),\n",
    "              \"  1.Time =\", round(stime1, 2), '  2.Time =', round(stime2, 2), '  q.moves:', i,\n",
    "              \"  1.speed =\", round(stime1 / i, 1), \"  2.speed =\", round(stime2 / i, 1),\n",
    "              \"  \\n\\nGame:\", str(ims).replace(\", \", \",\"),\n",
    "              \"  \\n\\nBoard:\", str(board).replace(\", \", \",\"),\"\\n\")\n",
    "        bn2d = np.array(board).reshape(Rows, Cols)\n",
    "        for r in range(len(bn2d)): print(bn2d[r])\n",
    "        print('{0}{1}'.format('\\n',\"=\"*101))\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def rec(total_points, _A, _D, ia, id, stime1, stime2):\n",
    "        reatta = total_points[name(Attack)]\n",
    "        redefe = total_points[name(Defense)]\n",
    "        reatta[\"games\"][0].extend([_A])\n",
    "        redefe[\"games\"][1].extend([_D])\n",
    "        reatta[\"T\"] = [tsum(reatta)]\n",
    "        redefe[\"T\"] = [tsum(redefe)]\n",
    "        reatta[\"time\"] = ttime(reatta, stime1)\n",
    "        redefe[\"time\"] = ttime(redefe, stime2)\n",
    "        reatta[\"moves\"] = tmoves(reatta, ia)\n",
    "        redefe[\"moves\"] = tmoves(redefe, id)\n",
    "        reatta[\"speed\"] = tper1move(reatta)\n",
    "        redefe[\"speed\"] = tper1move(redefe)\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def agent_work(Agent, board, mark, config):\n",
    "        obs = Obs(board, mark)\n",
    "        start = time.time()\n",
    "        im = Agent(obs, config)\n",
    "        t = time.time() - start\n",
    "        board_next = inject(im, obs.mark, board)\n",
    "        return im, t, board_next\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def write_Connect4(ims):\n",
    "        #     with open(\"Connect4.txt\", \"w\") as file:\n",
    "        #         file.write(str(ims))\n",
    "        pass \n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def write_uraConnect(ura_ims, Attack, Defense):\n",
    "        #     try:\n",
    "        #         with open(\"uraConnect.txt\", \"w\") as file:\n",
    "        #             stf = str(ura_ims) + '\\n' + name(Attack) + ' - ' + name(Defense)\n",
    "        #             file.write(stf)\n",
    "        #     finally: return\n",
    "        pass\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def name(agent): return agent.__name__.replace(\"my_\",\"\")\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    Agents = list_of_agents_to_trace # [ag1,ag2,..,agn]\n",
    "\n",
    "    observatory = Agents[0] if only_observation_first_agent and len(Agents) > 1 else None\n",
    "\n",
    "    total_points = { name(agent):{\"T\":[],\"games\":[[],[]],\"time\":[],\"moves\":[],\"speed\":[]} for agent in Agents }\n",
    "\n",
    "    ig, board__0 = 1, [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for home in Agents:\n",
    "        for away in Agents:\n",
    "            if home!=away and (observatory==None or observatory!=None and (observatory==home or observatory==away)):\n",
    "                if ig==1:\n",
    "                    print(\"\\n\")\n",
    "                    print(ig-1, \":\")\n",
    "                    print('----------------')\n",
    "                    for itp in total_points.items(): print(itp)\n",
    "                    ig += 1\n",
    "                stime1, stime2, imts1, imts2, ims, imt, = 0, 0, [], [], [], [[], []]\n",
    "                board__1 = board__0\n",
    "                # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                if start_games_at_away_first_agent:\n",
    "                    Attack, Defense = away, home\n",
    "                else:\n",
    "                    Attack, Defense = home, away\n",
    "                # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "                total_points_Attack, total_points_Defense = [], []\n",
    "                print(\"--------------------------\")\n",
    "                print(Attack.__name__, \"  vs  \", Defense.__name__)\n",
    "                print(\"-----------------------------------\")\n",
    "                for i in range(1,22):\n",
    "                    im1, t1, board__2  = agent_work(Attack, board__1, 1, config)\n",
    "                    stime1 +=t1\n",
    "                    imt[0] = str(round(t1,0))\n",
    "                    ims.append(im1)\n",
    "                    imts1.append(imt[0])\n",
    "                    write_uraConnect (ims, Attack, Defense)\n",
    "                    if win_4o_in(board__2):\n",
    "                        rec(total_points, 3, 0, i, i-1, stime1, stime2)\n",
    "                        print('\\nwin in n win in n win in n win in n win in n win in n win in n win in n')\n",
    "                        print('   1 1  1   1    1     1     ', Attack.__name__, \"     1     1    1   1  1 1\")\n",
    "                        print('win in n win in n win in n win in n win in n win in n win in n win in n\\n')\n",
    "                        print_to_clip (board__2, ims, i, Attack, Defense, stime1, stime2, 3,0)\n",
    "                        write_Connect4 (ims)\n",
    "                        write_uraConnect (ims, Attack, Defense)\n",
    "                        time.sleep(1)\n",
    "                        break\n",
    "                    im2, t2, board__1 = agent_work(Defense, board__2, 2, config)\n",
    "                    stime2 +=t2\n",
    "                    imt[1] = str(round(t2, 0))\n",
    "                    ims.append(im2)\n",
    "                    imts2.append(imt[1])\n",
    "                    print('({0}) . {1} > {2}  {3} < {4} . ({5}) --- {6} : {7} '\n",
    "                          .format(round(t1,1), name(Attack), im1, im2, name(Defense), round(t2,1), i, str(ims).replace(\", \",\",\")))\n",
    "                    write_uraConnect (ims, Attack, Defense)\n",
    "                    if win_4o_in(board__1):\n",
    "                        rec(total_points, 0, 3, i, i, stime1, stime2)\n",
    "                        print('\\nwin in n win in n win in n win in n win in n win in n win in n win in n')\n",
    "                        print('   2 2  2   2    2     2     ', Defense.__name__,\"     2     2    2   2  2 2\")\n",
    "                        print('win in n win in n win in n win in n win in n win in n win in n win in n\\n')\n",
    "                        print_to_clip(board__1, ims, i, Attack, Defense, stime1, stime2, 0,3)\n",
    "                        write_Connect4 (ims)\n",
    "                        write_uraConnect(ims, Attack, Defense)\n",
    "                        time.sleep(1)\n",
    "                        break\n",
    "                if not win_4o_in(board__1) and not win_4o_in(board__2):\n",
    "                    rec(total_points, 1, 2, i, i, stime1, stime2)\n",
    "                    print('\\nDRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W')\n",
    "                    print('  0 0  0   0    0     ', Attack.__name__, \" - \", Defense.__name__, \"     0    0   0  0 0\")\n",
    "                    print('DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\\n')\n",
    "                    print_to_clip(board__1, ims, i, Attack, Defense, stime1, stime2, 1,2)\n",
    "                    write_Connect4 (ims)\n",
    "                    write_uraConnect(ims, Attack, Defense)\n",
    "                    time.sleep(3)\n",
    "                print(\"\\n\")\n",
    "                print(ig,\":\\n\") # , \": \", total_points,\n",
    "                for itp in total_points.items():\n",
    "                    print(itp)\n",
    "                print(\"\\n\")\n",
    "                ig +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be546bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:25:15.853220Z",
     "iopub.status.busy": "2024-02-03T19:25:15.851834Z",
     "iopub.status.idle": "2024-02-03T19:41:25.671353Z",
     "shell.execute_reply": "2024-02-03T19:41:25.670110Z"
    },
    "papermill": {
     "duration": 969.843568,
     "end_time": "2024-02-03T19:41:25.674601",
     "exception": false,
     "start_time": "2024-02-03T19:25:15.831033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0 :\n",
      "----------------\n",
      "('agent__treebased__dott__', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_2', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_4', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "--------------------------\n",
      "agent__treebased__dott__   vs   agent_MCTS_MatanTsipory_\n",
      "-----------------------------------\n",
      "(0.1) . agent__treebased__dott__ > 3  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 1 : [3,2] \n",
      "(0.1) . agent__treebased__dott__ > 3  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 2 : [3,2,3,3] \n",
      "(0.1) . agent__treebased__dott__ > 3  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 3 : [3,2,3,3,3,2] \n",
      "(0.1) . agent__treebased__dott__ > 3  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 4 : [3,2,3,3,3,2,3,2] \n",
      "(0.1) . agent__treebased__dott__ > 2  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 5 : [3,2,3,3,3,2,3,2,2,4] \n",
      "(0.1) . agent__treebased__dott__ > 2  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 6 : [3,2,3,3,3,2,3,2,2,4,2,4] \n",
      "(0.1) . agent__treebased__dott__ > 4  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 7 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4] \n",
      "(0.1) . agent__treebased__dott__ > 1  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 8 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0] \n",
      "(0.1) . agent__treebased__dott__ > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 9 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4] \n",
      "(0.1) . agent__treebased__dott__ > 0  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 10 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0] \n",
      "(0.1) . agent__treebased__dott__ > 4  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 11 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0,4,0] \n",
      "(0.1) . agent__treebased__dott__ > 2  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 12 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0,4,0,2,6] \n",
      "(0.1) . agent__treebased__dott__ > 6  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 13 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0,4,0,2,6,6,6] \n",
      "(0.1) . agent__treebased__dott__ > 0  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 14 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0,4,0,2,6,6,6,0,6] \n",
      "(0.1) . agent__treebased__dott__ > 0  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 15 : [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0,4,0,2,6,6,6,0,6,0,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent__treebased__dott__      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent__treebased__dott__ [3]   2.agent_MCTS_MatanTsipory_ [0]   1.Time = 1.17   2.Time = 15.04   q.moves: 16   1.speed = 0.1   2.speed = 0.9   \n",
      "\n",
      "Game: [3,2,3,3,3,2,3,2,2,4,2,4,4,4,1,0,3,4,0,0,4,0,2,6,6,6,0,6,0,5,5]   \n",
      "\n",
      "Board: [1,0,1,1,1,0,0,1,0,1,1,2,0,0,2,0,1,1,2,0,2,2,0,2,2,1,0,2,1,0,2,1,2,1,1,2,1,2,1,2,2,2] \n",
      "\n",
      "[1 0 1 1 1 0 0]\n",
      "[1 0 1 1 2 0 0]\n",
      "[2 0 1 1 2 0 2]\n",
      "[2 0 2 2 1 0 2]\n",
      "[1 0 2 1 2 1 1]\n",
      "[2 1 2 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "2 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [3], 'games': [[3], []], 'time': [1.17], 'moves': [16], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[], [0]], 'time': [15.04], 'moves': [15], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_2', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_4', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent__treebased__dott__   vs   MCTS_wAdaptive_Playouts_\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.0 stone_count 1 playouts 1 ***\n",
      "time_limit 0.35 my duration 4.170287132263184 x 4\n",
      "(0.1) . agent__treebased__dott__ > 3  4 < MCTS_wAdaptive_Playouts_ . (4.2) --- 1 : [3,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.5565610859728507 stone_count 3 playouts 442 ***\n",
      "time_limit 0.7 my duration 0.7007589340209961 x 3\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 2 : [3,4,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.8945147679324894 stone_count 5 playouts 474 ***\n",
      "time_limit 0.7 my duration 0.7009189128875732 x 3\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 3 : [3,4,3,3,3,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.9263351749539595 stone_count 7 playouts 543 ***\n",
      "time_limit 0.7 my duration 0.7024011611938477 x 4\n",
      "(0.1) . agent__treebased__dott__ > 3  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 4 : [3,4,3,3,3,3,3,4] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.328080229226361 stone_count 9 playouts 698 ***\n",
      "time_limit 0.7 my duration 0.7007846832275391 x 4\n",
      "(0.1) . agent__treebased__dott__ > 4  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 5 : [3,4,3,3,3,3,3,4,4,4] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.7868217054263567 stone_count 11 playouts 774 ***\n",
      "time_limit 0.7 my duration 0.7005560398101807 x 1\n",
      "(0.1) . agent__treebased__dott__ > 4  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 6 : [3,4,3,3,3,3,3,4,4,4,4,1] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9981701738334858 stone_count 13 playouts 1093 ***\n",
      "time_limit 0.7 my duration 0.7011549472808838 x 1\n",
      "(0.1) . agent__treebased__dott__ > 4  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 7 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9269662921348314 stone_count 15 playouts 1068 ***\n",
      "time_limit 0.7 my duration 0.7006847858428955 x 6\n",
      "(0.1) . agent__treebased__dott__ > 1  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 8 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9433797909407666 stone_count 17 playouts 1148 ***\n",
      "time_limit 0.7 my duration 0.700817346572876 x 1\n",
      "(0.1) . agent__treebased__dott__ > 1  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 9 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1] \n",
      "[0. 0. 0. 0.] root_rewards 1.94109396914446 stone_count 19 playouts 1426 ***\n",
      "time_limit 0.7 my duration 0.7009153366088867 x 6\n",
      "(0.1) . agent__treebased__dott__ > 1  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 10 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6] \n",
      "[0. 0. 0. 0.] root_rewards 1.9624819624819625 stone_count 21 playouts 2079 ***\n",
      "time_limit 0.7 my duration 0.7004261016845703 x 5\n",
      "(0.1) . agent__treebased__dott__ > 6  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 11 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5] \n",
      "[0. 0. 0. 0.] root_rewards 1.999342105263158 stone_count 23 playouts 3040 ***\n",
      "time_limit 0.7 my duration 0.7006125450134277 x 0\n",
      "(0.1) . agent__treebased__dott__ > 0  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 12 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0] \n",
      "[0. 0. 0. 0.] root_rewards 1.9988415870257747 stone_count 25 playouts 3453 ***\n",
      "time_limit 0.7 my duration 0.700568437576294 x 6\n",
      "(0.1) . agent__treebased__dott__ > 0  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 13 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 27 playouts 4319 ***\n",
      "time_limit 0.7 my duration 0.7006926536560059 x 0\n",
      "(0.1) . agent__treebased__dott__ > 0  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 14 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0] \n",
      "[0. 0.] root_rewards 2.0 stone_count 29 playouts 14020 ***\n",
      "time_limit 0.7 my duration 0.7004954814910889 x 6\n",
      "(0.1) . agent__treebased__dott__ > 0  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 15 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0,0,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00015234947204589844 x 5\n",
      "(0.1) . agent__treebased__dott__ > 6  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 16 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0,0,6,6,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001513957977294922 x 5\n",
      "(0.1) . agent__treebased__dott__ > 5  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 17 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0,0,6,6,5,5,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00013756752014160156 x 5\n",
      "(0.1) . agent__treebased__dott__ > 5  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 18 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0,0,6,6,5,5,5,5,5] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0002410411834716797 x 2\n",
      "(0.1) . agent__treebased__dott__ > 2  2 < MCTS_wAdaptive_Playouts_ . (0.0) --- 19 : [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0,0,6,6,5,5,5,5,5,2,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MCTS_wAdaptive_Playouts_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent__treebased__dott__ [0]   2.MCTS_wAdaptive_Playouts_ [3]   1.Time = 1.28   2.Time = 13.98   q.moves: 19   1.speed = 0.1   2.speed = 0.7   \n",
      "\n",
      "Game: [3,4,3,3,3,3,3,4,4,4,4,1,4,1,1,6,1,1,1,6,6,5,0,0,0,6,0,0,0,6,6,5,5,5,5,5,2,2]   \n",
      "\n",
      "Board: [1,1,0,1,1,2,1,2,2,0,2,1,1,2,1,1,0,1,2,2,2,1,1,0,2,1,1,1,2,2,2,1,2,2,2,1,2,1,1,2,2,2] \n",
      "\n",
      "[1 1 0 1 1 2 1]\n",
      "[2 2 0 2 1 1 2]\n",
      "[1 1 0 1 2 2 2]\n",
      "[1 1 0 2 1 1 1]\n",
      "[2 2 2 1 2 2 2]\n",
      "[1 2 1 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "3 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [3], 'games': [[3, 0], []], 'time': [2.45], 'moves': [35], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[], [0]], 'time': [15.04], 'moves': [15], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [3], 'games': [[], [3]], 'time': [13.98], 'moves': [19], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_2', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_4', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent__treebased__dott__   vs   MontyCarlo_JamesMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloNode: p2 action = 0 after 0 simulations in 1.015s\n",
      "(0.1) . agent__treebased__dott__ > 3  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 1 : [3,0] \n",
      "MontyCarloNode: p2 action = 3 after 215 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 2 : [3,0,3,3] \n",
      "MontyCarloNode: p2 action = 4 after 1638 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 3 : [3,0,3,3,3,4] \n",
      "MontyCarloNode: p2 action = 4 after 1699 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 4 : [3,0,3,3,3,4,3,4] \n",
      "MontyCarloNode: p2 action = 4 after 1653 simulations in 1.001s\n",
      "(0.1) . agent__treebased__dott__ > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 5 : [3,0,3,3,3,4,3,4,3,4] \n",
      "MontyCarloNode: p2 action = 0 after 1643 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 4  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 6 : [3,0,3,3,3,4,3,4,3,4,4,0] \n",
      "MontyCarloNode: p2 action = 5 after 1698 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 4  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 7 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5] \n",
      "MontyCarloNode: p2 action = 2 after 2000 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 4  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 8 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2] \n",
      "MontyCarloNode: p2 action = 6 after 2371 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 2  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 9 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6] \n",
      "MontyCarloNode: p2 action = 6 after 2081 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 0  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 10 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6] \n",
      "MontyCarloNode: p2 action = 2 after 2180 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 6  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 11 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2] \n",
      "MontyCarloNode: p2 action = 2 after 2327 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 2  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 12 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2,2,2] \n",
      "MontyCarloNode: p2 action = 1 after 3033 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 2  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 13 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2,2,2,2,1] \n",
      "MontyCarloNode: p2 action = 0 after 3515 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 0  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 14 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2,2,2,2,1,0,0] \n",
      "MontyCarloNode: p2 action = 6 after 3672 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 0  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 15 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2,2,2,2,1,0,0,0,6] \n",
      "MontyCarloNode: p2 action = 5 after 4191 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 16 : [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2,2,2,2,1,0,0,0,6,5,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MontyCarlo_JamesMcGuigan      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent__treebased__dott__ [0]   2.MontyCarlo_JamesMcGuigan [3]   1.Time = 1.14   2.Time = 16.02   q.moves: 16   1.speed = 0.1   2.speed = 1.0   \n",
      "\n",
      "Game: [3,0,3,3,3,4,3,4,3,4,4,0,4,5,4,2,2,6,0,6,6,2,2,2,2,1,0,0,0,6,5,5]   \n",
      "\n",
      "Board: [1,0,1,1,1,0,0,2,0,2,1,1,0,0,1,0,1,1,1,0,2,1,0,2,2,2,2,1,2,0,1,1,2,1,2,2,2,2,1,2,2,2] \n",
      "\n",
      "[1 0 1 1 1 0 0]\n",
      "[2 0 2 1 1 0 0]\n",
      "[1 0 1 1 1 0 2]\n",
      "[1 0 2 2 2 2 1]\n",
      "[2 0 1 1 2 1 2]\n",
      "[2 2 2 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "4 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [3], 'games': [[3, 0, 0], []], 'time': [3.59], 'moves': [51], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[], [0]], 'time': [15.04], 'moves': [15], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [3], 'games': [[], [3]], 'time': [13.98], 'moves': [19], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [3], 'games': [[], [3]], 'time': [16.02], 'moves': [16], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_2', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_4', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent__treebased__dott__   vs   MontyCarlo_BtS_JMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2397 simulations in 1.466s\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.5) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2157 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 2 : [3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1885 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 3  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 3 : [3,3,3,3,3,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1699 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 4 : [3,3,3,3,3,2,2,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1593 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 5 : [3,3,3,3,3,2,2,2,2,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1753 simulations in 1.001s\n",
      "(0.1) . agent__treebased__dott__ > 3  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 6 : [3,3,3,3,3,2,2,2,2,2,3,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 1890 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 5  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 7 : [3,3,3,3,3,2,2,2,2,2,3,2,5,5] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 4680 simulations in 1.000s\n",
      "(0.1) . agent__treebased__dott__ > 6  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 8 : [3,3,3,3,3,2,2,2,2,2,3,2,5,5,6,0] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent__treebased__dott__      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent__treebased__dott__ [3]   2.MontyCarlo_BtS_JMcGuigan [0]   1.Time = 0.7   2.Time = 8.47   q.moves: 9   1.speed = 0.1   2.speed = 0.9   \n",
      "\n",
      "Game: [3,3,3,3,3,2,2,2,2,2,3,2,5,5,6,0,4]   \n",
      "\n",
      "Board: [0,0,2,1,0,0,0,0,0,2,1,0,0,0,0,0,1,2,0,0,0,0,0,2,1,0,0,0,0,0,1,2,0,2,0,2,0,2,1,1,1,1] \n",
      "\n",
      "[0 0 2 1 0 0 0]\n",
      "[0 0 2 1 0 0 0]\n",
      "[0 0 1 2 0 0 0]\n",
      "[0 0 2 1 0 0 0]\n",
      "[0 0 1 2 0 2 0]\n",
      "[2 0 2 1 1 1 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "5 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [6], 'games': [[3, 0, 0, 3], []], 'time': [4.29], 'moves': [60], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[], [0]], 'time': [15.04], 'moves': [15], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [3], 'games': [[], [3]], 'time': [13.98], 'moves': [19], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [3], 'games': [[], [3]], 'time': [16.02], 'moves': [16], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0]], 'time': [8.47], 'moves': [8], 'speed': [1.1]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "('agent_TheFast_mini_Max_4', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent__treebased__dott__   vs   agent_TheFast_mini_Max_2\n",
      "-----------------------------------\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < agent_TheFast_mini_Max_2 . (0.0) --- 1 : [3,3] \n",
      "(0.1) . agent__treebased__dott__ > 3  3 < agent_TheFast_mini_Max_2 . (0.5) --- 2 : [3,3,3,3] \n",
      "(0.1) . agent__treebased__dott__ > 3  2 < agent_TheFast_mini_Max_2 . (0.5) --- 3 : [3,3,3,3,3,2] \n",
      "(0.1) . agent__treebased__dott__ > 3  4 < agent_TheFast_mini_Max_2 . (0.5) --- 4 : [3,3,3,3,3,2,3,4] \n",
      "(0.1) . agent__treebased__dott__ > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 5 : [3,3,3,3,3,2,3,4,2,2] \n",
      "(0.1) . agent__treebased__dott__ > 2  4 < agent_TheFast_mini_Max_2 . (0.5) --- 6 : [3,3,3,3,3,2,3,4,2,2,2,4] \n",
      "(0.1) . agent__treebased__dott__ > 4  1 < agent_TheFast_mini_Max_2 . (0.5) --- 7 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1] \n",
      "(0.1) . agent__treebased__dott__ > 1  4 < agent_TheFast_mini_Max_2 . (0.5) --- 8 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4] \n",
      "(0.1) . agent__treebased__dott__ > 4  2 < agent_TheFast_mini_Max_2 . (0.5) --- 9 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2] \n",
      "(0.1) . agent__treebased__dott__ > 4  1 < agent_TheFast_mini_Max_2 . (0.5) --- 10 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1] \n",
      "(0.1) . agent__treebased__dott__ > 1  2 < agent_TheFast_mini_Max_2 . (0.5) --- 11 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2] \n",
      "(0.1) . agent__treebased__dott__ > 1  0 < agent_TheFast_mini_Max_2 . (0.5) --- 12 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0] \n",
      "(0.1) . agent__treebased__dott__ > 6  6 < agent_TheFast_mini_Max_2 . (0.5) --- 13 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6] \n",
      "(0.1) . agent__treebased__dott__ > 6  1 < agent_TheFast_mini_Max_2 . (0.2) --- 14 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6,6,1] \n",
      "(0.1) . agent__treebased__dott__ > 0  0 < agent_TheFast_mini_Max_2 . (0.0) --- 15 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6,6,1,0,0] \n",
      "(0.1) . agent__treebased__dott__ > 6  0 < agent_TheFast_mini_Max_2 . (0.0) --- 16 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6,6,1,0,0,6,0] \n",
      "(0.1) . agent__treebased__dott__ > 0  0 < agent_TheFast_mini_Max_2 . (0.0) --- 17 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6,6,1,0,0,6,0,0,0] \n",
      "(0.1) . agent__treebased__dott__ > 5  5 < agent_TheFast_mini_Max_2 . (0.0) --- 18 : [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6,6,1,0,0,6,0,0,0,5,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_TheFast_mini_Max_2      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent__treebased__dott__ [0]   2.agent_TheFast_mini_Max_2 [3]   1.Time = 1.22   2.Time = 6.22   q.moves: 18   1.speed = 0.1   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,3,3,3,2,3,4,2,2,2,4,4,1,1,4,4,2,4,1,1,2,1,0,6,6,6,1,0,0,6,0,0,0,5,5]   \n",
      "\n",
      "Board: [2,2,2,1,1,0,0,1,1,2,1,1,0,0,2,1,1,2,2,0,1,2,2,2,1,1,0,1,1,1,1,2,2,2,2,2,2,2,1,2,1,1] \n",
      "\n",
      "[2 2 2 1 1 0 0]\n",
      "[1 1 2 1 1 0 0]\n",
      "[2 1 1 2 2 0 1]\n",
      "[2 2 2 1 1 0 1]\n",
      "[1 1 1 2 2 2 2]\n",
      "[2 2 2 1 2 1 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "6 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [6], 'games': [[3, 0, 0, 3, 0], []], 'time': [5.51], 'moves': [78], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[], [0]], 'time': [15.04], 'moves': [15], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [3], 'games': [[], [3]], 'time': [13.98], 'moves': [19], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [3], 'games': [[], [3]], 'time': [16.02], 'moves': [16], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0]], 'time': [8.47], 'moves': [8], 'speed': [1.1]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [3], 'games': [[], [3]], 'time': [6.22], 'moves': [18], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [], 'games': [[], []], 'time': [], 'moves': [], 'speed': []})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent__treebased__dott__   vs   agent_TheFast_mini_Max_4\n",
      "-----------------------------------\n",
      "(0.1) . agent__treebased__dott__ > 3  3 < agent_TheFast_mini_Max_4 . (0.0) --- 1 : [3,3] \n",
      "(0.1) . agent__treebased__dott__ > 3  3 < agent_TheFast_mini_Max_4 . (0.5) --- 2 : [3,3,3,3] \n",
      "(0.1) . agent__treebased__dott__ > 3  2 < agent_TheFast_mini_Max_4 . (0.5) --- 3 : [3,3,3,3,3,2] \n",
      "(0.1) . agent__treebased__dott__ > 2  1 < agent_TheFast_mini_Max_4 . (0.5) --- 4 : [3,3,3,3,3,2,2,1] \n",
      "(0.1) . agent__treebased__dott__ > 3  2 < agent_TheFast_mini_Max_4 . (0.5) --- 5 : [3,3,3,3,3,2,2,1,3,2] \n",
      "(0.1) . agent__treebased__dott__ > 1  2 < agent_TheFast_mini_Max_4 . (0.5) --- 6 : [3,3,3,3,3,2,2,1,3,2,1,2] \n",
      "(0.1) . agent__treebased__dott__ > 2  4 < agent_TheFast_mini_Max_4 . (0.5) --- 7 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4] \n",
      "(0.1) . agent__treebased__dott__ > 2  6 < agent_TheFast_mini_Max_4 . (0.5) --- 8 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6] \n",
      "(0.1) . agent__treebased__dott__ > 4  0 < agent_TheFast_mini_Max_4 . (0.5) --- 9 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0] \n",
      "(0.1) . agent__treebased__dott__ > 4  4 < agent_TheFast_mini_Max_4 . (0.5) --- 10 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4] \n",
      "(0.1) . agent__treebased__dott__ > 4  4 < agent_TheFast_mini_Max_4 . (0.5) --- 11 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4] \n",
      "(0.1) . agent__treebased__dott__ > 0  0 < agent_TheFast_mini_Max_4 . (0.5) --- 12 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0] \n",
      "(0.1) . agent__treebased__dott__ > 0  1 < agent_TheFast_mini_Max_4 . (0.1) --- 13 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1] \n",
      "(0.1) . agent__treebased__dott__ > 1  1 < agent_TheFast_mini_Max_4 . (0.0) --- 14 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1,1,1] \n",
      "(0.1) . agent__treebased__dott__ > 6  6 < agent_TheFast_mini_Max_4 . (0.1) --- 15 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1,1,1,6,6] \n",
      "(0.1) . agent__treebased__dott__ > 6  1 < agent_TheFast_mini_Max_4 . (0.0) --- 16 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1,1,1,6,6,6,1] \n",
      "(0.1) . agent__treebased__dott__ > 6  5 < agent_TheFast_mini_Max_4 . (0.0) --- 17 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1,1,1,6,6,6,1,6,5] \n",
      "(0.1) . agent__treebased__dott__ > 0  5 < agent_TheFast_mini_Max_4 . (0.0) --- 18 : [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1,1,1,6,6,6,1,6,5,0,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent__treebased__dott__      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent__treebased__dott__ [3]   2.agent_TheFast_mini_Max_4 [0]   1.Time = 1.3   2.Time = 5.75   q.moves: 19   1.speed = 0.1   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,3,3,3,2,2,1,3,2,1,2,2,4,2,6,4,0,4,4,4,4,0,0,0,1,1,1,6,6,6,1,6,5,0,5,5]   \n",
      "\n",
      "Board: [0,2,1,1,2,0,0,1,2,1,1,1,0,1,1,1,2,2,2,0,1,2,2,2,1,1,1,2,1,1,1,2,1,2,1,2,2,2,1,2,2,2] \n",
      "\n",
      "[0 2 1 1 2 0 0]\n",
      "[1 2 1 1 1 0 1]\n",
      "[1 1 2 2 2 0 1]\n",
      "[2 2 2 1 1 1 2]\n",
      "[1 1 1 2 1 2 1]\n",
      "[2 2 2 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "7 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [9], 'games': [[3, 0, 0, 3, 0, 3], []], 'time': [6.81], 'moves': [97], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[], [0]], 'time': [15.04], 'moves': [15], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [3], 'games': [[], [3]], 'time': [13.98], 'moves': [19], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [3], 'games': [[], [3]], 'time': [16.02], 'moves': [16], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0]], 'time': [8.47], 'moves': [8], 'speed': [1.1]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [3], 'games': [[], [3]], 'time': [6.22], 'moves': [18], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [0], 'games': [[], [0]], 'time': [5.75], 'moves': [18], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_MCTS_MatanTsipory_   vs   agent__treebased__dott__\n",
      "-----------------------------------\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  3 < agent__treebased__dott__ . (0.1) --- 1 : [3,3] \n",
      "(1.3) . agent_MCTS_MatanTsipory_ > 4  5 < agent__treebased__dott__ . (0.1) --- 2 : [3,3,4,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  1 < agent__treebased__dott__ . (0.1) --- 3 : [3,3,4,5,2,1] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  3 < agent__treebased__dott__ . (0.1) --- 4 : [3,3,4,5,2,1,4,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  2 < agent__treebased__dott__ . (0.1) --- 5 : [3,3,4,5,2,1,4,3,2,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  4 < agent__treebased__dott__ . (0.1) --- 6 : [3,3,4,5,2,1,4,3,2,2,4,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  3 < agent__treebased__dott__ . (0.1) --- 7 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  6 < agent__treebased__dott__ . (0.1) --- 8 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  3 < agent__treebased__dott__ . (0.1) --- 9 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  4 < agent__treebased__dott__ . (0.1) --- 10 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4] \n",
      "(1.1) . agent_MCTS_MatanTsipory_ > 6  5 < agent__treebased__dott__ . (0.1) --- 11 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < agent__treebased__dott__ . (0.1) --- 12 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < agent__treebased__dott__ . (0.1) --- 13 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < agent__treebased__dott__ . (0.1) --- 14 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  2 < agent__treebased__dott__ . (0.1) --- 15 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  1 < agent__treebased__dott__ . (0.1) --- 16 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2,2,1] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  6 < agent__treebased__dott__ . (0.1) --- 17 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2,2,1,6,6] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  5 < agent__treebased__dott__ . (0.1) --- 18 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2,2,1,6,6,1,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  5 < agent__treebased__dott__ . (0.1) --- 19 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2,2,1,6,6,1,5,1,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  6 < agent__treebased__dott__ . (0.1) --- 20 : [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2,2,1,6,6,1,5,1,5,6,6] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent__treebased__dott__      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_MCTS_MatanTsipory_ [0]   2.agent__treebased__dott__ [3]   1.Time = 20.39   2.Time = 1.42   q.moves: 20   1.speed = 1.0   2.speed = 0.1   \n",
      "\n",
      "Game: [3,3,4,5,2,1,4,3,2,2,4,4,3,3,5,6,5,3,4,4,6,5,0,0,0,0,0,0,2,2,2,1,6,6,1,5,1,5,6,6]   \n",
      "\n",
      "Board: [2,0,1,2,2,2,2,1,0,2,2,1,2,1,2,1,1,1,2,2,2,1,1,2,2,1,1,1,2,2,1,2,1,1,1,1,2,1,1,1,2,2] \n",
      "\n",
      "[2 0 1 2 2 2 2]\n",
      "[1 0 2 2 1 2 1]\n",
      "[2 1 1 1 2 2 2]\n",
      "[1 1 2 2 1 1 1]\n",
      "[2 2 1 2 1 1 1]\n",
      "[1 2 1 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "8 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3]], 'time': [8.23], 'moves': [117], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[0], [0]], 'time': [35.43], 'moves': [35], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [3], 'games': [[], [3]], 'time': [13.98], 'moves': [19], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [3], 'games': [[], [3]], 'time': [16.02], 'moves': [16], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0]], 'time': [8.47], 'moves': [8], 'speed': [1.1]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [3], 'games': [[], [3]], 'time': [6.22], 'moves': [18], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [0], 'games': [[], [0]], 'time': [5.75], 'moves': [18], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_MCTS_MatanTsipory_   vs   MCTS_wAdaptive_Playouts_\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.062780269058296 stone_count 1 playouts 223 ***\n",
      "time_limit 0.35 my duration 0.35135865211486816 x 2\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  2 < MCTS_wAdaptive_Playouts_ . (0.4) --- 1 : [3,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9298642533936652 stone_count 3 playouts 442 ***\n",
      "time_limit 0.7 my duration 0.7016415596008301 x 3\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 2 : [3,2,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.6799276672694394 stone_count 5 playouts 553 ***\n",
      "time_limit 0.7 my duration 0.7018194198608398 x 3\n",
      "(1.3) . agent_MCTS_MatanTsipory_ > 4  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 3 : [3,2,3,3,4,3] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00026035308837890625 x 5\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 4 : [3,2,3,3,4,3,6,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9093137254901962 stone_count 9 playouts 816 ***\n",
      "time_limit 0.7 my duration 0.7012803554534912 x 5\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 5 : [3,2,3,3,4,3,6,5,4,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9194476409666283 stone_count 11 playouts 869 ***\n",
      "time_limit 0.7 my duration 0.7006776332855225 x 4\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 6 : [3,2,3,3,4,3,6,5,4,5,5,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9475703324808185 stone_count 13 playouts 782 ***\n",
      "time_limit 0.7 my duration 0.7012696266174316 x 3\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 7 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.8856172140430352 stone_count 15 playouts 883 ***\n",
      "time_limit 0.7 my duration 0.700913667678833 x 5\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 8 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.964349376114082 stone_count 17 playouts 1122 ***\n",
      "time_limit 0.7 my duration 0.7009658813476562 x 5\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 9 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5] \n",
      "[0. 0. 0. 0.] root_rewards 1.9851652056641942 stone_count 19 playouts 1483 ***\n",
      "time_limit 0.7 my duration 0.7008309364318848 x 2\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 10 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2] \n",
      "[0. 0. 0. 0.] root_rewards 2.0 stone_count 21 playouts 2362 ***\n",
      "time_limit 0.7 my duration 0.7008543014526367 x 0\n",
      "(1.4) . agent_MCTS_MatanTsipory_ > 4  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 11 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 23 playouts 3254 ***\n",
      "time_limit 0.7 my duration 0.7005836963653564 x 0\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 12 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 25 playouts 3966 ***\n",
      "time_limit 0.7 my duration 0.7011232376098633 x 0\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 13 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0,0,0] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0006210803985595703 x 6\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  6 < MCTS_wAdaptive_Playouts_ . (0.0) --- 14 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0,0,0,6,6] \n",
      "[0. 0. 0. 0.] root_rewards 1.9996774713755845 stone_count 29 playouts 6201 ***\n",
      "time_limit 0.7 my duration 0.7006056308746338 x 5\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 15 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0,0,0,6,6,6,5] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 31 playouts 15118 ***\n",
      "time_limit 0.7 my duration 0.7005112171173096 x 6\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 16 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0,0,0,6,6,6,5,0,6] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0002613067626953125 x 1\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  1 < MCTS_wAdaptive_Playouts_ . (0.0) --- 17 : [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0,0,0,6,6,6,5,0,6,1,1] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MCTS_wAdaptive_Playouts_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_MCTS_MatanTsipory_ [0]   2.MCTS_wAdaptive_Playouts_ [3]   1.Time = 17.7   2.Time = 9.47   q.moves: 17   1.speed = 1.0   2.speed = 0.6   \n",
      "\n",
      "Game: [3,2,3,3,4,3,6,5,4,5,5,4,4,3,3,5,6,5,2,2,4,0,4,0,0,0,6,6,6,5,0,6,1,1]   \n",
      "\n",
      "Board: [0,0,0,1,1,2,2,1,0,0,2,1,2,1,2,0,0,2,1,2,2,1,0,2,2,2,1,1,2,2,1,1,1,2,1,2,1,2,1,1,2,1] \n",
      "\n",
      "[0 0 0 1 1 2 2]\n",
      "[1 0 0 2 1 2 1]\n",
      "[2 0 0 2 1 2 2]\n",
      "[1 0 2 2 2 1 1]\n",
      "[2 2 1 1 1 2 1]\n",
      "[2 1 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "9 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3]], 'time': [8.23], 'moves': [117], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [0], 'games': [[0, 0], [0]], 'time': [53.13], 'moves': [52], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [6], 'games': [[], [3, 3]], 'time': [23.45], 'moves': [36], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [3], 'games': [[], [3]], 'time': [16.02], 'moves': [16], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0]], 'time': [8.47], 'moves': [8], 'speed': [1.1]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [3], 'games': [[], [3]], 'time': [6.22], 'moves': [18], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [0], 'games': [[], [0]], 'time': [5.75], 'moves': [18], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_MCTS_MatanTsipory_   vs   MontyCarlo_JamesMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloNode: p2 action = 4 after 1742 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 1 : [3,4] \n",
      "MontyCarloNode: p2 action = 3 after 1776 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 2 : [3,4,3,3] \n",
      "MontyCarloNode: p2 action = 4 after 1707 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 3 : [3,4,3,3,3,4] \n",
      "MontyCarloNode: p2 action = 4 after 1712 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 4 : [3,4,3,3,3,4,4,4] \n",
      "MontyCarloNode: p2 action = 0 after 1727 simulations in 1.001s\n",
      "(1.6) . agent_MCTS_MatanTsipory_ > 1  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 5 : [3,4,3,3,3,4,4,4,1,0] \n",
      "MontyCarloNode: p2 action = 1 after 1818 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 6 : [3,4,3,3,3,4,4,4,1,0,1,1] \n",
      "MontyCarloNode: p2 action = 1 after 1799 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 7 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1] \n",
      "MontyCarloNode: p2 action = 6 after 1877 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 8 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6] \n",
      "MontyCarloNode: p2 action = 6 after 1878 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 9 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6] \n",
      "MontyCarloNode: p2 action = 2 after 2005 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 10 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2] \n",
      "MontyCarloNode: p2 action = 2 after 1807 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 11 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2] \n",
      "MontyCarloNode: p2 action = 2 after 1472 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 12 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2] \n",
      "MontyCarloNode: p2 action = 6 after 1632 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 13 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6] \n",
      "MontyCarloNode: p2 action = 6 after 1608 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 14 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6] \n",
      "MontyCarloNode: p2 action = 0 after 1723 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 15 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0] \n",
      "MontyCarloNode: p2 action = 1 after 1847 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 16 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1] \n",
      "MontyCarloNode: p2 action = 6 after 2138 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 17 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1,5,6] \n",
      "MontyCarloNode: p2 action = 0 after 2484 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 18 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1,5,6,0,0] \n",
      "MontyCarloNode: p2 action = 2 after 2669 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 19 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1,5,6,0,0,4,2] \n",
      "MontyCarloNode: p2 action = 5 after 2910 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 20 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1,5,6,0,0,4,2,5,5] \n",
      "MontyCarloNode: p2 action = 5 after 2943 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 21 : [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1,5,6,0,0,4,2,5,5,5,5] \n",
      "\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "  0 0  0   0    0      agent_MCTS_MatanTsipory_  -  MontyCarlo_JamesMcGuigan      0    0   0  0 0\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "\n",
      "1.agent_MCTS_MatanTsipory_ [1]   2.MontyCarlo_JamesMcGuigan [2]   1.Time = 21.56   2.Time = 21.01   q.moves: 21   1.speed = 1.0   2.speed = 1.0   \n",
      "\n",
      "Game: [3,4,3,3,3,4,4,4,1,0,1,1,0,1,1,6,0,6,2,2,4,2,2,2,3,6,6,6,3,0,5,1,5,6,0,0,4,2,5,5,5,5]   \n",
      "\n",
      "Board: [2,2,2,1,1,2,2,1,1,2,1,1,1,2,2,2,1,1,2,2,1,1,2,2,2,1,1,2,1,1,2,1,2,1,2,2,1,1,1,2,1,2] \n",
      "\n",
      "[2 2 2 1 1 2 2]\n",
      "[1 1 2 1 1 1 2]\n",
      "[2 2 1 1 2 2 1]\n",
      "[1 2 2 2 1 1 2]\n",
      "[1 1 2 1 2 1 2]\n",
      "[2 1 1 1 2 1 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "10 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3]], 'time': [8.23], 'moves': [117], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [1], 'games': [[0, 0, 1], [0]], 'time': [74.69], 'moves': [73], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [6], 'games': [[], [3, 3]], 'time': [23.45], 'moves': [36], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [5], 'games': [[], [3, 2]], 'time': [37.03], 'moves': [37], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0]], 'time': [8.47], 'moves': [8], 'speed': [1.1]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [3], 'games': [[], [3]], 'time': [6.22], 'moves': [18], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [0], 'games': [[], [0]], 'time': [5.75], 'moves': [18], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_MCTS_MatanTsipory_   vs   MontyCarlo_BtS_JMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 370 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 2775 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 2 : [3,3,2,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 2405 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 3 : [3,3,2,4,1,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2076 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 4 : [3,3,2,4,1,0,4,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 955 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 5 : [3,3,2,4,1,0,4,3,3,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 2008 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 6 : [3,3,2,4,1,0,4,3,3,4,1,1] \n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 1791 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 7 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1828 simulations in 1.001s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 8 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1802 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 9 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 1908 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 10 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 1942 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 11 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 1913 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 12 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1] \n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 1921 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 13 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1,0,5] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 1968 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 14 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1,0,5,5,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 3129 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 15 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1,0,5,5,6,0,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 3474 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 16 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1,0,5,5,6,0,6,5,5] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 3438 simulations in 1.000s\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 17 : [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1,0,5,5,6,0,6,5,5,5,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent_MCTS_MatanTsipory_      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_MCTS_MatanTsipory_ [3]   2.MontyCarlo_BtS_JMcGuigan [0]   1.Time = 18.0   2.Time = 17.01   q.moves: 18   1.speed = 1.0   2.speed = 0.9   \n",
      "\n",
      "Game: [3,3,2,4,1,0,4,3,3,4,1,1,4,1,4,3,1,3,0,0,0,4,5,1,0,5,5,6,0,6,5,5,5,2,2]   \n",
      "\n",
      "Board: [1,2,0,2,2,1,0,1,1,0,2,1,2,0,1,2,0,1,1,1,0,2,2,1,2,2,1,0,1,1,2,2,1,2,2,2,1,1,1,2,1,2] \n",
      "\n",
      "[1 2 0 2 2 1 0]\n",
      "[1 1 0 2 1 2 0]\n",
      "[1 2 0 1 1 1 0]\n",
      "[2 2 1 2 2 1 0]\n",
      "[1 1 2 2 1 2 2]\n",
      "[2 1 1 1 2 1 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "11 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3]], 'time': [8.23], 'moves': [117], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3], [0]], 'time': [92.69], 'moves': [91], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [6], 'games': [[], [3, 3]], 'time': [23.45], 'moves': [36], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [5], 'games': [[], [3, 2]], 'time': [37.03], 'moves': [37], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0, 0]], 'time': [25.48], 'moves': [25], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [3], 'games': [[], [3]], 'time': [6.22], 'moves': [18], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [0], 'games': [[], [0]], 'time': [5.75], 'moves': [18], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_MCTS_MatanTsipory_   vs   agent_TheFast_mini_Max_2\n",
      "-----------------------------------\n",
      "(1.5) . agent_MCTS_MatanTsipory_ > 3  3 < agent_TheFast_mini_Max_2 . (0.0) --- 1 : [3,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  5 < agent_TheFast_mini_Max_2 . (0.5) --- 2 : [3,3,4,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  3 < agent_TheFast_mini_Max_2 . (0.5) --- 3 : [3,3,4,5,4,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  4 < agent_TheFast_mini_Max_2 . (0.5) --- 4 : [3,3,4,5,4,3,4,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 1  2 < agent_TheFast_mini_Max_2 . (0.5) --- 5 : [3,3,4,5,4,3,4,4,1,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  5 < agent_TheFast_mini_Max_2 . (0.5) --- 6 : [3,3,4,5,4,3,4,4,1,2,3,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  2 < agent_TheFast_mini_Max_2 . (0.5) --- 7 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  1 < agent_TheFast_mini_Max_2 . (0.5) --- 8 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 9 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  1 < agent_TheFast_mini_Max_2 . (0.5) --- 10 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1] \n",
      "(1.6) . agent_MCTS_MatanTsipory_ > 1  4 < agent_TheFast_mini_Max_2 . (0.5) --- 11 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  2 < agent_TheFast_mini_Max_2 . (0.5) --- 12 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  5 < agent_TheFast_mini_Max_2 . (0.5) --- 13 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  4 < agent_TheFast_mini_Max_2 . (0.5) --- 14 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5,5,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  1 < agent_TheFast_mini_Max_2 . (0.1) --- 15 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5,5,4,6,1] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  6 < agent_TheFast_mini_Max_2 . (0.0) --- 16 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5,5,4,6,1,6,6] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  1 < agent_TheFast_mini_Max_2 . (0.0) --- 17 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5,5,4,6,1,6,6,6,1] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < agent_TheFast_mini_Max_2 . (0.0) --- 18 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5,5,4,6,1,6,6,6,1,0,0] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_TheFast_mini_Max_2      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_MCTS_MatanTsipory_ [0]   2.agent_TheFast_mini_Max_2 [3]   1.Time = 19.06   2.Time = 6.59   q.moves: 18   1.speed = 1.1   2.speed = 0.4   \n",
      "\n",
      "Game: [3,3,4,5,4,3,4,4,1,2,3,5,5,2,2,1,2,2,3,1,1,4,3,2,6,5,5,4,6,1,6,6,6,1,0,0]   \n",
      "\n",
      "Board: [0,2,2,1,2,0,0,0,2,2,1,2,1,1,0,1,1,1,2,2,2,0,2,1,2,1,1,1,2,2,2,2,1,2,1,1,1,2,1,1,2,1] \n",
      "\n",
      "[0 2 2 1 2 0 0]\n",
      "[0 2 2 1 2 1 1]\n",
      "[0 1 1 1 2 2 2]\n",
      "[0 2 1 2 1 1 1]\n",
      "[2 2 2 2 1 2 1]\n",
      "[1 1 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "12 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3]], 'time': [8.23], 'moves': [117], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0], [0]], 'time': [111.75], 'moves': [109], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [6], 'games': [[], [3, 3]], 'time': [23.45], 'moves': [36], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [5], 'games': [[], [3, 2]], 'time': [37.03], 'moves': [37], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0, 0]], 'time': [25.48], 'moves': [25], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3]], 'time': [12.81], 'moves': [36], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [0], 'games': [[], [0]], 'time': [5.75], 'moves': [18], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_MCTS_MatanTsipory_   vs   agent_TheFast_mini_Max_4\n",
      "-----------------------------------\n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  3 < agent_TheFast_mini_Max_4 . (0.0) --- 1 : [3,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  1 < agent_TheFast_mini_Max_4 . (0.5) --- 2 : [3,3,2,1] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  5 < agent_TheFast_mini_Max_4 . (0.5) --- 3 : [3,3,2,1,4,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  3 < agent_TheFast_mini_Max_4 . (0.5) --- 4 : [3,3,2,1,4,5,2,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  4 < agent_TheFast_mini_Max_4 . (0.5) --- 5 : [3,3,2,1,4,5,2,3,3,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  2 < agent_TheFast_mini_Max_4 . (0.5) --- 6 : [3,3,2,1,4,5,2,3,3,4,5,2] \n",
      "(1.5) . agent_MCTS_MatanTsipory_ > 2  4 < agent_TheFast_mini_Max_4 . (0.5) --- 7 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  4 < agent_TheFast_mini_Max_4 . (0.5) --- 8 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 4  5 < agent_TheFast_mini_Max_4 . (0.5) --- 9 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 3  4 < agent_TheFast_mini_Max_4 . (0.5) --- 10 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 5  2 < agent_TheFast_mini_Max_4 . (0.5) --- 11 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 2  3 < agent_TheFast_mini_Max_4 . (0.5) --- 12 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  5 < agent_TheFast_mini_Max_4 . (0.2) --- 13 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  0 < agent_TheFast_mini_Max_4 . (0.0) --- 14 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5,0,0] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  0 < agent_TheFast_mini_Max_4 . (0.0) --- 15 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5,0,0,6,0] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  0 < agent_TheFast_mini_Max_4 . (0.0) --- 16 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5,0,0,6,0,6,0] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 0  6 < agent_TheFast_mini_Max_4 . (0.0) --- 17 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5,0,0,6,0,6,0,0,6] \n",
      "(1.0) . agent_MCTS_MatanTsipory_ > 6  6 < agent_TheFast_mini_Max_4 . (0.0) --- 18 : [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5,0,0,6,0,6,0,0,6,6,6] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_TheFast_mini_Max_4      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_MCTS_MatanTsipory_ [0]   2.agent_TheFast_mini_Max_4 [3]   1.Time = 18.5   2.Time = 5.7   q.moves: 18   1.speed = 1.0   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,2,1,4,5,2,3,3,4,5,2,2,4,5,4,4,5,3,4,5,2,2,3,0,5,0,0,6,0,6,0,0,6,6,6]   \n",
      "\n",
      "Board: [1,0,1,2,2,2,0,2,0,2,1,1,1,2,2,0,1,1,2,2,1,2,0,2,2,2,1,2,1,0,1,2,2,1,1,1,2,1,1,1,2,1] \n",
      "\n",
      "[1 0 1 2 2 2 0]\n",
      "[2 0 2 1 1 1 2]\n",
      "[2 0 1 1 2 2 1]\n",
      "[2 0 2 2 2 1 2]\n",
      "[1 0 1 2 2 1 1]\n",
      "[1 2 1 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "13 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3]], 'time': [8.23], 'moves': [117], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0]], 'time': [130.25], 'moves': [127], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [6], 'games': [[], [3, 3]], 'time': [23.45], 'moves': [36], 'speed': [0.7]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [5], 'games': [[], [3, 2]], 'time': [37.03], 'moves': [37], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0, 0]], 'time': [25.48], 'moves': [25], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3]], 'time': [12.81], 'moves': [36], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [3], 'games': [[], [0, 3]], 'time': [11.45], 'moves': [36], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MCTS_wAdaptive_Playouts_   vs   agent__treebased__dott__\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9804878048780488 stone_count 0 playouts 205 ***\n",
      "time_limit 0.35 my duration 0.3520660400390625 x 3\n",
      "(0.4) . MCTS_wAdaptive_Playouts_ > 3  3 < agent__treebased__dott__ . (0.1) --- 1 : [3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0078895463510849 stone_count 2 playouts 507 ***\n",
      "time_limit 0.7 my duration 0.7022466659545898 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  2 < agent__treebased__dott__ . (0.1) --- 2 : [3,3,4,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7392120075046904 stone_count 4 playouts 533 ***\n",
      "time_limit 0.7 my duration 0.7017707824707031 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  5 < agent__treebased__dott__ . (0.1) --- 3 : [3,3,4,2,6,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.4081967213114754 stone_count 6 playouts 610 ***\n",
      "time_limit 0.7 my duration 0.7011013031005859 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  3 < agent__treebased__dott__ . (0.1) --- 4 : [3,3,4,2,6,5,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0454545454545454 stone_count 8 playouts 572 ***\n",
      "time_limit 0.7 my duration 0.7012667655944824 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  4 < agent__treebased__dott__ . (0.1) --- 5 : [3,3,4,2,6,5,3,3,4,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.5619047619047619 stone_count 10 playouts 630 ***\n",
      "time_limit 0.7 my duration 0.7010838985443115 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  3 < agent__treebased__dott__ . (0.1) --- 6 : [3,3,4,2,6,5,3,3,4,4,4,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.1769662921348314 stone_count 12 playouts 712 ***\n",
      "time_limit 0.7 my duration 0.701197624206543 x 2\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  1 < agent__treebased__dott__ . (0.1) --- 7 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.11467324290998766 stone_count 14 playouts 811 ***\n",
      "time_limit 0.7 my duration 0.7005538940429688 x 5\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  2 < agent__treebased__dott__ . (0.1) --- 8 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.04250386398763524 stone_count 16 playouts 1294 ***\n",
      "time_limit 0.7 my duration 0.7010278701782227 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  2 < agent__treebased__dott__ . (0.1) --- 9 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.0413589364844904 stone_count 18 playouts 1354 ***\n",
      "time_limit 0.7 my duration 0.7004220485687256 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  2 < agent__treebased__dott__ . (0.1) --- 10 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00015401840209960938 x 2\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 2  3 < agent__treebased__dott__ . (0.1) --- 11 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3] \n",
      "[0. 0. 0. 0.] root_rewards 0.0 stone_count 22 playouts 3868 ***\n",
      "time_limit 0.7 my duration 0.7007772922515869 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  0 < agent__treebased__dott__ . (0.1) --- 12 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0] \n",
      "[0. 0.] root_rewards 0.0006654466810846781 stone_count 24 playouts 6011 ***\n",
      "time_limit 0.7 my duration 0.7004995346069336 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  5 < agent__treebased__dott__ . (0.1) --- 13 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001380443572998047 x 5\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 5  5 < agent__treebased__dott__ . (0.1) --- 14 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 28 playouts 4743 ***\n",
      "time_limit 0.7 my duration 0.7007935047149658 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  0 < agent__treebased__dott__ . (0.1) --- 15 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5,0,0] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 30 playouts 8299 ***\n",
      "time_limit 0.7 my duration 0.7005548477172852 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  5 < agent__treebased__dott__ . (0.1) --- 16 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5,0,0,0,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00013065338134765625 x 6\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 6  6 < agent__treebased__dott__ . (0.1) --- 17 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5,0,0,0,5,6,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00011658668518066406 x 6\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 6  6 < agent__treebased__dott__ . (0.1) --- 18 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5,0,0,0,5,6,6,6,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00011515617370605469 x 6\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 6  1 < agent__treebased__dott__ . (0.1) --- 19 : [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5,0,0,0,5,6,6,6,6,6,1] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001418590545654297 x 1\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MCTS_wAdaptive_Playouts_      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MCTS_wAdaptive_Playouts_ [3]   2.agent__treebased__dott__ [0]   1.Time = 9.47   2.Time = 1.4   q.moves: 20   1.speed = 0.5   2.speed = 0.1   \n",
      "\n",
      "Game: [3,3,4,2,6,5,3,3,4,4,4,3,2,1,5,2,4,2,0,2,2,3,4,0,0,5,5,5,0,0,0,5,6,6,6,6,6,1,1]   \n",
      "\n",
      "Board: [1,0,1,2,1,2,1,2,0,2,2,1,2,2,1,0,2,2,1,1,1,1,1,2,1,2,2,2,2,2,1,2,1,1,1,1,2,2,1,1,2,1] \n",
      "\n",
      "[1 0 1 2 1 2 1]\n",
      "[2 0 2 2 1 2 2]\n",
      "[1 0 2 2 1 1 1]\n",
      "[1 1 2 1 2 2 2]\n",
      "[2 2 1 2 1 1 1]\n",
      "[1 2 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "14 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0]], 'time': [9.63], 'moves': [136], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0]], 'time': [130.25], 'moves': [127], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [9], 'games': [[3], [3, 3]], 'time': [32.92], 'moves': [56], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [5], 'games': [[], [3, 2]], 'time': [37.03], 'moves': [37], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0, 0]], 'time': [25.48], 'moves': [25], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3]], 'time': [12.81], 'moves': [36], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [3], 'games': [[], [0, 3]], 'time': [11.45], 'moves': [36], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MCTS_wAdaptive_Playouts_   vs   agent_MCTS_MatanTsipory_\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.8769230769230769 stone_count 0 playouts 195 ***\n",
      "time_limit 0.35 my duration 0.35117030143737793 x 3\n",
      "(0.4) . MCTS_wAdaptive_Playouts_ > 3  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 1 : [3,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.6559485530546625 stone_count 2 playouts 622 ***\n",
      "time_limit 0.7 my duration 0.7018227577209473 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 2 : [3,1,4,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9958333333333333 stone_count 4 playouts 480 ***\n",
      "time_limit 0.7 my duration 0.7005255222320557 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 3 : [3,1,4,5,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.6712062256809338 stone_count 6 playouts 514 ***\n",
      "time_limit 0.7 my duration 0.7013893127441406 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 4 : [3,1,4,5,3,3,3,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0636042402826855 stone_count 8 playouts 566 ***\n",
      "time_limit 0.7 my duration 0.7015180587768555 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 5 : [3,1,4,5,3,3,3,4,3,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.647887323943662 stone_count 10 playouts 710 ***\n",
      "time_limit 0.7 my duration 0.7012984752655029 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 6 : [3,1,4,5,3,3,3,4,3,4,4,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.8985507246376812 stone_count 12 playouts 690 ***\n",
      "time_limit 0.7 my duration 0.7020301818847656 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 7 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.8904282115869018 stone_count 14 playouts 794 ***\n",
      "time_limit 0.7 my duration 0.7013306617736816 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 8 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9059561128526645 stone_count 16 playouts 957 ***\n",
      "time_limit 0.7 my duration 0.701209306716919 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 9 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.952662721893491 stone_count 18 playouts 1183 ***\n",
      "time_limit 0.7 my duration 0.7004482746124268 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 10 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3] \n",
      "[0. 0. 0. 0.] root_rewards 1.9450171821305842 stone_count 20 playouts 1746 ***\n",
      "time_limit 0.7 my duration 0.7007627487182617 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 11 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6] \n",
      "[0. 0. 0. 0.] root_rewards 0.03056768558951965 stone_count 22 playouts 1832 ***\n",
      "time_limit 0.7 my duration 0.7007303237915039 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  6 < agent_MCTS_MatanTsipory_ . (1.1) --- 12 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6] \n",
      "[0. 0. 0. 0.] root_rewards 1.9211087420042643 stone_count 24 playouts 1876 ***\n",
      "time_limit 0.7 my duration 0.7007181644439697 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 13 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0] \n",
      "[0. 0. 0. 0.] root_rewards 1.9584352078239609 stone_count 26 playouts 2454 ***\n",
      "time_limit 0.7 my duration 0.7006902694702148 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 14 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0,6,0] \n",
      "[0. 0. 0.] root_rewards 1.9623931623931623 stone_count 28 playouts 3510 ***\n",
      "time_limit 0.7 my duration 0.7005836963653564 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 15 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0,6,0,0,5] \n",
      "[0. 0. 0.] root_rewards 1.9510537049626104 stone_count 30 playouts 4413 ***\n",
      "time_limit 0.7 my duration 0.7004835605621338 x 5\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 16 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0,6,0,0,5,5,0] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 32 playouts 12348 ***\n",
      "time_limit 0.7 my duration 0.7006218433380127 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 17 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0,6,0,0,5,5,0,1,0] \n",
      "[0. 0.] root_rewards 2.0 stone_count 34 playouts 30000 ***\n",
      "time_limit 0.7 my duration 0.6127386093139648 x 0\n",
      "(0.6) . MCTS_wAdaptive_Playouts_ > 0  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 18 : [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0,6,0,0,5,5,0,1,0,0,5] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00020074844360351562 x 5\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MCTS_wAdaptive_Playouts_      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MCTS_wAdaptive_Playouts_ [3]   2.agent_MCTS_MatanTsipory_ [0]   1.Time = 12.18   2.Time = 18.17   q.moves: 19   1.speed = 0.6   2.speed = 1.0   \n",
      "\n",
      "Game: [3,1,4,5,3,3,3,4,3,4,4,1,1,1,4,2,1,2,4,3,6,6,6,6,6,0,6,0,0,5,5,0,1,0,0,5,5]   \n",
      "\n",
      "Board: [1,1,0,2,1,0,1,2,1,0,1,1,1,1,2,2,0,1,1,2,2,1,1,0,2,2,1,1,2,2,2,1,2,2,2,2,2,2,1,1,2,1] \n",
      "\n",
      "[1 1 0 2 1 0 1]\n",
      "[2 1 0 1 1 1 1]\n",
      "[2 2 0 1 1 2 2]\n",
      "[1 1 0 2 2 1 1]\n",
      "[2 2 2 1 2 2 2]\n",
      "[2 2 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "15 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0]], 'time': [9.63], 'moves': [136], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0]], 'time': [148.42], 'moves': [145], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [12], 'games': [[3, 3], [3, 3]], 'time': [45.1], 'moves': [75], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [5], 'games': [[], [3, 2]], 'time': [37.03], 'moves': [37], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0, 0]], 'time': [25.48], 'moves': [25], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3]], 'time': [12.81], 'moves': [36], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [3], 'games': [[], [0, 3]], 'time': [11.45], 'moves': [36], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MCTS_wAdaptive_Playouts_   vs   MontyCarlo_JamesMcGuigan\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0574162679425838 stone_count 0 playouts 209 ***\n",
      "time_limit 0.35 my duration 0.3522357940673828 x 3\n",
      "MontyCarloNode: p2 action = 3 after 1768 simulations in 1.000s\n",
      "(0.4) . MCTS_wAdaptive_Playouts_ > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 1 : [3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.1886051080550097 stone_count 2 playouts 509 ***\n",
      "time_limit 0.7 my duration 0.7013523578643799 x 4\n",
      "MontyCarloNode: p2 action = 5 after 2017 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 2 : [3,3,4,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.6392785571142284 stone_count 4 playouts 499 ***\n",
      "time_limit 0.7 my duration 0.7009387016296387 x 1\n",
      "MontyCarloNode: p2 action = 2 after 1703 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 3 : [3,3,4,5,1,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7375478927203065 stone_count 6 playouts 522 ***\n",
      "time_limit 0.7 my duration 0.7015132904052734 x 1\n",
      "MontyCarloNode: p2 action = 1 after 1797 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 4 : [3,3,4,5,1,2,1,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.8213627992633518 stone_count 8 playouts 543 ***\n",
      "time_limit 0.7 my duration 0.701221227645874 x 5\n",
      "MontyCarloNode: p2 action = 5 after 1613 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 5 : [3,3,4,5,1,2,1,1,5,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.6124567474048442 stone_count 10 playouts 578 ***\n",
      "time_limit 0.7 my duration 0.7015600204467773 x 5\n",
      "MontyCarloNode: p2 action = 3 after 1579 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 6 : [3,3,4,5,1,2,1,1,5,5,5,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7565084226646248 stone_count 12 playouts 653 ***\n",
      "time_limit 0.7 my duration 0.7015492916107178 x 3\n",
      "MontyCarloNode: p2 action = 6 after 1847 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 7 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7997275204359673 stone_count 14 playouts 734 ***\n",
      "time_limit 0.7 my duration 0.7008564472198486 x 5\n",
      "MontyCarloNode: p2 action = 0 after 1734 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 8 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.4612352168199737 stone_count 16 playouts 761 ***\n",
      "time_limit 0.7 my duration 0.7007312774658203 x 3\n",
      "MontyCarloNode: p2 action = 1 after 1738 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 9 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9935553168635876 stone_count 18 playouts 931 ***\n",
      "time_limit 0.7 my duration 0.7007849216461182 x 1\n",
      "MontyCarloNode: p2 action = 1 after 1662 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 10 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.971843778383288 stone_count 20 playouts 1101 ***\n",
      "time_limit 0.7 my duration 0.7009007930755615 x 2\n",
      "MontyCarloNode: p2 action = 2 after 2111 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 11 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2] \n",
      "[0. 0. 0. 0.] root_rewards 0.0006903693476009665 stone_count 22 playouts 2897 ***\n",
      "time_limit 0.7 my duration 0.700587272644043 x 2\n",
      "MontyCarloNode: p2 action = 2 after 2013 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 12 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2] \n",
      "[0. 0. 0. 0.] root_rewards 0.042328042328042326 stone_count 24 playouts 2268 ***\n",
      "time_limit 0.7 my duration 0.7004399299621582 x 3\n",
      "MontyCarloNode: p2 action = 5 after 2284 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 13 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2,3,5] \n",
      "[0. 0.] root_rewards 0.029276164130935915 stone_count 26 playouts 4338 ***\n",
      "time_limit 0.7 my duration 0.700509786605835 x 2\n",
      "MontyCarloNode: p2 action = 6 after 3227 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 14 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2,3,5,2,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001804828643798828 x 6\n",
      "MontyCarloNode: p2 action = 6 after 3457 simulations in 1.000s\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 6  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 15 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2,3,5,2,6,6,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001633167266845703 x 6\n",
      "MontyCarloNode: p2 action = 6 after 3374 simulations in 1.000s\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 6  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 16 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2,3,5,2,6,6,6,6,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00016927719116210938 x 4\n",
      "MontyCarloNode: p2 action = 4 after 3530 simulations in 1.000s\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 4  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 17 : [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2,3,5,2,6,6,6,6,6,4,4] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MontyCarlo_JamesMcGuigan      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MCTS_wAdaptive_Playouts_ [0]   2.MontyCarlo_JamesMcGuigan [3]   1.Time = 9.47   2.Time = 17.01   q.moves: 17   1.speed = 0.6   2.speed = 1.0   \n",
      "\n",
      "Game: [3,3,4,5,1,2,1,1,5,5,5,3,3,6,5,0,3,1,1,1,2,2,2,2,3,5,2,6,6,6,6,6,4,4]   \n",
      "\n",
      "Board: [0,2,1,1,0,2,2,0,1,2,1,0,1,1,0,2,1,1,0,1,2,0,2,2,2,2,2,1,0,1,1,2,1,1,2,2,1,2,1,1,2,2] \n",
      "\n",
      "[0 2 1 1 0 2 2]\n",
      "[0 1 2 1 0 1 1]\n",
      "[0 2 1 1 0 1 2]\n",
      "[0 2 2 2 2 2 1]\n",
      "[0 1 1 2 1 1 2]\n",
      "[2 1 2 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "16 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0]], 'time': [9.63], 'moves': [136], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0]], 'time': [148.42], 'moves': [145], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [12], 'games': [[3, 3, 0], [3, 3]], 'time': [54.57], 'moves': [92], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [8], 'games': [[], [3, 2, 3]], 'time': [54.04], 'moves': [54], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [0], 'games': [[], [0, 0]], 'time': [25.48], 'moves': [25], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3]], 'time': [12.81], 'moves': [36], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [3], 'games': [[], [0, 3]], 'time': [11.45], 'moves': [36], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MCTS_wAdaptive_Playouts_   vs   MontyCarlo_BtS_JMcGuigan\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.059782608695652 stone_count 0 playouts 184 ***\n",
      "time_limit 0.35 my duration 0.3508121967315674 x 3\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2718 simulations in 1.000s\n",
      "(0.4) . MCTS_wAdaptive_Playouts_ > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 1 : [3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0633946830265848 stone_count 2 playouts 489 ***\n",
      "time_limit 0.7 my duration 0.7017393112182617 x 4\n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 2700 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 2 : [3,3,4,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7790697674418605 stone_count 4 playouts 516 ***\n",
      "time_limit 0.7 my duration 0.7017064094543457 x 3\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2103 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 3 : [3,3,4,2,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9664570230607966 stone_count 6 playouts 477 ***\n",
      "time_limit 0.7 my duration 0.7006394863128662 x 6\n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 1988 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 4 : [3,3,4,2,3,3,6,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.5272407732864675 stone_count 8 playouts 569 ***\n",
      "time_limit 0.7 my duration 0.7011544704437256 x 5\n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 292 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 5 : [3,3,4,2,3,3,6,5,5,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.2796610169491525 stone_count 10 playouts 590 ***\n",
      "time_limit 0.7 my duration 0.7013208866119385 x 5\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1986 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 6 : [3,3,4,2,3,3,6,5,5,5,5,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.13818722139673106 stone_count 12 playouts 673 ***\n",
      "time_limit 0.7 my duration 0.7009594440460205 x 2\n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1749 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 7 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.08163265306122448 stone_count 14 playouts 735 ***\n",
      "time_limit 0.7 my duration 0.7011947631835938 x 2\n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1628 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 8 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.04638009049773756 stone_count 16 playouts 884 ***\n",
      "time_limit 0.7 my duration 0.7007355690002441 x 5\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1597 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 9 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.053783044667274384 stone_count 18 playouts 1097 ***\n",
      "time_limit 0.7 my duration 0.7008631229400635 x 2\n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 1654 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 10 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4] \n",
      "[0. 0. 0. 0. 0.] root_rewards 0.049924357034795766 stone_count 20 playouts 1322 ***\n",
      "time_limit 0.7 my duration 0.7005922794342041 x 5\n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 1630 simulations in 1.001s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 5  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 11 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6] \n",
      "[0. 0. 0.] root_rewards 0.0 stone_count 22 playouts 1963 ***\n",
      "time_limit 0.7 my duration 0.7005743980407715 x 1\n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 1658 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 12 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6,1,1] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001430511474609375 x 0\n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 1687 simulations in 1.000s\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 0  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 13 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6,1,1,0,1] \n",
      "[0. 0. 0.] root_rewards 0.0 stone_count 26 playouts 3299 ***\n",
      "time_limit 0.7 my duration 0.7005279064178467 x 1\n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 1969 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 14 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6,1,1,0,1,1,1] \n",
      "[0. 0. 0.] root_rewards 0.0 stone_count 28 playouts 3690 ***\n",
      "time_limit 0.7 my duration 0.7005057334899902 x 0\n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 3398 simulations in 1.000s\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 15 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6,1,1,0,1,1,1,0,1] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001571178436279297 x 4\n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 3636 simulations in 1.000s\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 4  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 16 : [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6,1,1,0,1,1,1,0,1,4,4] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MontyCarlo_BtS_JMcGuigan      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MCTS_wAdaptive_Playouts_ [0]   2.MontyCarlo_BtS_JMcGuigan [3]   1.Time = 9.46   2.Time = 16.01   q.moves: 16   1.speed = 0.6   2.speed = 1.0   \n",
      "\n",
      "Game: [3,3,4,2,3,3,6,5,5,5,5,3,2,2,2,2,5,3,2,4,5,6,1,1,0,1,1,1,0,1,4,4]   \n",
      "\n",
      "Board: [0,2,1,2,0,1,0,0,2,2,2,0,1,0,0,1,1,2,2,1,0,0,2,2,1,1,2,0,1,2,1,2,2,1,2,1,1,2,1,1,2,1] \n",
      "\n",
      "[0 2 1 2 0 1 0]\n",
      "[0 2 2 2 0 1 0]\n",
      "[0 1 1 2 2 1 0]\n",
      "[0 2 2 1 1 2 0]\n",
      "[1 2 1 2 2 1 2]\n",
      "[1 1 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "17 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0]], 'time': [9.63], 'moves': [136], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0]], 'time': [148.42], 'moves': [145], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [12], 'games': [[3, 3, 0, 0], [3, 3]], 'time': [64.03], 'moves': [108], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [8], 'games': [[], [3, 2, 3]], 'time': [54.04], 'moves': [54], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3]], 'time': [41.49], 'moves': [41], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3]], 'time': [12.81], 'moves': [36], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [3], 'games': [[], [0, 3]], 'time': [11.45], 'moves': [36], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MCTS_wAdaptive_Playouts_   vs   agent_TheFast_mini_Max_2\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.1767441860465115 stone_count 0 playouts 215 ***\n",
      "time_limit 0.35 my duration 0.35166263580322266 x 3\n",
      "(0.4) . MCTS_wAdaptive_Playouts_ > 3  3 < agent_TheFast_mini_Max_2 . (0.0) --- 1 : [3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.2980769230769231 stone_count 2 playouts 520 ***\n",
      "time_limit 0.7 my duration 0.7014427185058594 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  5 < agent_TheFast_mini_Max_2 . (0.5) --- 2 : [3,3,4,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0020576131687242 stone_count 4 playouts 486 ***\n",
      "time_limit 0.7 my duration 0.7008919715881348 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  3 < agent_TheFast_mini_Max_2 . (0.5) --- 3 : [3,3,4,5,4,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7041984732824428 stone_count 6 playouts 524 ***\n",
      "time_limit 0.7 my duration 0.7004985809326172 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  2 < agent_TheFast_mini_Max_2 . (0.5) --- 4 : [3,3,4,5,4,3,1,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.8934707903780069 stone_count 8 playouts 582 ***\n",
      "time_limit 0.7 my duration 0.7007133960723877 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  4 < agent_TheFast_mini_Max_2 . (0.5) --- 5 : [3,3,4,5,4,3,1,2,4,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.5389830508474577 stone_count 10 playouts 590 ***\n",
      "time_limit 0.7 my duration 0.7009539604187012 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  5 < agent_TheFast_mini_Max_2 . (0.5) --- 6 : [3,3,4,5,4,3,1,2,4,4,3,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.6311584553928096 stone_count 12 playouts 751 ***\n",
      "time_limit 0.7 my duration 0.7008557319641113 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  5 < agent_TheFast_mini_Max_2 . (0.5) --- 7 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00018167495727539062 x 5\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 5  5 < agent_TheFast_mini_Max_2 . (0.5) --- 8 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00018787384033203125 x 2\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 9 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9437086092715232 stone_count 18 playouts 906 ***\n",
      "time_limit 0.7 my duration 0.7013916969299316 x 2\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 10 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.913604766633565 stone_count 20 playouts 1007 ***\n",
      "time_limit 0.7 my duration 0.700653076171875 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  3 < agent_TheFast_mini_Max_2 . (0.5) --- 11 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.92 stone_count 22 playouts 1750 ***\n",
      "time_limit 0.7 my duration 0.7007977962493896 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  2 < agent_TheFast_mini_Max_2 . (0.5) --- 12 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2] \n",
      "[0. 0. 0. 0.] root_rewards 2.0 stone_count 24 playouts 2618 ***\n",
      "time_limit 0.7 my duration 0.7006633281707764 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  0 < agent_TheFast_mini_Max_2 . (0.5) --- 13 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0] \n",
      "[0. 0. 0. 0.] root_rewards 2.0 stone_count 26 playouts 2574 ***\n",
      "time_limit 0.7 my duration 0.700763463973999 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  0 < agent_TheFast_mini_Max_2 . (0.1) --- 14 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0,0,0] \n",
      "[0. 0. 0. 0.] root_rewards 1.9614349775784754 stone_count 28 playouts 2230 ***\n",
      "time_limit 0.7 my duration 0.7006077766418457 x 0\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 0  0 < agent_TheFast_mini_Max_2 . (0.1) --- 15 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0,0,0,0,0] \n",
      "[0. 0. 0.] root_rewards 1.9643633202955237 stone_count 30 playouts 4602 ***\n",
      "time_limit 0.7 my duration 0.7004859447479248 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  5 < agent_TheFast_mini_Max_2 . (0.0) --- 16 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0,0,0,0,0,6,5] \n",
      "[0. 0.] root_rewards 1.966145275337476 stone_count 32 playouts 9334 ***\n",
      "time_limit 0.7 my duration 0.7004444599151611 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  6 < agent_TheFast_mini_Max_2 . (0.0) --- 17 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0,0,0,0,0,6,5,6,6] \n",
      "[0. 0.] root_rewards 2.0 stone_count 34 playouts 21389 ***\n",
      "time_limit 0.7 my duration 0.7004742622375488 x 6\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 6  1 < agent_TheFast_mini_Max_2 . (0.0) --- 18 : [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0,0,0,0,0,6,5,6,6,6,1] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001475811004638672 x 1\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MCTS_wAdaptive_Playouts_      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MCTS_wAdaptive_Playouts_ [3]   2.agent_TheFast_mini_Max_2 [0]   1.Time = 10.86   2.Time = 6.28   q.moves: 19   1.speed = 0.6   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,4,5,4,3,1,2,4,4,3,5,3,5,5,5,2,2,2,2,4,3,4,2,0,0,0,0,0,0,6,5,6,6,6,1,1]   \n",
      "\n",
      "Board: [2,0,2,2,1,2,0,1,0,2,1,1,2,0,2,0,1,1,2,1,1,1,1,2,2,1,2,2,2,2,1,2,1,2,1,1,1,2,1,1,2,1] \n",
      "\n",
      "[2 0 2 2 1 2 0]\n",
      "[1 0 2 1 1 2 0]\n",
      "[2 0 1 1 2 1 1]\n",
      "[1 1 2 2 1 2 2]\n",
      "[2 2 1 2 1 2 1]\n",
      "[1 1 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "18 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0]], 'time': [9.63], 'moves': [136], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0]], 'time': [148.42], 'moves': [145], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [15], 'games': [[3, 3, 0, 0, 3], [3, 3]], 'time': [74.89], 'moves': [127], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [8], 'games': [[], [3, 2, 3]], 'time': [54.04], 'moves': [54], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3]], 'time': [41.49], 'moves': [41], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0]], 'time': [19.09], 'moves': [54], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [3], 'games': [[], [0, 3]], 'time': [11.45], 'moves': [36], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MCTS_wAdaptive_Playouts_   vs   agent_TheFast_mini_Max_4\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.102439024390244 stone_count 0 playouts 205 ***\n",
      "time_limit 0.35 my duration 0.35177016258239746 x 3\n",
      "(0.4) . MCTS_wAdaptive_Playouts_ > 3  3 < agent_TheFast_mini_Max_4 . (0.0) --- 1 : [3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.1588594704684319 stone_count 2 playouts 491 ***\n",
      "time_limit 0.7 my duration 0.7012190818786621 x 2\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  1 < agent_TheFast_mini_Max_4 . (0.5) --- 2 : [3,3,2,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0165289256198347 stone_count 4 playouts 484 ***\n",
      "time_limit 0.7 my duration 0.7019097805023193 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  5 < agent_TheFast_mini_Max_4 . (0.5) --- 3 : [3,3,2,1,3,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.4011299435028248 stone_count 6 playouts 531 ***\n",
      "time_limit 0.7 my duration 0.7004759311676025 x 2\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  3 < agent_TheFast_mini_Max_4 . (0.5) --- 4 : [3,3,2,1,3,5,2,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.720226843100189 stone_count 8 playouts 529 ***\n",
      "time_limit 0.7 my duration 0.7013061046600342 x 2\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 2  2 < agent_TheFast_mini_Max_4 . (0.5) --- 5 : [3,3,2,1,3,5,2,3,2,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.8865814696485623 stone_count 10 playouts 626 ***\n",
      "time_limit 0.7 my duration 0.7012431621551514 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  1 < agent_TheFast_mini_Max_4 . (0.5) --- 6 : [3,3,2,1,3,5,2,3,2,2,1,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.4058988764044944 stone_count 12 playouts 712 ***\n",
      "time_limit 0.7 my duration 0.701725959777832 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  3 < agent_TheFast_mini_Max_4 . (0.5) --- 7 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.7450523864959255 stone_count 14 playouts 859 ***\n",
      "time_limit 0.7 my duration 0.7009291648864746 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  2 < agent_TheFast_mini_Max_4 . (0.5) --- 8 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.5161870503597121 stone_count 16 playouts 1112 ***\n",
      "time_limit 0.7 my duration 0.7009050846099854 x 1\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 1  2 < agent_TheFast_mini_Max_4 . (0.5) --- 9 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2,1,2] \n",
      "[0. 0. 0. 0.] root_rewards 0.02830188679245283 stone_count 18 playouts 1696 ***\n",
      "time_limit 0.7 my duration 0.7004294395446777 x 3\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 3  4 < agent_TheFast_mini_Max_4 . (0.5) --- 10 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2,1,2,3,4] \n",
      "[0. 0. 0.] root_rewards 0.08663883089770355 stone_count 20 playouts 1916 ***\n",
      "time_limit 0.7 my duration 0.7014148235321045 x 4\n",
      "(0.7) . MCTS_wAdaptive_Playouts_ > 4  4 < agent_TheFast_mini_Max_4 . (0.0) --- 11 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2,1,2,3,4,4,4] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001919269561767578 x 5\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 5  4 < agent_TheFast_mini_Max_4 . (0.0) --- 12 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2,1,2,3,4,4,4,5,4] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00018978118896484375 x 5\n",
      "(0.0) . MCTS_wAdaptive_Playouts_ > 5  5 < agent_TheFast_mini_Max_4 . (0.0) --- 13 : [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2,1,2,3,4,4,4,5,4,5,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_TheFast_mini_Max_4      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MCTS_wAdaptive_Playouts_ [0]   2.agent_TheFast_mini_Max_4 [3]   1.Time = 7.36   2.Time = 4.52   q.moves: 13   1.speed = 0.6   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,2,1,3,5,2,3,2,2,1,1,1,3,1,2,1,2,3,4,4,4,5,4,5,5]   \n",
      "\n",
      "Board: [0,1,2,1,0,0,0,0,1,2,2,0,0,0,0,1,2,2,2,2,0,0,2,1,1,2,1,0,0,1,1,2,1,1,0,0,2,1,1,2,2,0] \n",
      "\n",
      "[0 1 2 1 0 0 0]\n",
      "[0 1 2 2 0 0 0]\n",
      "[0 1 2 2 2 2 0]\n",
      "[0 2 1 1 2 1 0]\n",
      "[0 1 1 2 1 1 0]\n",
      "[0 2 1 1 2 2 0]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "19 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0]], 'time': [9.63], 'moves': [136], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0]], 'time': [148.42], 'moves': [145], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [15], 'games': [[3, 3, 0, 0, 3, 0], [3, 3]], 'time': [82.25], 'moves': [140], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [8], 'games': [[], [3, 2, 3]], 'time': [54.04], 'moves': [54], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3]], 'time': [41.49], 'moves': [41], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0]], 'time': [19.09], 'moves': [54], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [6], 'games': [[], [0, 3, 3]], 'time': [15.97], 'moves': [49], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_JamesMcGuigan   vs   agent__treebased__dott__\n",
      "-----------------------------------\n",
      "MontyCarloNode: p1 action = 3 after 1666 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent__treebased__dott__ . (0.1) --- 1 : [3,3] \n",
      "MontyCarloNode: p1 action = 3 after 1766 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent__treebased__dott__ . (0.1) --- 2 : [3,3,3,3] \n",
      "MontyCarloNode: p1 action = 4 after 1789 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  5 < agent__treebased__dott__ . (0.1) --- 3 : [3,3,3,3,4,5] \n",
      "MontyCarloNode: p1 action = 1 after 1832 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  2 < agent__treebased__dott__ . (0.1) --- 4 : [3,3,3,3,4,5,1,2] \n",
      "MontyCarloNode: p1 action = 1 after 1048 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  3 < agent__treebased__dott__ . (0.1) --- 5 : [3,3,3,3,4,5,1,2,1,3] \n",
      "MontyCarloNode: p1 action = 1 after 1756 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < agent__treebased__dott__ . (0.1) --- 6 : [3,3,3,3,4,5,1,2,1,3,1,1] \n",
      "MontyCarloNode: p1 action = 5 after 1784 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  3 < agent__treebased__dott__ . (0.1) --- 7 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3] \n",
      "MontyCarloNode: p1 action = 0 after 1727 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  5 < agent__treebased__dott__ . (0.1) --- 8 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5] \n",
      "MontyCarloNode: p1 action = 5 after 1662 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  5 < agent__treebased__dott__ . (0.1) --- 9 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5,5,5] \n",
      "MontyCarloNode: p1 action = 4 after 1820 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  1 < agent__treebased__dott__ . (0.1) --- 10 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5,5,5,4,1] \n",
      "MontyCarloNode: p1 action = 0 after 2040 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < agent__treebased__dott__ . (0.1) --- 11 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5,5,5,4,1,0,0] \n",
      "MontyCarloNode: p1 action = 4 after 3097 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < agent__treebased__dott__ . (0.1) --- 12 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5,5,5,4,1,0,0,4,4] \n",
      "MontyCarloNode: p1 action = 0 after 3693 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  4 < agent__treebased__dott__ . (0.1) --- 13 : [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5,5,5,4,1,0,0,4,4,0,4] \n",
      "MontyCarloNode: p1 action = 2 after 4501 simulations in 1.000s\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MontyCarlo_JamesMcGuigan      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_JamesMcGuigan [3]   2.agent__treebased__dott__ [0]   1.Time = 14.01   2.Time = 1.0   q.moves: 14   1.speed = 1.0   2.speed = 0.1   \n",
      "\n",
      "Game: [3,3,3,3,4,5,1,2,1,3,1,1,5,3,0,5,5,5,4,1,0,0,4,4,0,4,2]   \n",
      "\n",
      "Board: [0,0,0,2,0,0,0,0,2,0,2,2,2,0,1,2,0,2,2,1,0,2,1,0,1,1,2,0,1,1,1,2,1,1,0,1,1,2,1,1,2,0] \n",
      "\n",
      "[0 0 0 2 0 0 0]\n",
      "[0 2 0 2 2 2 0]\n",
      "[1 2 0 2 2 1 0]\n",
      "[2 1 0 1 1 2 0]\n",
      "[1 1 1 2 1 1 0]\n",
      "[1 1 2 1 1 2 0]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "20 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0]], 'time': [10.63], 'moves': [149], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0]], 'time': [148.42], 'moves': [145], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [15], 'games': [[3, 3, 0, 0, 3, 0], [3, 3]], 'time': [82.25], 'moves': [140], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [11], 'games': [[3], [3, 2, 3]], 'time': [68.05], 'moves': [68], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3]], 'time': [41.49], 'moves': [41], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0]], 'time': [19.09], 'moves': [54], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [6], 'games': [[], [0, 3, 3]], 'time': [15.97], 'moves': [49], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_JamesMcGuigan   vs   agent_MCTS_MatanTsipory_\n",
      "-----------------------------------\n",
      "MontyCarloNode: p1 action = 3 after 1638 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 1 : [3,3] \n",
      "MontyCarloNode: p1 action = 4 after 1831 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 2 : [3,3,4,5] \n",
      "MontyCarloNode: p1 action = 5 after 1847 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 3 : [3,3,4,5,5,1] \n",
      "MontyCarloNode: p1 action = 3 after 1593 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 4 : [3,3,4,5,5,1,3,1] \n",
      "MontyCarloNode: p1 action = 1 after 1743 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 5 : [3,3,4,5,5,1,3,1,1,3] \n",
      "MontyCarloNode: p1 action = 5 after 1785 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 6 : [3,3,4,5,5,1,3,1,1,3,5,5] \n",
      "MontyCarloNode: p1 action = 1 after 1969 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 7 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1] \n",
      "MontyCarloNode: p1 action = 0 after 1886 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 8 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4] \n",
      "MontyCarloNode: p1 action = 4 after 1788 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 9 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4] \n",
      "MontyCarloNode: p1 action = 4 after 1949 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 10 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0] \n",
      "MontyCarloNode: p1 action = 5 after 1987 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 11 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0] \n",
      "MontyCarloNode: p1 action = 0 after 2053 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 12 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0,0,4] \n",
      "MontyCarloNode: p1 action = 6 after 2294 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 13 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0,0,4,6,0] \n",
      "MontyCarloNode: p1 action = 3 after 2396 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 14 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0,0,4,6,0,3,3] \n",
      "MontyCarloNode: p1 action = 5 after 3053 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 15 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0,0,4,6,0,3,3,5,0] \n",
      "MontyCarloNode: p1 action = 1 after 3328 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 16 : [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0,0,4,6,0,3,3,5,0,1,6] \n",
      "MontyCarloNode: p1 action = 6 after 3615 simulations in 1.000s\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MontyCarlo_JamesMcGuigan      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_JamesMcGuigan [3]   2.agent_MCTS_MatanTsipory_ [0]   1.Time = 17.01   2.Time = 16.01   q.moves: 17   1.speed = 1.0   2.speed = 0.9   \n",
      "\n",
      "Game: [3,3,4,5,5,1,3,1,1,3,5,5,1,1,0,4,4,4,4,0,5,0,0,4,6,0,3,3,5,0,1,6,6]   \n",
      "\n",
      "Board: [2,1,0,2,2,1,0,2,2,0,1,1,1,0,1,1,0,2,2,2,0,2,1,0,1,1,1,1,2,2,0,2,2,1,2,1,2,0,1,1,2,1] \n",
      "\n",
      "[2 1 0 2 2 1 0]\n",
      "[2 2 0 1 1 1 0]\n",
      "[1 1 0 2 2 2 0]\n",
      "[2 1 0 1 1 1 1]\n",
      "[2 2 0 2 2 1 2]\n",
      "[1 2 0 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "21 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0]], 'time': [10.63], 'moves': [149], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0]], 'time': [164.43], 'moves': [161], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [15], 'games': [[3, 3, 0, 0, 3, 0], [3, 3]], 'time': [82.25], 'moves': [140], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [14], 'games': [[3, 3], [3, 2, 3]], 'time': [85.06], 'moves': [85], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3]], 'time': [41.49], 'moves': [41], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0]], 'time': [19.09], 'moves': [54], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [6], 'games': [[], [0, 3, 3]], 'time': [15.97], 'moves': [49], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_JamesMcGuigan   vs   MCTS_wAdaptive_Playouts_\n",
      "-----------------------------------\n",
      "MontyCarloNode: p1 action = 3 after 1526 simulations in 1.000s\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.6044444444444445 stone_count 1 playouts 225 ***\n",
      "time_limit 0.35 my duration 0.35175275802612305 x 4\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  4 < MCTS_wAdaptive_Playouts_ . (0.4) --- 1 : [3,4] \n",
      "MontyCarloNode: p1 action = 3 after 1564 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7069767441860465 stone_count 3 playouts 430 ***\n",
      "time_limit 0.7 my duration 0.7018184661865234 x 3\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 2 : [3,4,3,3] \n",
      "MontyCarloNode: p1 action = 3 after 1625 simulations in 1.001s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.6680161943319838 stone_count 5 playouts 494 ***\n",
      "time_limit 0.7 my duration 0.7019064426422119 x 4\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 3 : [3,4,3,3,3,4] \n",
      "MontyCarloNode: p1 action = 4 after 1612 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.8388888888888889 stone_count 7 playouts 540 ***\n",
      "time_limit 0.7 my duration 0.7017464637756348 x 3\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 4 : [3,4,3,3,3,4,4,3] \n",
      "MontyCarloNode: p1 action = 1 after 1654 simulations in 1.001s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0135593220338983 stone_count 9 playouts 590 ***\n",
      "time_limit 0.7 my duration 0.7009732723236084 x 1\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 5 : [3,4,3,3,3,4,4,3,1,1] \n",
      "MontyCarloNode: p1 action = 6 after 1823 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.2748447204968945 stone_count 11 playouts 644 ***\n",
      "time_limit 0.7 my duration 0.7014951705932617 x 0\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 6 : [3,4,3,3,3,4,4,3,1,1,6,0] \n",
      "MontyCarloNode: p1 action = 4 after 1710 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.975609756097561 stone_count 13 playouts 697 ***\n",
      "time_limit 0.7 my duration 0.7008023262023926 x 4\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 7 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4] \n",
      "MontyCarloNode: p1 action = 6 after 541 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.6816431322207959 stone_count 15 playouts 779 ***\n",
      "time_limit 0.7 my duration 0.7007730007171631 x 3\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 8 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3] \n",
      "MontyCarloNode: p1 action = 1 after 1841 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0.] root_rewards 1.8376156217882837 stone_count 17 playouts 973 ***\n",
      "time_limit 0.7 my duration 0.7007968425750732 x 1\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 9 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1] \n",
      "MontyCarloNode: p1 action = 6 after 1848 simulations in 1.000s\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001399517059326172 x 6\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  6 < MCTS_wAdaptive_Playouts_ . (0.0) --- 10 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6] \n",
      "MontyCarloNode: p1 action = 0 after 1805 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0.] root_rewards 1.997971602434077 stone_count 21 playouts 1479 ***\n",
      "time_limit 0.7 my duration 0.7007153034210205 x 1\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 11 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1] \n",
      "MontyCarloNode: p1 action = 6 after 1974 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9989754098360655 stone_count 23 playouts 1952 ***\n",
      "time_limit 0.7 my duration 0.7004804611206055 x 0\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 12 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0] \n",
      "MontyCarloNode: p1 action = 0 after 2134 simulations in 1.000s\n",
      "[0. 0. 0. 0.] root_rewards 2.0 stone_count 25 playouts 2554 ***\n",
      "time_limit 0.7 my duration 0.7009072303771973 x 0\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 13 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0] \n",
      "MontyCarloNode: p1 action = 0 after 2241 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 27 playouts 3756 ***\n",
      "time_limit 0.7 my duration 0.7006161212921143 x 1\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 14 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0,0,1] \n",
      "MontyCarloNode: p1 action = 5 after 2115 simulations in 1.000s\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0003795623779296875 x 5\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 15 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0,0,1,5,5] \n",
      "MontyCarloNode: p1 action = 5 after 2371 simulations in 1.000s\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00017642974853515625 x 5\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 16 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0,0,1,5,5,5,5] \n",
      "MontyCarloNode: p1 action = 2 after 2356 simulations in 1.000s\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00015735626220703125 x 2\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  2 < MCTS_wAdaptive_Playouts_ . (0.0) --- 17 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0,0,1,5,5,5,5,2,2] \n",
      "MontyCarloNode: p1 action = 2 after 2405 simulations in 1.000s\n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00016260147094726562 x 2\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  2 < MCTS_wAdaptive_Playouts_ . (0.0) --- 18 : [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0,0,1,5,5,5,5,2,2,2,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MCTS_wAdaptive_Playouts_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_JamesMcGuigan [0]   2.MCTS_wAdaptive_Playouts_ [3]   1.Time = 18.01   2.Time = 8.77   q.moves: 18   1.speed = 1.0   2.speed = 0.5   \n",
      "\n",
      "Game: [3,4,3,3,3,4,4,3,1,1,6,0,4,4,6,3,1,1,6,6,0,1,6,0,0,0,0,1,5,5,5,5,2,2,2,2]   \n",
      "\n",
      "Board: [1,2,0,2,0,0,0,2,2,0,2,2,0,1,1,2,2,1,1,2,2,2,1,1,2,1,1,1,1,2,2,1,2,2,1,2,1,1,1,2,1,1] \n",
      "\n",
      "[1 2 0 2 0 0 0]\n",
      "[2 2 0 2 2 0 1]\n",
      "[1 2 2 1 1 2 2]\n",
      "[2 1 1 2 1 1 1]\n",
      "[1 2 2 1 2 2 1]\n",
      "[2 1 1 1 2 1 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "22 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0]], 'time': [10.63], 'moves': [149], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0]], 'time': [164.43], 'moves': [161], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [18], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3]], 'time': [91.02], 'moves': [158], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [14], 'games': [[3, 3, 0], [3, 2, 3]], 'time': [103.07], 'moves': [103], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3]], 'time': [41.49], 'moves': [41], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0]], 'time': [19.09], 'moves': [54], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [6], 'games': [[], [0, 3, 3]], 'time': [15.97], 'moves': [49], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_JamesMcGuigan   vs   MontyCarlo_BtS_JMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloNode: p1 action = 3 after 1574 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2680 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloNode: p1 action = 4 after 1817 simulations in 1.001s\n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 2661 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 2 : [3,3,4,2] \n",
      "MontyCarloNode: p1 action = 4 after 1882 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 2171 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 3 : [3,3,4,2,4,4] \n",
      "MontyCarloNode: p1 action = 3 after 1776 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1957 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 4 : [3,3,4,2,4,4,3,3] \n",
      "MontyCarloNode: p1 action = 2 after 1936 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 1965 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 5 : [3,3,4,2,4,4,3,3,2,4] \n",
      "MontyCarloNode: p1 action = 2 after 2058 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1968 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 6 : [3,3,4,2,4,4,3,3,2,4,2,2] \n",
      "MontyCarloNode: p1 action = 6 after 1918 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 1070 simulations in 1.469s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  5 < MontyCarlo_BtS_JMcGuigan . (1.5) --- 7 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5] \n",
      "MontyCarloNode: p1 action = 6 after 1919 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1909 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 8 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2] \n",
      "MontyCarloNode: p1 action = 5 after 2118 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1787 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 9 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3] \n",
      "MontyCarloNode: p1 action = 6 after 1996 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 1553 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 10 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6] \n",
      "MontyCarloNode: p1 action = 4 after 2062 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1418 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 11 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3] \n",
      "MontyCarloNode: p1 action = 2 after 2042 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 1542 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 12 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4] \n",
      "MontyCarloNode: p1 action = 0 after 2173 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 1653 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 13 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0] \n",
      "MontyCarloNode: p1 action = 0 after 2219 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 1604 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 14 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0] \n",
      "MontyCarloNode: p1 action = 1 after 2413 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 1884 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 15 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0,1,5] \n",
      "MontyCarloNode: p1 action = 5 after 2632 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 2376 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 16 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0,1,5,5,6] \n",
      "MontyCarloNode: p1 action = 6 after 2753 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 2860 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 17 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0,1,5,5,6,6,0] \n",
      "MontyCarloNode: p1 action = 0 after 2791 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 3031 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 18 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0,1,5,5,6,6,0,0,5] \n",
      "MontyCarloNode: p1 action = 5 after 2933 simulations in 1.000s\n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 3043 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 19 : [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0,1,5,5,6,6,0,0,5,5,1] \n",
      "MontyCarloNode: p1 action = 1 after 3095 simulations in 1.000s\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MontyCarlo_JamesMcGuigan      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_JamesMcGuigan [3]   2.MontyCarlo_BtS_JMcGuigan [0]   1.Time = 20.01   2.Time = 19.48   q.moves: 20   1.speed = 1.0   2.speed = 1.0   \n",
      "\n",
      "Game: [3,3,4,2,4,4,3,3,2,4,2,2,6,5,6,2,5,3,6,6,4,3,2,4,0,0,0,0,1,5,5,6,6,0,0,5,5,1,1]   \n",
      "\n",
      "Board: [1,0,1,2,2,1,1,2,0,2,2,1,2,2,2,0,2,2,2,1,2,1,1,1,1,2,2,1,2,2,1,2,1,1,1,1,1,2,1,1,2,1] \n",
      "\n",
      "[1 0 1 2 2 1 1]\n",
      "[2 0 2 2 1 2 2]\n",
      "[2 0 2 2 2 1 2]\n",
      "[1 1 1 1 2 2 1]\n",
      "[2 2 1 2 1 1 1]\n",
      "[1 1 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "23 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0]], 'time': [10.63], 'moves': [149], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0]], 'time': [164.43], 'moves': [161], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [18], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3]], 'time': [91.02], 'moves': [158], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [17], 'games': [[3, 3, 0, 3], [3, 2, 3]], 'time': [123.08], 'moves': [123], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3, 0]], 'time': [60.97], 'moves': [60], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0]], 'time': [19.09], 'moves': [54], 'speed': [0.4]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [6], 'games': [[], [0, 3, 3]], 'time': [15.97], 'moves': [49], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_JamesMcGuigan   vs   agent_TheFast_mini_Max_2\n",
      "-----------------------------------\n",
      "MontyCarloNode: p1 action = 3 after 1625 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent_TheFast_mini_Max_2 . (0.0) --- 1 : [3,3] \n",
      "MontyCarloNode: p1 action = 4 after 1729 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  5 < agent_TheFast_mini_Max_2 . (0.5) --- 2 : [3,3,4,5] \n",
      "MontyCarloNode: p1 action = 4 after 1856 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  3 < agent_TheFast_mini_Max_2 . (0.5) --- 3 : [3,3,4,5,4,3] \n",
      "MontyCarloNode: p1 action = 4 after 1898 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < agent_TheFast_mini_Max_2 . (0.5) --- 4 : [3,3,4,5,4,3,4,4] \n",
      "MontyCarloNode: p1 action = 1 after 1804 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  2 < agent_TheFast_mini_Max_2 . (0.5) --- 5 : [3,3,4,5,4,3,4,4,1,2] \n",
      "MontyCarloNode: p1 action = 3 after 1573 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  5 < agent_TheFast_mini_Max_2 . (0.5) --- 6 : [3,3,4,5,4,3,4,4,1,2,3,5] \n",
      "MontyCarloNode: p1 action = 5 after 1767 simulations in 1.560s\n",
      "(1.6) . MontyCarlo_JamesMcGuigan > 5  2 < agent_TheFast_mini_Max_2 . (0.5) --- 7 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2] \n",
      "MontyCarloNode: p1 action = 1 after 1910 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  2 < agent_TheFast_mini_Max_2 . (0.5) --- 8 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2] \n",
      "MontyCarloNode: p1 action = 2 after 1775 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 9 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2] \n",
      "MontyCarloNode: p1 action = 1 after 1886 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < agent_TheFast_mini_Max_2 . (0.5) --- 10 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1] \n",
      "MontyCarloNode: p1 action = 3 after 2113 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent_TheFast_mini_Max_2 . (0.5) --- 11 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3] \n",
      "MontyCarloNode: p1 action = 2 after 2255 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  1 < agent_TheFast_mini_Max_2 . (0.5) --- 12 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1] \n",
      "MontyCarloNode: p1 action = 1 after 2636 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  6 < agent_TheFast_mini_Max_2 . (0.1) --- 13 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1,1,6] \n",
      "MontyCarloNode: p1 action = 4 after 2989 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < agent_TheFast_mini_Max_2 . (0.0) --- 14 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1,1,6,4,4] \n",
      "MontyCarloNode: p1 action = 6 after 2968 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  5 < agent_TheFast_mini_Max_2 . (0.0) --- 15 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1,1,6,4,4,6,5] \n",
      "MontyCarloNode: p1 action = 5 after 3178 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  5 < agent_TheFast_mini_Max_2 . (0.0) --- 16 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1,1,6,4,4,6,5,5,5] \n",
      "MontyCarloNode: p1 action = 6 after 3157 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  0 < agent_TheFast_mini_Max_2 . (0.0) --- 17 : [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1,1,6,4,4,6,5,5,5,6,0] \n",
      "MontyCarloNode: p1 action = 0 after 3395 simulations in 1.000s\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MontyCarlo_JamesMcGuigan      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_JamesMcGuigan [3]   2.agent_TheFast_mini_Max_2 [0]   1.Time = 18.57   2.Time = 5.58   q.moves: 18   1.speed = 1.0   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,4,5,4,3,4,4,1,2,3,5,5,2,1,2,2,2,1,1,3,3,2,1,1,6,4,4,6,5,5,5,6,0,0]   \n",
      "\n",
      "Board: [0,1,1,2,2,2,0,0,2,2,1,1,1,0,0,2,1,1,2,2,0,0,1,2,2,1,1,1,1,1,2,2,1,2,1,2,1,2,1,1,2,2] \n",
      "\n",
      "[0 1 1 2 2 2 0]\n",
      "[0 2 2 1 1 1 0]\n",
      "[0 2 1 1 2 2 0]\n",
      "[0 1 2 2 1 1 1]\n",
      "[1 1 2 2 1 2 1]\n",
      "[2 1 2 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "24 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0]], 'time': [10.63], 'moves': [149], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0]], 'time': [164.43], 'moves': [161], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [18], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3]], 'time': [91.02], 'moves': [158], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3], [3, 2, 3]], 'time': [141.65], 'moves': [141], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3, 0]], 'time': [60.97], 'moves': [60], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0, 0]], 'time': [24.67], 'moves': [71], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [6], 'games': [[], [0, 3, 3]], 'time': [15.97], 'moves': [49], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_JamesMcGuigan   vs   agent_TheFast_mini_Max_4\n",
      "-----------------------------------\n",
      "MontyCarloNode: p1 action = 3 after 1482 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent_TheFast_mini_Max_4 . (0.0) --- 1 : [3,3] \n",
      "MontyCarloNode: p1 action = 2 after 1758 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  1 < agent_TheFast_mini_Max_4 . (0.5) --- 2 : [3,3,2,1] \n",
      "MontyCarloNode: p1 action = 3 after 1869 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  5 < agent_TheFast_mini_Max_4 . (0.5) --- 3 : [3,3,2,1,3,5] \n",
      "MontyCarloNode: p1 action = 3 after 1690 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  2 < agent_TheFast_mini_Max_4 . (0.5) --- 4 : [3,3,2,1,3,5,3,2] \n",
      "MontyCarloNode: p1 action = 3 after 1969 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 3  3 < agent_TheFast_mini_Max_4 . (0.5) --- 5 : [3,3,2,1,3,5,3,2,3,3] \n",
      "MontyCarloNode: p1 action = 1 after 1671 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  2 < agent_TheFast_mini_Max_4 . (0.5) --- 6 : [3,3,2,1,3,5,3,2,3,3,1,2] \n",
      "MontyCarloNode: p1 action = 5 after 1526 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  6 < agent_TheFast_mini_Max_4 . (0.5) --- 7 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6] \n",
      "MontyCarloNode: p1 action = 4 after 1641 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  6 < agent_TheFast_mini_Max_4 . (0.5) --- 8 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6] \n",
      "MontyCarloNode: p1 action = 2 after 1750 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  2 < agent_TheFast_mini_Max_4 . (0.5) --- 9 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2] \n",
      "MontyCarloNode: p1 action = 6 after 1747 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  5 < agent_TheFast_mini_Max_4 . (0.5) --- 10 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5] \n",
      "MontyCarloNode: p1 action = 5 after 1753 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 5  4 < agent_TheFast_mini_Max_4 . (0.5) --- 11 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4] \n",
      "MontyCarloNode: p1 action = 1 after 1897 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < agent_TheFast_mini_Max_4 . (0.5) --- 12 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1] \n",
      "MontyCarloNode: p1 action = 2 after 2116 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 2  5 < agent_TheFast_mini_Max_4 . (0.5) --- 13 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5] \n",
      "MontyCarloNode: p1 action = 6 after 2150 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  5 < agent_TheFast_mini_Max_4 . (0.5) --- 14 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5] \n",
      "MontyCarloNode: p1 action = 1 after 2247 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 1  1 < agent_TheFast_mini_Max_4 . (0.0) --- 15 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1] \n",
      "MontyCarloNode: p1 action = 4 after 2448 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < agent_TheFast_mini_Max_4 . (0.0) --- 16 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4] \n",
      "MontyCarloNode: p1 action = 0 after 2624 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < agent_TheFast_mini_Max_4 . (0.0) --- 17 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4,0,0] \n",
      "MontyCarloNode: p1 action = 0 after 2753 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < agent_TheFast_mini_Max_4 . (0.0) --- 18 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4,0,0,0,0] \n",
      "MontyCarloNode: p1 action = 0 after 2909 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 0  0 < agent_TheFast_mini_Max_4 . (0.0) --- 19 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4,0,0,0,0,0,0] \n",
      "MontyCarloNode: p1 action = 6 after 2685 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 6  6 < agent_TheFast_mini_Max_4 . (0.0) --- 20 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4,0,0,0,0,0,0,6,6] \n",
      "MontyCarloNode: p1 action = 4 after 2813 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_JamesMcGuigan > 4  4 < agent_TheFast_mini_Max_4 . (0.0) --- 21 : [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4,0,0,0,0,0,0,6,6,4,4] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_TheFast_mini_Max_4      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_JamesMcGuigan [0]   2.agent_TheFast_mini_Max_4 [3]   1.Time = 21.01   2.Time = 6.57   q.moves: 21   1.speed = 1.0   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,2,1,3,5,3,2,3,3,1,2,5,6,4,6,2,2,6,5,5,4,1,1,2,5,6,5,1,1,4,4,0,0,0,0,0,0,6,6,4,4]   \n",
      "\n",
      "Board: [2,2,1,2,2,2,2,1,1,2,1,1,2,1,2,2,1,1,2,1,1,1,1,2,1,1,2,1,2,1,2,2,2,1,2,1,2,1,1,1,2,2] \n",
      "\n",
      "[2 2 1 2 2 2 2]\n",
      "[1 1 2 1 1 2 1]\n",
      "[2 2 1 1 2 1 1]\n",
      "[1 1 2 1 1 2 1]\n",
      "[2 1 2 2 2 1 2]\n",
      "[1 2 1 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "25 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [12], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0]], 'time': [10.63], 'moves': [149], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0]], 'time': [164.43], 'moves': [161], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [18], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3]], 'time': [91.02], 'moves': [158], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3]], 'time': [162.66], 'moves': [162], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [3], 'games': [[], [0, 0, 3, 0]], 'time': [60.97], 'moves': [60], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0, 0]], 'time': [24.67], 'moves': [71], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3]], 'time': [22.54], 'moves': [70], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_BtS_JMcGuigan   vs   agent__treebased__dott__\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2764 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent__treebased__dott__ . (0.1) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2240 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent__treebased__dott__ . (0.1) --- 2 : [3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1942 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent__treebased__dott__ . (0.1) --- 3 : [3,3,3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2169 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent__treebased__dott__ . (0.1) --- 4 : [3,3,3,3,3,3,4,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 2224 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < agent__treebased__dott__ . (0.1) --- 5 : [3,3,3,3,3,3,4,5,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 827 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < agent__treebased__dott__ . (0.1) --- 6 : [3,3,3,3,3,3,4,5,5,5,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1956 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  2 < agent__treebased__dott__ . (0.1) --- 7 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1812 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  2 < agent__treebased__dott__ . (0.1) --- 8 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1617 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  1 < agent__treebased__dott__ . (0.1) --- 9 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1663 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  1 < agent__treebased__dott__ . (0.1) --- 10 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 1736 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  4 < agent__treebased__dott__ . (0.1) --- 11 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1733 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  6 < agent__treebased__dott__ . (0.1) --- 12 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1772 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < agent__treebased__dott__ . (0.1) --- 13 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1725 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  6 < agent__treebased__dott__ . (0.1) --- 14 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1723 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  2 < agent__treebased__dott__ . (0.1) --- 15 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1690 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  2 < agent__treebased__dott__ . (0.1) --- 16 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1912 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < agent__treebased__dott__ . (0.1) --- 17 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2375 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  4 < agent__treebased__dott__ . (0.1) --- 18 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2,0,0,4,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 2521 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < agent__treebased__dott__ . (0.1) --- 19 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2,0,0,4,4,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2660 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  0 < agent__treebased__dott__ . (0.1) --- 20 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2,0,0,4,4,0,0,4,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2883 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  6 < agent__treebased__dott__ . (0.1) --- 21 : [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2,0,0,4,4,0,0,4,0,4,6] \n",
      "\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "  0 0  0   0    0      MontyCarlo_BtS_JMcGuigan  -  agent__treebased__dott__      0    0   0  0 0\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "\n",
      "1.MontyCarlo_BtS_JMcGuigan [1]   2.agent__treebased__dott__ [2]   1.Time = 21.01   2.Time = 1.38   q.moves: 21   1.speed = 1.0   2.speed = 0.1   \n",
      "\n",
      "Game: [3,3,3,3,3,3,4,5,5,5,5,5,1,2,2,2,1,1,1,1,5,4,1,6,6,6,2,6,6,2,0,2,0,0,4,4,0,0,4,0,4,6]   \n",
      "\n",
      "Board: [2,1,2,2,1,1,2,2,2,2,1,1,2,1,1,1,1,2,2,1,2,2,2,2,1,1,2,2,1,1,1,2,2,1,1,1,1,2,1,1,2,2] \n",
      "\n",
      "[2 1 2 2 1 1 2]\n",
      "[2 2 2 1 1 2 1]\n",
      "[1 1 1 2 2 1 2]\n",
      "[2 2 2 1 1 2 2]\n",
      "[1 1 1 2 2 1 1]\n",
      "[1 1 2 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "26 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [14], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2]], 'time': [12.01], 'moves': [170], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [4], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0]], 'time': [164.43], 'moves': [161], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [18], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3]], 'time': [91.02], 'moves': [158], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3]], 'time': [162.66], 'moves': [162], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [4], 'games': [[1], [0, 0, 3, 0]], 'time': [81.98], 'moves': [81], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0, 0]], 'time': [24.67], 'moves': [71], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3]], 'time': [22.54], 'moves': [70], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_BtS_JMcGuigan   vs   agent_MCTS_MatanTsipory_\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2765 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 1 : [3,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 2578 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 2 : [3,1,1,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 442 simulations in 1.247s\n",
      "(1.2) . MontyCarlo_BtS_JMcGuigan > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 3 : [3,1,1,3,3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2167 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 4 : [3,1,1,3,3,4,3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1942 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 5 : [3,1,1,3,3,4,3,4,4,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1761 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 6 : [3,1,1,3,3,4,3,4,4,3,4,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1590 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 7 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1510 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 8 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1526 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 9 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1616 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 10 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1594 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 11 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1790 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 12 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1810 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 13 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1760 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 14 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0,0,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 2688 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 15 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0,0,1,6,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 3177 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 16 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0,0,1,6,0,4,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 3209 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 17 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0,0,1,6,0,4,5,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 3125 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 18 : [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0,0,1,6,0,4,5,5,5,2,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_MCTS_MatanTsipory_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_BtS_JMcGuigan [0]   2.agent_MCTS_MatanTsipory_ [3]   1.Time = 18.25   2.Time = 18.02   q.moves: 18   1.speed = 1.0   2.speed = 1.0   \n",
      "\n",
      "Game: [3,1,1,3,3,4,3,4,4,3,4,4,1,1,3,6,1,6,6,6,0,2,0,6,0,0,0,1,6,0,4,5,5,5,2,2]   \n",
      "\n",
      "Board: [2,2,0,1,1,0,1,1,1,0,2,2,0,2,2,2,0,1,1,0,2,1,1,2,1,1,2,1,1,1,1,2,2,1,2,1,2,2,1,2,2,2] \n",
      "\n",
      "[2 2 0 1 1 0 1]\n",
      "[1 1 0 2 2 0 2]\n",
      "[2 2 0 1 1 0 2]\n",
      "[1 1 2 1 1 2 1]\n",
      "[1 1 1 2 2 1 2]\n",
      "[1 2 2 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "27 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [14], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2]], 'time': [12.01], 'moves': [170], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3]], 'time': [182.45], 'moves': [179], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [18], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3]], 'time': [91.02], 'moves': [158], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3]], 'time': [162.66], 'moves': [162], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [4], 'games': [[1, 0], [0, 0, 3, 0]], 'time': [100.23], 'moves': [99], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0, 0]], 'time': [24.67], 'moves': [71], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3]], 'time': [22.54], 'moves': [70], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_BtS_JMcGuigan   vs   MCTS_wAdaptive_Playouts_\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2665 simulations in 1.000s\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.8632075471698113 stone_count 1 playouts 212 ***\n",
      "time_limit 0.35 my duration 0.35167574882507324 x 4\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  4 < MCTS_wAdaptive_Playouts_ . (0.4) --- 1 : [3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2609 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9516483516483516 stone_count 3 playouts 455 ***\n",
      "time_limit 0.7 my duration 0.7014412879943848 x 2\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 2 : [3,4,4,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2454 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.24621212121212122 stone_count 5 playouts 528 ***\n",
      "time_limit 0.7 my duration 0.70162034034729 x 4\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 3 : [3,4,4,2,4,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2137 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9726495726495726 stone_count 7 playouts 585 ***\n",
      "time_limit 0.7 my duration 0.7008731365203857 x 3\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 4 : [3,4,4,2,4,4,3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1835 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0.] root_rewards 1.867177522349936 stone_count 9 playouts 783 ***\n",
      "time_limit 0.7 my duration 0.7005178928375244 x 3\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 5 : [3,4,4,2,4,4,3,3,2,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1748 simulations in 1.000s\n",
      "[0. 0. 0. 0. 0.] root_rewards 1.8850787766450416 stone_count 11 playouts 1079 ***\n",
      "time_limit 0.7 my duration 0.7008411884307861 x 2\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 6 : [3,4,4,2,4,4,3,3,2,3,3,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1724 simulations in 1.001s\n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9508700102354146 stone_count 13 playouts 977 ***\n",
      "time_limit 0.7 my duration 0.700531005859375 x 4\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 7 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1797 simulations in 1.000s\n",
      "[0. 0. 0. 0.] root_rewards 1.9986833443054641 stone_count 15 playouts 1519 ***\n",
      "time_limit 0.7 my duration 0.7008645534515381 x 4\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 8 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1810 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 1.9661222020568663 stone_count 17 playouts 1653 ***\n",
      "time_limit 0.7 my duration 0.7008635997772217 x 0\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 9 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1666 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 1.9691081215744892 stone_count 19 playouts 2007 ***\n",
      "time_limit 0.7 my duration 0.7006895542144775 x 0\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 10 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1661 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 21 playouts 3053 ***\n",
      "time_limit 0.7 my duration 0.7005014419555664 x 0\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 11 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1598 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 1.9734939759036145 stone_count 23 playouts 3320 ***\n",
      "time_limit 0.7 my duration 0.7005586624145508 x 6\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 12 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1688 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 1.9989197947610047 stone_count 25 playouts 3703 ***\n",
      "time_limit 0.7 my duration 0.7005324363708496 x 6\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 13 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 2132 simulations in 1.000s\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00013303756713867188 x 5\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 14 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6,6,6,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 3100 simulations in 1.000s\n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 29 playouts 10663 ***\n",
      "time_limit 0.7 my duration 0.7005207538604736 x 2\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 15 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6,6,6,5,5,6,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 3337 simulations in 1.000s\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001811981201171875 x 6\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  6 < MCTS_wAdaptive_Playouts_ . (0.0) --- 16 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6,6,6,5,5,6,2,0,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 3473 simulations in 1.000s\n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00020694732666015625 x 5\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < MCTS_wAdaptive_Playouts_ . (0.0) --- 17 : [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6,6,6,5,5,6,2,0,6,5,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MCTS_wAdaptive_Playouts_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_BtS_JMcGuigan [0]   2.MCTS_wAdaptive_Playouts_ [3]   1.Time = 17.01   2.Time = 9.46   q.moves: 17   1.speed = 1.0   2.speed = 0.6   \n",
      "\n",
      "Game: [3,4,4,2,4,4,3,3,2,3,3,2,2,4,3,4,2,0,0,0,0,0,6,6,6,6,5,5,6,2,0,6,5,5]   \n",
      "\n",
      "Board: [1,0,2,1,2,0,2,2,0,1,1,2,0,1,1,0,1,2,2,2,2,2,0,2,2,1,1,1,1,0,1,1,1,2,2,2,0,2,1,2,1,1] \n",
      "\n",
      "[1 0 2 1 2 0 2]\n",
      "[2 0 1 1 2 0 1]\n",
      "[1 0 1 2 2 2 2]\n",
      "[2 0 2 2 1 1 1]\n",
      "[1 0 1 1 1 2 2]\n",
      "[2 0 2 1 2 1 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "28 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [14], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2]], 'time': [12.01], 'moves': [170], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3]], 'time': [182.45], 'moves': [179], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [21], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3]], 'time': [100.48], 'moves': [175], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3]], 'time': [162.66], 'moves': [162], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [4], 'games': [[1, 0, 0], [0, 0, 3, 0]], 'time': [117.24], 'moves': [116], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0, 0]], 'time': [24.67], 'moves': [71], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3]], 'time': [22.54], 'moves': [70], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_BtS_JMcGuigan   vs   MontyCarlo_JamesMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1096 simulations in 1.695s\n",
      "MontyCarloNode: p2 action = 3 after 1577 simulations in 1.000s\n",
      "(1.7) . MontyCarlo_BtS_JMcGuigan > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2401 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 4 after 1647 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 2 : [3,3,3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2233 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 4 after 1920 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 3 : [3,3,3,4,3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1919 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 3 after 1989 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 4 : [3,3,3,4,3,4,4,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1637 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 1 after 1955 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 5 : [3,3,3,4,3,4,4,3,4,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1812 simulations in 1.001s\n",
      "MontyCarloNode: p2 action = 4 after 1945 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 6 : [3,3,3,4,3,4,4,3,4,1,4,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1652 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 1 after 1897 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 7 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1606 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 6 after 1921 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 8 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1592 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 6 after 1779 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 9 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1609 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 6 after 1745 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 10 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1615 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 0 after 1905 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 11 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1571 simulations in 1.002s\n",
      "MontyCarloNode: p2 action = 0 after 1858 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 12 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1545 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 6 after 2003 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 13 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1520 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 0 after 2279 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 14 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6,1,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 378 simulations in 1.111s\n",
      "MontyCarloNode: p2 action = 5 after 2992 simulations in 1.000s\n",
      "(1.1) . MontyCarlo_BtS_JMcGuigan > 2  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 15 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6,1,0,2,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 2013 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 5 after 2814 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 16 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6,1,0,2,5,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 1936 simulations in 1.000s\n",
      "MontyCarloNode: p2 action = 5 after 2741 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 17 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6,1,0,2,5,5,5,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 2738 simulations in 1.001s\n",
      "MontyCarloNode: p2 action = 2 after 3093 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 18 : [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6,1,0,2,5,5,5,5,5,1,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 3152 simulations in 1.000s\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MontyCarlo_BtS_JMcGuigan      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_BtS_JMcGuigan [3]   2.MontyCarlo_JamesMcGuigan [0]   1.Time = 19.82   2.Time = 18.01   q.moves: 19   1.speed = 1.0   2.speed = 0.9   \n",
      "\n",
      "Game: [3,3,3,4,3,4,4,3,4,1,4,4,1,1,1,6,6,6,3,6,6,0,0,0,0,6,1,0,2,5,5,5,5,5,1,2,2]   \n",
      "\n",
      "Board: [0,1,0,1,2,0,2,2,1,0,2,1,2,1,1,1,0,1,1,1,2,2,2,1,1,1,2,2,1,1,2,2,2,1,1,2,2,1,1,2,2,2] \n",
      "\n",
      "[0 1 0 1 2 0 2]\n",
      "[2 1 0 2 1 2 1]\n",
      "[1 1 0 1 1 1 2]\n",
      "[2 2 1 1 1 2 2]\n",
      "[1 1 2 2 2 1 1]\n",
      "[2 2 1 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "29 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [14], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2]], 'time': [12.01], 'moves': [170], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3]], 'time': [182.45], 'moves': [179], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [21], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3]], 'time': [100.48], 'moves': [175], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0]], 'time': [180.67], 'moves': [180], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [7], 'games': [[1, 0, 0, 3], [0, 0, 3, 0]], 'time': [137.06], 'moves': [135], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [6], 'games': [[], [3, 3, 0, 0]], 'time': [24.67], 'moves': [71], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3]], 'time': [22.54], 'moves': [70], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_BtS_JMcGuigan   vs   agent_TheFast_mini_Max_2\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2749 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent_TheFast_mini_Max_2 . (0.0) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2170 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent_TheFast_mini_Max_2 . (0.5) --- 2 : [3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1900 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  2 < agent_TheFast_mini_Max_2 . (0.5) --- 3 : [3,3,3,3,3,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1693 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 4 : [3,3,3,3,3,2,2,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1638 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  4 < agent_TheFast_mini_Max_2 . (0.5) --- 5 : [3,3,3,3,3,2,2,2,2,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1949 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent_TheFast_mini_Max_2 . (0.5) --- 6 : [3,3,3,3,3,2,2,2,2,4,4,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1677 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  1 < agent_TheFast_mini_Max_2 . (0.5) --- 7 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1669 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  1 < agent_TheFast_mini_Max_2 . (0.5) --- 8 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1709 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  3 < agent_TheFast_mini_Max_2 . (0.5) --- 9 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1686 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  1 < agent_TheFast_mini_Max_2 . (0.5) --- 10 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 1560 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  1 < agent_TheFast_mini_Max_2 . (0.5) --- 11 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1592 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  0 < agent_TheFast_mini_Max_2 . (0.5) --- 12 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1586 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  1 < agent_TheFast_mini_Max_2 . (0.5) --- 13 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1628 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < agent_TheFast_mini_Max_2 . (0.5) --- 14 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1661 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  0 < agent_TheFast_mini_Max_2 . (0.2) --- 15 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1714 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  0 < agent_TheFast_mini_Max_2 . (0.1) --- 16 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 2583 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < agent_TheFast_mini_Max_2 . (0.0) --- 17 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 2740 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  4 < agent_TheFast_mini_Max_2 . (0.0) --- 18 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0,5,5,5,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2772 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent_TheFast_mini_Max_2 . (0.0) --- 19 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0,5,5,5,4,4,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 2561 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < agent_TheFast_mini_Max_2 . (0.0) --- 20 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0,5,5,5,4,4,5,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 2853 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent_TheFast_mini_Max_2 . (0.0) --- 21 : [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0,5,5,5,4,4,5,6,6,4,5] \n",
      "\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "  0 0  0   0    0      MontyCarlo_BtS_JMcGuigan  -  agent_TheFast_mini_Max_2      0    0   0  0 0\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "\n",
      "1.MontyCarlo_BtS_JMcGuigan [1]   2.agent_TheFast_mini_Max_2 [2]   1.Time = 21.01   2.Time = 6.83   q.moves: 21   1.speed = 1.0   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,3,3,3,2,2,2,2,4,4,5,2,1,4,1,0,3,0,1,1,1,2,0,0,1,6,6,6,0,6,0,5,5,5,4,4,5,6,6,4,5]   \n",
      "\n",
      "Board: [2,2,1,2,1,2,2,2,2,1,1,1,2,1,1,1,1,2,2,1,1,2,2,2,1,1,2,1,1,2,1,2,1,1,2,1,2,2,1,2,2,1] \n",
      "\n",
      "[2 2 1 2 1 2 2]\n",
      "[2 2 1 1 1 2 1]\n",
      "[1 1 1 2 2 1 1]\n",
      "[2 2 2 1 1 2 1]\n",
      "[1 2 1 2 1 1 2]\n",
      "[1 2 2 1 2 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "30 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [14], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2]], 'time': [12.01], 'moves': [170], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3]], 'time': [182.45], 'moves': [179], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [21], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3]], 'time': [100.48], 'moves': [175], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0]], 'time': [180.67], 'moves': [180], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [8], 'games': [[1, 0, 0, 3, 1], [0, 0, 3, 0]], 'time': [158.07], 'moves': [156], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [8], 'games': [[], [3, 3, 0, 0, 2]], 'time': [31.5], 'moves': [92], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3]], 'time': [22.54], 'moves': [70], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "MontyCarlo_BtS_JMcGuigan   vs   agent_TheFast_mini_Max_4\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2613 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent_TheFast_mini_Max_4 . (0.0) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 2233 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  3 < agent_TheFast_mini_Max_4 . (0.5) --- 2 : [3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1737 simulations in 2.090s\n",
      "(2.1) . MontyCarlo_BtS_JMcGuigan > 3  4 < agent_TheFast_mini_Max_4 . (0.5) --- 3 : [3,3,3,3,3,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1768 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  4 < agent_TheFast_mini_Max_4 . (0.5) --- 4 : [3,3,3,3,3,4,4,4] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1607 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  2 < agent_TheFast_mini_Max_4 . (0.5) --- 5 : [3,3,3,3,3,4,4,4,4,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1939 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  1 < agent_TheFast_mini_Max_4 . (0.5) --- 6 : [3,3,3,3,3,4,4,4,4,2,2,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1653 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent_TheFast_mini_Max_4 . (0.5) --- 7 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 1666 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 2  5 < agent_TheFast_mini_Max_4 . (0.5) --- 8 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1704 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  6 < agent_TheFast_mini_Max_4 . (0.5) --- 9 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 3 after 1658 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 3  6 < agent_TheFast_mini_Max_4 . (0.5) --- 10 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 6 after 1710 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 6  5 < agent_TheFast_mini_Max_4 . (0.5) --- 11 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 5 after 1576 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 5  5 < agent_TheFast_mini_Max_4 . (0.5) --- 12 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 4 after 1508 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 4  5 < agent_TheFast_mini_Max_4 . (0.5) --- 13 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1452 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < agent_TheFast_mini_Max_4 . (0.3) --- 14 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1577 simulations in 1.001s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  0 < agent_TheFast_mini_Max_4 . (0.1) --- 15 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0,0,0] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 1919 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  6 < agent_TheFast_mini_Max_4 . (0.0) --- 16 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0,0,0,0,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 0 after 2618 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 0  6 < agent_TheFast_mini_Max_4 . (0.0) --- 17 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0,0,0,0,6,0,6] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 2753 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  1 < agent_TheFast_mini_Max_4 . (0.0) --- 18 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0,0,0,0,6,0,6,1,1] \n",
      "MontyCarloBitsquaresNode: p1 action = 1 after 2768 simulations in 1.000s\n",
      "(1.0) . MontyCarlo_BtS_JMcGuigan > 1  2 < agent_TheFast_mini_Max_4 . (0.0) --- 19 : [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0,0,0,0,6,0,6,1,1,1,2] \n",
      "MontyCarloBitsquaresNode: p1 action = 2 after 2826 simulations in 1.000s\n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      MontyCarlo_BtS_JMcGuigan      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.MontyCarlo_BtS_JMcGuigan [3]   2.agent_TheFast_mini_Max_4 [0]   1.Time = 21.1   2.Time = 6.39   q.moves: 20   1.speed = 1.1   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,3,3,3,4,4,4,4,2,2,1,4,5,2,5,6,6,3,6,6,5,5,5,4,5,0,0,0,0,0,6,0,6,1,1,1,2,2]   \n",
      "\n",
      "Board: [1,0,0,1,1,2,2,1,0,1,1,1,2,2,2,1,2,2,1,1,1,1,2,1,1,2,2,2,2,1,1,2,1,2,2,1,2,2,1,2,2,1] \n",
      "\n",
      "[1 0 0 1 1 2 2]\n",
      "[1 0 1 1 1 2 2]\n",
      "[2 1 2 2 1 1 1]\n",
      "[1 2 1 1 2 2 2]\n",
      "[2 1 1 2 1 2 2]\n",
      "[1 2 2 1 2 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "31 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [14], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2]], 'time': [12.01], 'moves': [170], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3]], 'time': [182.45], 'moves': [179], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [21], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3]], 'time': [100.48], 'moves': [175], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0]], 'time': [180.67], 'moves': [180], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0]], 'time': [179.17], 'moves': [176], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [8], 'games': [[], [3, 3, 0, 0, 2]], 'time': [31.5], 'moves': [92], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0]], 'time': [28.93], 'moves': [89], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_2   vs   agent__treebased__dott__\n",
      "-----------------------------------\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 3  3 < agent__treebased__dott__ . (0.1) --- 1 : [3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < agent__treebased__dott__ . (0.1) --- 2 : [3,3,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  2 < agent__treebased__dott__ . (0.1) --- 3 : [3,3,3,3,4,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  6 < agent__treebased__dott__ . (0.1) --- 4 : [3,3,3,3,4,2,5,6] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  4 < agent__treebased__dott__ . (0.1) --- 5 : [3,3,3,3,4,2,5,6,4,4] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  4 < agent__treebased__dott__ . (0.1) --- 6 : [3,3,3,3,4,2,5,6,4,4,5,4] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  4 < agent__treebased__dott__ . (0.1) --- 7 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  3 < agent__treebased__dott__ . (0.1) --- 8 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  0 < agent__treebased__dott__ . (0.1) --- 9 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  6 < agent__treebased__dott__ . (0.1) --- 10 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  5 < agent__treebased__dott__ . (0.1) --- 11 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  1 < agent__treebased__dott__ . (0.1) --- 12 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  1 < agent__treebased__dott__ . (0.1) --- 13 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  1 < agent__treebased__dott__ . (0.1) --- 14 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  2 < agent__treebased__dott__ . (0.1) --- 15 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  0 < agent__treebased__dott__ . (0.1) --- 16 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0] \n",
      "(0.3) . agent_TheFast_mini_Max_2 > 2  6 < agent__treebased__dott__ . (0.1) --- 17 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0,2,6] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 5  6 < agent__treebased__dott__ . (0.1) --- 18 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0,2,6,5,6] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 1  0 < agent__treebased__dott__ . (0.1) --- 19 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0,2,6,5,6,1,0] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 0  0 < agent__treebased__dott__ . (0.1) --- 20 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0,2,6,5,6,1,0,0,0] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 5  0 < agent__treebased__dott__ . (0.1) --- 21 : [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0,2,6,5,6,1,0,0,0,5,0] \n",
      "\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "  0 0  0   0    0      agent_TheFast_mini_Max_2  -  agent__treebased__dott__      0    0   0  0 0\n",
      "DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W DRAW RAW AW W\n",
      "\n",
      "1.agent_TheFast_mini_Max_2 [1]   2.agent__treebased__dott__ [2]   1.Time = 7.85   2.Time = 1.5   q.moves: 21   1.speed = 0.4   2.speed = 0.1   \n",
      "\n",
      "Game: [3,3,3,3,4,2,5,6,4,4,5,4,3,4,4,3,2,0,6,6,6,5,5,1,1,1,1,1,2,2,2,0,2,6,5,6,1,0,0,0,5,0]   \n",
      "\n",
      "Board: [2,1,1,2,1,1,2,2,2,1,1,2,1,2,1,1,2,2,2,1,1,2,2,1,1,2,2,2,2,1,1,2,1,1,1,2,2,2,1,1,1,2] \n",
      "\n",
      "[2 1 1 2 1 1 2]\n",
      "[2 2 1 1 2 1 2]\n",
      "[1 1 2 2 2 1 1]\n",
      "[2 2 1 1 2 2 2]\n",
      "[2 1 1 2 1 1 1]\n",
      "[2 2 2 1 1 1 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "32 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [16], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2]], 'time': [13.51], 'moves': [191], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3]], 'time': [182.45], 'moves': [179], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [21], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3]], 'time': [100.48], 'moves': [175], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0]], 'time': [180.67], 'moves': [180], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0]], 'time': [179.17], 'moves': [176], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [9], 'games': [[1], [3, 3, 0, 0, 2]], 'time': [39.35], 'moves': [113], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0]], 'time': [28.93], 'moves': [89], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_2   vs   agent_MCTS_MatanTsipory_\n",
      "-----------------------------------\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 3  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 1 : [3,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 2 : [3,2,3,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 3 : [3,2,3,2,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 4 : [3,2,3,2,3,3,2,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 5 : [3,2,3,2,3,3,2,2,4,6] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 6 : [3,2,3,2,3,3,2,2,4,6,6,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 7 : [3,2,3,2,3,3,2,2,4,6,6,3,1,4] \n",
      "(0.1) . agent_TheFast_mini_Max_2 > 4  4 < agent_MCTS_MatanTsipory_ . (1.1) --- 8 : [3,2,3,2,3,3,2,2,4,6,6,3,1,4,4,4] \n",
      "(0.1) . agent_TheFast_mini_Max_2 > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 9 : [3,2,3,2,3,3,2,2,4,6,6,3,1,4,4,4,3,4] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 4  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 10 : [3,2,3,2,3,3,2,2,4,6,6,3,1,4,4,4,3,4,4,0] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 1  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 11 : [3,2,3,2,3,3,2,2,4,6,6,3,1,4,4,4,3,4,4,0,1,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent_TheFast_mini_Max_2      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_2 [3]   2.agent_MCTS_MatanTsipory_ [0]   1.Time = 3.24   2.Time = 11.08   q.moves: 12   1.speed = 0.3   2.speed = 0.9   \n",
      "\n",
      "Game: [3,2,3,2,3,3,2,2,4,6,6,3,1,4,4,4,3,4,4,0,1,5,1]   \n",
      "\n",
      "Board: [0,0,0,1,1,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,0,0,1,1,1,1,0,0,0,1,2,1,2,0,1,2,1,2,1,1,2,2] \n",
      "\n",
      "[0 0 0 1 1 0 0]\n",
      "[0 0 0 2 2 0 0]\n",
      "[0 0 2 2 2 0 0]\n",
      "[0 1 1 1 1 0 0]\n",
      "[0 1 2 1 2 0 1]\n",
      "[2 1 2 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "33 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [16], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2]], 'time': [13.51], 'moves': [191], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0]], 'time': [193.53], 'moves': [190], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [21], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3]], 'time': [100.48], 'moves': [175], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0]], 'time': [180.67], 'moves': [180], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0]], 'time': [179.17], 'moves': [176], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [12], 'games': [[1, 3], [3, 3, 0, 0, 2]], 'time': [42.59], 'moves': [125], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0]], 'time': [28.93], 'moves': [89], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_2   vs   MCTS_wAdaptive_Playouts_\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.824 stone_count 1 playouts 250 ***\n",
      "time_limit 0.35 my duration 0.3510897159576416 x 1\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 3  1 < MCTS_wAdaptive_Playouts_ . (0.4) --- 1 : [3,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9828326180257511 stone_count 3 playouts 466 ***\n",
      "time_limit 0.7 my duration 0.702584981918335 x 5\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 2 : [3,1,4,5] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7742574257425743 stone_count 5 playouts 505 ***\n",
      "time_limit 0.7 my duration 0.7026088237762451 x 4\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 3 : [3,1,4,5,4,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.7227036395147313 stone_count 7 playouts 577 ***\n",
      "time_limit 0.7 my duration 0.7010788917541504 x 3\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 4 : [3,1,4,5,4,4,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9483568075117371 stone_count 9 playouts 639 ***\n",
      "time_limit 0.7 my duration 0.7009899616241455 x 4\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 5 : [3,1,4,5,4,4,3,3,4,4] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7620528771384136 stone_count 11 playouts 643 ***\n",
      "time_limit 0.7 my duration 0.7009530067443848 x 3\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 6 : [3,1,4,5,4,4,3,3,4,4,3,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 0.8432642487046632 stone_count 13 playouts 772 ***\n",
      "time_limit 0.7 my duration 0.701228141784668 x 0\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 7 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.7717528373266078 stone_count 15 playouts 793 ***\n",
      "time_limit 0.7 my duration 0.7012379169464111 x 3\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 0  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 8 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.8565891472868217 stone_count 17 playouts 1032 ***\n",
      "time_limit 0.7 my duration 0.7012670040130615 x 1\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 9 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9993069993069994 stone_count 19 playouts 1443 ***\n",
      "time_limit 0.7 my duration 0.7009811401367188 x 4\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 10 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4] \n",
      "[0. 0. 0. 0.] root_rewards 1.9787844036697249 stone_count 21 playouts 1744 ***\n",
      "time_limit 0.7 my duration 0.7013628482818604 x 1\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 0  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 11 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1] \n",
      "[0. 0.] root_rewards 2.0 stone_count 23 playouts 2536 ***\n",
      "time_limit 0.7 my duration 0.7007260322570801 x 0\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 12 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0] \n",
      "[0. 0.] root_rewards 2.0 stone_count 25 playouts 2750 ***\n",
      "time_limit 0.7 my duration 0.7011096477508545 x 5\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 13 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0002970695495605469 x 2\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  2 < MCTS_wAdaptive_Playouts_ . (0.0) --- 14 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2] \n",
      "[0. 0. 0.] root_rewards 1.9907102187593646 stone_count 29 playouts 3337 ***\n",
      "time_limit 0.7 my duration 0.7005729675292969 x 2\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 15 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2] \n",
      "[0. 0. 0.] root_rewards 1.9998003593531644 stone_count 31 playouts 5009 ***\n",
      "time_limit 0.7 my duration 0.7005615234375 x 0\n",
      "(0.1) . agent_TheFast_mini_Max_2 > 2  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 16 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2,2,0] \n",
      "[0. 0.] root_rewards 2.0 stone_count 33 playouts 10244 ***\n",
      "time_limit 0.7 my duration 0.7005646228790283 x 0\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 5  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 17 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2,2,0,5,0] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001761913299560547 x 6\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 6  6 < MCTS_wAdaptive_Playouts_ . (0.0) --- 18 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2,2,0,5,0,6,6] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.000125885009765625 x 6\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 6  6 < MCTS_wAdaptive_Playouts_ . (0.0) --- 19 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2,2,0,5,0,6,6,6,6] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0001728534698486328 x 2\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 2  2 < MCTS_wAdaptive_Playouts_ . (0.0) --- 20 : [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2,2,0,5,0,6,6,6,6,2,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MCTS_wAdaptive_Playouts_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_2 [0]   2.MCTS_wAdaptive_Playouts_ [3]   1.Time = 7.1   2.Time = 10.87   q.moves: 20   1.speed = 0.4   2.speed = 0.5   \n",
      "\n",
      "Game: [3,1,4,5,4,4,3,3,4,4,3,3,1,0,0,3,1,1,1,4,0,1,5,0,5,5,2,2,5,2,2,0,5,0,6,6,6,6,2,2]   \n",
      "\n",
      "Board: [2,2,2,2,2,1,0,2,1,1,2,2,1,0,2,2,1,1,1,2,2,1,1,2,2,2,1,1,1,1,2,1,1,1,2,2,2,1,1,1,2,1] \n",
      "\n",
      "[2 2 2 2 2 1 0]\n",
      "[2 1 1 2 2 1 0]\n",
      "[2 2 1 1 1 2 2]\n",
      "[1 1 2 2 2 1 1]\n",
      "[1 1 2 1 1 1 2]\n",
      "[2 2 1 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "34 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [16], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2]], 'time': [13.51], 'moves': [191], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0]], 'time': [193.53], 'moves': [190], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [24], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3]], 'time': [111.35], 'moves': [195], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0]], 'time': [180.67], 'moves': [180], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0]], 'time': [179.17], 'moves': [176], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [12], 'games': [[1, 3, 0], [3, 3, 0, 0, 2]], 'time': [49.69], 'moves': [145], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0]], 'time': [28.93], 'moves': [89], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_2   vs   MontyCarlo_JamesMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloNode: p2 action = 3 after 1756 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloNode: p2 action = 4 after 1783 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 2 : [3,3,3,4] \n",
      "MontyCarloNode: p2 action = 4 after 1973 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 3 : [3,3,3,4,3,4] \n",
      "MontyCarloNode: p2 action = 6 after 2068 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 4 : [3,3,3,4,3,4,4,6] \n",
      "MontyCarloNode: p2 action = 6 after 2313 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 5 : [3,3,3,4,3,4,4,6,4,6] \n",
      "MontyCarloNode: p2 action = 1 after 2296 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 6 : [3,3,3,4,3,4,4,6,4,6,6,1] \n",
      "MontyCarloNode: p2 action = 3 after 2120 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 7 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3] \n",
      "MontyCarloNode: p2 action = 4 after 2111 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 8 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4] \n",
      "MontyCarloNode: p2 action = 6 after 2167 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 9 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6] \n",
      "MontyCarloNode: p2 action = 6 after 2073 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 0  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 10 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6] \n",
      "MontyCarloNode: p2 action = 0 after 2112 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 11 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0] \n",
      "MontyCarloNode: p2 action = 0 after 2488 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 12 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0,2,0] \n",
      "MontyCarloNode: p2 action = 2 after 3115 simulations in 1.000s\n",
      "(0.1) . agent_TheFast_mini_Max_2 > 2  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 13 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0,2,0,2,2] \n",
      "MontyCarloNode: p2 action = 1 after 3475 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 0  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 14 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0,2,0,2,2,0,1] \n",
      "MontyCarloNode: p2 action = 2 after 3640 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 1  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 15 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0,2,0,2,2,0,1,1,2] \n",
      "MontyCarloNode: p2 action = 0 after 3667 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 2  0 < MontyCarlo_JamesMcGuigan . (1.0) --- 16 : [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0,2,0,2,2,0,1,1,2,2,0] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent_TheFast_mini_Max_2      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_2 [3]   2.MontyCarlo_JamesMcGuigan [0]   1.Time = 5.65   2.Time = 16.01   q.moves: 17   1.speed = 0.3   2.speed = 0.9   \n",
      "\n",
      "Game: [3,3,3,4,3,4,4,6,4,6,6,1,3,3,4,4,1,6,0,6,6,0,2,0,2,2,0,1,1,2,2,0,1]   \n",
      "\n",
      "Board: [0,0,0,2,2,0,1,2,1,1,1,1,0,2,1,1,2,1,1,0,2,2,2,2,1,1,0,1,2,1,1,2,2,0,2,1,2,1,1,2,0,2] \n",
      "\n",
      "[0 0 0 2 2 0 1]\n",
      "[2 1 1 1 1 0 2]\n",
      "[1 1 2 1 1 0 2]\n",
      "[2 2 2 1 1 0 1]\n",
      "[2 1 1 2 2 0 2]\n",
      "[1 2 1 1 2 0 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "35 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [16], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2]], 'time': [13.51], 'moves': [191], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0]], 'time': [193.53], 'moves': [190], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [24], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3]], 'time': [111.35], 'moves': [195], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0]], 'time': [196.68], 'moves': [196], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0]], 'time': [179.17], 'moves': [176], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [15], 'games': [[1, 3, 0, 3], [3, 3, 0, 0, 2]], 'time': [55.34], 'moves': [162], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0]], 'time': [28.93], 'moves': [89], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_2   vs   MontyCarlo_BtS_JMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2658 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2187 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 2 : [3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 2362 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 3 : [3,3,3,3,2,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 2212 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 4 : [3,3,3,3,2,4,1,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 2122 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 5 : [3,3,3,3,2,4,1,0,2,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1986 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 6 : [3,3,3,3,2,4,1,0,2,2,1,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1989 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 7 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1968 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 0  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 8 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1830 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 9 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 1784 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 10 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 1620 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 11 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 1605 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 12 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 1556 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 13 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6,6,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 136 simulations in 1.410s\n",
      "(0.4) . agent_TheFast_mini_Max_2 > 1  6 < MontyCarlo_BtS_JMcGuigan . (1.4) --- 14 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6,6,0,1,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 2597 simulations in 1.000s\n",
      "(0.1) . agent_TheFast_mini_Max_2 > 1  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 15 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6,6,0,1,6,1,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 3144 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 0  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 16 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6,6,0,1,6,1,0,0,5] \n",
      "MontyCarloBitsquaresNode: p2 action = 5 after 3270 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 6  5 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 17 : [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6,6,0,1,6,1,0,0,5,6,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent_TheFast_mini_Max_2      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_2 [3]   2.MontyCarlo_BtS_JMcGuigan [0]   1.Time = 6.49   2.Time = 17.42   q.moves: 18   1.speed = 0.4   2.speed = 1.0   \n",
      "\n",
      "Game: [3,3,3,3,2,4,1,0,2,2,1,2,2,3,0,3,4,2,6,1,1,0,6,6,6,0,1,6,1,0,0,5,6,5,5]   \n",
      "\n",
      "Board: [1,1,2,2,0,0,1,2,1,1,2,0,0,2,2,1,2,2,0,0,1,2,2,2,1,0,1,2,1,1,1,2,1,2,1,2,1,1,1,2,2,1] \n",
      "\n",
      "[1 1 2 2 0 0 1]\n",
      "[2 1 1 2 0 0 2]\n",
      "[2 1 2 2 0 0 1]\n",
      "[2 2 2 1 0 1 2]\n",
      "[1 1 1 2 1 2 1]\n",
      "[2 1 1 1 2 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "36 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [16], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2]], 'time': [13.51], 'moves': [191], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0]], 'time': [193.53], 'moves': [190], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [24], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3]], 'time': [111.35], 'moves': [195], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0]], 'time': [196.68], 'moves': [196], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0]], 'time': [196.59], 'moves': [193], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [18], 'games': [[1, 3, 0, 3, 3], [3, 3, 0, 0, 2]], 'time': [61.83], 'moves': [180], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0]], 'time': [28.93], 'moves': [89], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_2   vs   agent_TheFast_mini_Max_4\n",
      "-----------------------------------\n",
      "(0.0) . agent_TheFast_mini_Max_2 > 3  3 < agent_TheFast_mini_Max_4 . (0.0) --- 1 : [3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  3 < agent_TheFast_mini_Max_4 . (0.5) --- 2 : [3,3,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  5 < agent_TheFast_mini_Max_4 . (0.5) --- 3 : [3,3,3,3,4,5] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 1  2 < agent_TheFast_mini_Max_4 . (0.5) --- 4 : [3,3,3,3,4,5,1,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  5 < agent_TheFast_mini_Max_4 . (0.5) --- 5 : [3,3,3,3,4,5,1,2,2,5] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  5 < agent_TheFast_mini_Max_4 . (0.5) --- 6 : [3,3,3,3,4,5,1,2,2,5,4,5] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 5  6 < agent_TheFast_mini_Max_4 . (0.5) --- 7 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  4 < agent_TheFast_mini_Max_4 . (0.5) --- 8 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  4 < agent_TheFast_mini_Max_4 . (0.5) --- 9 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 2  2 < agent_TheFast_mini_Max_4 . (0.5) --- 10 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 3  2 < agent_TheFast_mini_Max_4 . (0.5) --- 11 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 4  2 < agent_TheFast_mini_Max_4 . (0.5) --- 12 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 0  0 < agent_TheFast_mini_Max_4 . (0.5) --- 13 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2,0,0] \n",
      "(0.5) . agent_TheFast_mini_Max_2 > 6  5 < agent_TheFast_mini_Max_4 . (0.2) --- 14 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2,0,0,6,5] \n",
      "(0.1) . agent_TheFast_mini_Max_2 > 5  0 < agent_TheFast_mini_Max_4 . (0.0) --- 15 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2,0,0,6,5,5,0] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 0  0 < agent_TheFast_mini_Max_4 . (0.0) --- 16 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2,0,0,6,5,5,0,0,0] \n",
      "(0.0) . agent_TheFast_mini_Max_2 > 0  1 < agent_TheFast_mini_Max_4 . (0.0) --- 17 : [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2,0,0,6,5,5,0,0,0,0,1] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent_TheFast_mini_Max_2      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_2 [3]   2.agent_TheFast_mini_Max_4 [0]   1.Time = 6.63   2.Time = 6.24   q.moves: 18   1.speed = 0.4   2.speed = 0.3   \n",
      "\n",
      "Game: [3,3,3,3,4,5,1,2,2,5,4,5,5,6,4,4,3,4,2,2,3,2,4,2,0,0,6,5,5,0,0,0,0,1,1]   \n",
      "\n",
      "Board: [1,0,2,1,1,1,0,2,0,2,1,2,2,0,1,0,2,2,2,1,0,2,1,1,1,1,2,0,2,2,1,2,1,2,1,1,1,2,1,1,2,2] \n",
      "\n",
      "[1 0 2 1 1 1 0]\n",
      "[2 0 2 1 2 2 0]\n",
      "[1 0 2 2 2 1 0]\n",
      "[2 1 1 1 1 2 0]\n",
      "[2 2 1 2 1 2 1]\n",
      "[1 1 2 1 1 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "37 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [16], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2]], 'time': [13.51], 'moves': [191], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0]], 'time': [193.53], 'moves': [190], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [24], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3]], 'time': [111.35], 'moves': [195], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0]], 'time': [196.68], 'moves': [196], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0]], 'time': [196.59], 'moves': [193], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [21], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2]], 'time': [68.46], 'moves': [198], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[], [0, 3, 3, 3, 0, 0]], 'time': [35.17], 'moves': [106], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_4   vs   agent__treebased__dott__\n",
      "-----------------------------------\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 3  3 < agent__treebased__dott__ . (0.1) --- 1 : [3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  3 < agent__treebased__dott__ . (0.1) --- 2 : [3,3,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  2 < agent__treebased__dott__ . (0.1) --- 3 : [3,3,3,3,4,2] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 5  6 < agent__treebased__dott__ . (0.1) --- 4 : [3,3,3,3,4,2,5,6] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  4 < agent__treebased__dott__ . (0.1) --- 5 : [3,3,3,3,4,2,5,6,4,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 5  4 < agent__treebased__dott__ . (0.1) --- 6 : [3,3,3,3,4,2,5,6,4,4,5,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  5 < agent__treebased__dott__ . (0.1) --- 7 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 5  3 < agent__treebased__dott__ . (0.1) --- 8 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  3 < agent__treebased__dott__ . (0.1) --- 9 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  2 < agent__treebased__dott__ . (0.1) --- 10 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  6 < agent__treebased__dott__ . (0.1) --- 11 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 5  6 < agent__treebased__dott__ . (0.1) --- 12 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 5  6 < agent__treebased__dott__ . (0.1) --- 13 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 0  0 < agent__treebased__dott__ . (0.1) --- 14 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6,0,0] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 0  0 < agent__treebased__dott__ . (0.1) --- 15 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6,0,0,0,0] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 0  0 < agent__treebased__dott__ . (0.1) --- 16 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6,0,0,0,0,0,0] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 2  2 < agent__treebased__dott__ . (0.1) --- 17 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6,0,0,0,0,0,0,2,2] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 2  2 < agent__treebased__dott__ . (0.1) --- 18 : [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6,0,0,0,0,0,0,2,2,2,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent__treebased__dott__      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_4 [0]   2.agent__treebased__dott__ [3]   1.Time = 6.06   2.Time = 1.28   q.moves: 18   1.speed = 0.3   2.speed = 0.1   \n",
      "\n",
      "Game: [3,3,3,3,4,2,5,6,4,4,5,4,4,5,5,3,6,3,4,2,6,6,5,6,5,6,0,0,0,0,0,0,2,2,2,2]   \n",
      "\n",
      "Board: [2,0,2,2,1,1,2,1,0,1,2,1,1,2,2,0,2,2,2,1,2,1,0,1,1,2,2,1,2,0,2,2,1,1,1,1,0,2,1,1,1,2] \n",
      "\n",
      "[2 0 2 2 1 1 2]\n",
      "[1 0 1 2 1 1 2]\n",
      "[2 0 2 2 2 1 2]\n",
      "[1 0 1 1 2 2 1]\n",
      "[2 0 2 2 1 1 1]\n",
      "[1 0 2 1 1 1 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "38 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [19], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2, 3]], 'time': [14.79], 'moves': [209], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [7], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0]], 'time': [193.53], 'moves': [190], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [24], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3]], 'time': [111.35], 'moves': [195], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0]], 'time': [196.68], 'moves': [196], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0]], 'time': [196.59], 'moves': [193], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [21], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2]], 'time': [68.46], 'moves': [198], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[0], [0, 3, 3, 3, 0, 0]], 'time': [41.23], 'moves': [124], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_4   vs   agent_MCTS_MatanTsipory_\n",
      "-----------------------------------\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 1 : [3,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 2 : [3,4,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  3 < agent_MCTS_MatanTsipory_ . (1.0) --- 3 : [3,4,3,3,4,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  4 < agent_MCTS_MatanTsipory_ . (1.0) --- 4 : [3,4,3,3,4,3,3,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 5 : [3,4,3,3,4,3,3,4,4,6] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 6 : [3,4,3,3,4,3,3,4,4,6,6,0] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 7 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 8 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 0  0 < agent_MCTS_MatanTsipory_ . (1.0) --- 9 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 10 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 11 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 12 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2,2,2] \n",
      "(0.3) . agent_TheFast_mini_Max_4 > 4  6 < agent_MCTS_MatanTsipory_ . (1.0) --- 13 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2,2,2,4,6] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 2  2 < agent_MCTS_MatanTsipory_ . (1.0) --- 14 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2,2,2,4,6,2,2] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 1  1 < agent_MCTS_MatanTsipory_ . (1.0) --- 15 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2,2,2,4,6,2,2,1,1] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 5  5 < agent_MCTS_MatanTsipory_ . (1.0) --- 16 : [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2,2,2,4,6,2,2,1,1,5,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_MCTS_MatanTsipory_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_4 [0]   2.agent_MCTS_MatanTsipory_ [3]   1.Time = 5.82   2.Time = 16.0   q.moves: 16   1.speed = 0.4   2.speed = 1.0   \n",
      "\n",
      "Game: [3,4,3,3,4,3,3,4,4,6,6,0,6,6,4,0,0,0,3,6,2,2,2,2,4,6,2,2,1,1,5,5]   \n",
      "\n",
      "Board: [0,0,2,1,1,0,2,0,0,1,1,1,0,2,2,0,2,2,1,0,2,1,0,1,2,2,0,1,2,2,2,1,1,2,1,2,1,1,1,2,1,2] \n",
      "\n",
      "[0 0 2 1 1 0 2]\n",
      "[0 0 1 1 1 0 2]\n",
      "[2 0 2 2 1 0 2]\n",
      "[1 0 1 2 2 0 1]\n",
      "[2 2 2 1 1 2 1]\n",
      "[2 1 1 1 2 1 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "39 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [19], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2, 3]], 'time': [14.79], 'moves': [209], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [10], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0, 3]], 'time': [209.53], 'moves': [206], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [24], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3]], 'time': [111.35], 'moves': [195], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0]], 'time': [196.68], 'moves': [196], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0]], 'time': [196.59], 'moves': [193], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [21], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2]], 'time': [68.46], 'moves': [198], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[0, 0], [0, 3, 3, 3, 0, 0]], 'time': [47.05], 'moves': [140], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_4   vs   MCTS_wAdaptive_Playouts_\n",
      "-----------------------------------\n",
      "time_limit 0.35\n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.72 stone_count 1 playouts 225 ***\n",
      "time_limit 0.35 my duration 0.35063982009887695 x 3\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 3  3 < MCTS_wAdaptive_Playouts_ . (0.4) --- 1 : [3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.7903225806451613 stone_count 3 playouts 496 ***\n",
      "time_limit 0.7 my duration 0.700629711151123 x 3\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 2 : [3,3,3,3] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 0.9595070422535211 stone_count 5 playouts 568 ***\n",
      "time_limit 0.7 my duration 0.7010045051574707 x 5\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 3 : [3,3,3,3,4,5] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.0002460479736328125 x 2\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  2 < MCTS_wAdaptive_Playouts_ . (0.0) --- 4 : [3,3,3,3,4,5,1,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.0248756218905473 stone_count 9 playouts 603 ***\n",
      "time_limit 0.7 my duration 0.7010014057159424 x 2\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 5 : [3,3,3,3,4,5,1,2,2,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.8628640776699028 stone_count 11 playouts 824 ***\n",
      "time_limit 0.7 my duration 0.7014939785003662 x 2\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 6 : [3,3,3,3,4,5,1,2,2,2,1,2] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9530892448512587 stone_count 13 playouts 874 ***\n",
      "time_limit 0.7 my duration 0.7012288570404053 x 1\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 7 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1] \n",
      "[0. 0. 0. 0. 0. 0. 0.] root_rewards 1.9112662013958126 stone_count 15 playouts 1003 ***\n",
      "time_limit 0.7 my duration 0.7008354663848877 x 3\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  3 < MCTS_wAdaptive_Playouts_ . (0.7) --- 8 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3] \n",
      "[0. 0. 0. 0. 0. 0.] root_rewards 1.9971489665003563 stone_count 17 playouts 1403 ***\n",
      "time_limit 0.7 my duration 0.7007107734680176 x 4\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  4 < MCTS_wAdaptive_Playouts_ . (0.7) --- 9 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.997823721436344 stone_count 19 playouts 1838 ***\n",
      "time_limit 0.7 my duration 0.700568675994873 x 6\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 10 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6] \n",
      "[0. 0. 0. 0. 0.] root_rewards 1.9994264410668197 stone_count 21 playouts 3487 ***\n",
      "time_limit 0.7 my duration 0.7006406784057617 x 2\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 5  2 < MCTS_wAdaptive_Playouts_ . (0.7) --- 11 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2] \n",
      "[0. 0. 0. 0.] root_rewards 1.9978753541076488 stone_count 23 playouts 2824 ***\n",
      "time_limit 0.7 my duration 0.700523853302002 x 5\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  5 < MCTS_wAdaptive_Playouts_ . (0.7) --- 12 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5] \n",
      "[0. 0. 0. 0.] root_rewards 2.0 stone_count 25 playouts 3772 ***\n",
      "time_limit 0.7 my duration 0.7007367610931396 x 1\n",
      "(0.2) . agent_TheFast_mini_Max_4 > 5  1 < MCTS_wAdaptive_Playouts_ . (0.7) --- 13 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 27 playouts 3045 ***\n",
      "time_limit 0.7 my duration 0.7009515762329102 x 0\n",
      "(0.1) . agent_TheFast_mini_Max_4 > 1  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 14 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0] \n",
      "forced move\n",
      "time_limit 0.7 my duration 0.000141143798828125 x 0\n",
      "(0.1) . agent_TheFast_mini_Max_4 > 0  0 < MCTS_wAdaptive_Playouts_ . (0.0) --- 15 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0,0,0] \n",
      "[0. 0. 0.] root_rewards 2.0 stone_count 31 playouts 5915 ***\n",
      "time_limit 0.7 my duration 0.7005345821380615 x 0\n",
      "(0.1) . agent_TheFast_mini_Max_4 > 5  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 16 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0,0,0,5,0] \n",
      "[0. 0.] root_rewards 2.0 stone_count 33 playouts 6455 ***\n",
      "time_limit 0.7 my duration 0.7006392478942871 x 6\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 5  6 < MCTS_wAdaptive_Playouts_ . (0.7) --- 17 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0,0,0,5,0,5,6] \n",
      "[0. 0.] root_rewards 2.0 stone_count 35 playouts 24879 ***\n",
      "time_limit 0.7 my duration 0.7006487846374512 x 0\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 6  0 < MCTS_wAdaptive_Playouts_ . (0.7) --- 18 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0,0,0,5,0,5,6,6,0] \n",
      "win\n",
      "forced move\n",
      "time_limit 0.7 my duration 0.00013494491577148438 x 0\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 4  0 < MCTS_wAdaptive_Playouts_ . (0.0) --- 19 : [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0,0,0,5,0,5,6,6,0,4,0] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MCTS_wAdaptive_Playouts_      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_4 [0]   2.MCTS_wAdaptive_Playouts_ [3]   1.Time = 5.79   2.Time = 10.86   q.moves: 19   1.speed = 0.3   2.speed = 0.6   \n",
      "\n",
      "Game: [3,3,3,3,4,5,1,2,2,2,1,2,2,1,1,3,3,4,6,6,5,2,6,5,5,1,1,0,0,0,5,0,5,6,6,0,4,0]   \n",
      "\n",
      "Board: [2,1,2,1,0,1,0,2,2,1,2,0,1,1,2,1,2,2,0,1,2,2,2,2,1,1,2,1,1,1,1,2,2,1,2,2,1,2,1,1,2,1] \n",
      "\n",
      "[2 1 2 1 0 1 0]\n",
      "[2 2 1 2 0 1 1]\n",
      "[2 1 2 2 0 1 2]\n",
      "[2 2 2 1 1 2 1]\n",
      "[1 1 1 2 2 1 2]\n",
      "[2 1 2 1 1 2 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "40 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [19], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2, 3]], 'time': [14.79], 'moves': [209], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [10], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0, 3]], 'time': [209.53], 'moves': [206], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [27], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3, 3]], 'time': [122.21], 'moves': [214], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0]], 'time': [196.68], 'moves': [196], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0]], 'time': [196.59], 'moves': [193], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [21], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2]], 'time': [68.46], 'moves': [198], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [9], 'games': [[0, 0, 0], [0, 3, 3, 3, 0, 0]], 'time': [52.84], 'moves': [159], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_4   vs   MontyCarlo_JamesMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloNode: p2 action = 3 after 1670 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 3  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloNode: p2 action = 4 after 1681 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 2 : [3,3,3,4] \n",
      "MontyCarloNode: p2 action = 4 after 1814 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 3 : [3,3,3,4,3,4] \n",
      "MontyCarloNode: p2 action = 3 after 1969 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  3 < MontyCarlo_JamesMcGuigan . (1.0) --- 4 : [3,3,3,4,3,4,4,3] \n",
      "MontyCarloNode: p2 action = 1 after 2186 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 5 : [3,3,3,4,3,4,4,3,1,1] \n",
      "MontyCarloNode: p2 action = 2 after 2279 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 0  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 6 : [3,3,3,4,3,4,4,3,1,1,0,2] \n",
      "MontyCarloNode: p2 action = 4 after 1967 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  4 < MontyCarlo_JamesMcGuigan . (1.0) --- 7 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4] \n",
      "MontyCarloNode: p2 action = 2 after 1892 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  2 < MontyCarlo_JamesMcGuigan . (1.0) --- 8 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2] \n",
      "MontyCarloNode: p2 action = 1 after 1831 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 9 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1] \n",
      "MontyCarloNode: p2 action = 1 after 1820 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  1 < MontyCarlo_JamesMcGuigan . (1.0) --- 10 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1] \n",
      "MontyCarloNode: p2 action = 6 after 1821 simulations in 1.001s\n",
      "(1.3) . agent_TheFast_mini_Max_4 > 3  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 11 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6] \n",
      "MontyCarloNode: p2 action = 6 after 1802 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 6  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 12 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6] \n",
      "MontyCarloNode: p2 action = 5 after 1974 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 13 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6,4,5] \n",
      "MontyCarloNode: p2 action = 5 after 2100 simulations in 1.000s\n",
      "(0.2) . agent_TheFast_mini_Max_4 > 5  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 14 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6,4,5,5,5] \n",
      "MontyCarloNode: p2 action = 6 after 2411 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 5  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 15 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6,4,5,5,5,5,6] \n",
      "MontyCarloNode: p2 action = 5 after 2953 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 0  5 < MontyCarlo_JamesMcGuigan . (1.0) --- 16 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6,4,5,5,5,5,6,0,5] \n",
      "MontyCarloNode: p2 action = 6 after 3346 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 5  6 < MontyCarlo_JamesMcGuigan . (1.0) --- 17 : [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6,4,5,5,5,5,6,0,5,5,6] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   1 1  1   1    1     1      agent_TheFast_mini_Max_4      1     1    1   1  1 1\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_4 [3]   2.MontyCarlo_JamesMcGuigan [0]   1.Time = 7.01   2.Time = 17.01   q.moves: 18   1.speed = 0.4   2.speed = 0.9   \n",
      "\n",
      "Game: [3,3,3,4,3,4,4,3,1,1,0,2,2,4,1,2,1,1,4,1,3,6,6,6,4,5,5,5,5,6,0,5,5,6,6]   \n",
      "\n",
      "Board: [0,2,0,1,1,1,1,0,2,0,2,1,2,2,0,1,0,1,2,1,2,0,1,2,1,1,2,2,1,2,1,2,2,1,1,1,1,2,1,2,2,2] \n",
      "\n",
      "[0 2 0 1 1 1 1]\n",
      "[0 2 0 2 1 2 2]\n",
      "[0 1 0 1 2 1 2]\n",
      "[0 1 2 1 1 2 2]\n",
      "[1 2 1 2 2 1 1]\n",
      "[1 1 2 1 2 2 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "41 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [19], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2, 3]], 'time': [14.79], 'moves': [209], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [10], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0, 3]], 'time': [209.53], 'moves': [206], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [27], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3, 3]], 'time': [122.21], 'moves': [214], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0, 0]], 'time': [213.69], 'moves': [213], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [11], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0]], 'time': [196.59], 'moves': [193], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [21], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2]], 'time': [68.46], 'moves': [198], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [12], 'games': [[0, 0, 0, 3], [0, 3, 3, 3, 0, 0]], 'time': [59.85], 'moves': [177], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_4   vs   MontyCarlo_BtS_JMcGuigan\n",
      "-----------------------------------\n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2583 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 1 : [3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2203 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 2 : [3,3,3,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 2244 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 3 : [3,3,3,3,2,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 2012 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 4 : [3,3,3,3,2,4,1,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 1 after 2120 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  1 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 5 : [3,3,3,3,2,4,1,0,1,1] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 2051 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 6 : [3,3,3,3,2,4,1,0,1,1,1,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 1796 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 7 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 1703 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 8 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2] \n",
      "MontyCarloBitsquaresNode: p2 action = 3 after 1702 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  3 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 9 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3] \n",
      "MontyCarloBitsquaresNode: p2 action = 4 after 1743 simulations in 1.001s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  4 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 10 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 1750 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 11 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 1813 simulations in 1.000s\n",
      "(0.5) . agent_TheFast_mini_Max_4 > 0  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 12 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0,0,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 0 after 2079 simulations in 1.001s\n",
      "(0.1) . agent_TheFast_mini_Max_4 > 0  0 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 13 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0,0,6,0,0] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 3211 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 0  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 14 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0,0,6,0,0,0,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 6 after 3228 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 6  6 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 15 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0,0,6,0,0,0,6,6,6] \n",
      "MontyCarloBitsquaresNode: p2 action = 2 after 3828 simulations in 1.000s\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 2  2 < MontyCarlo_BtS_JMcGuigan . (1.0) --- 16 : [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0,0,6,0,0,0,6,6,6,2,2] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      MontyCarlo_BtS_JMcGuigan      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_4 [0]   2.MontyCarlo_BtS_JMcGuigan [3]   1.Time = 5.64   2.Time = 16.01   q.moves: 16   1.speed = 0.4   2.speed = 1.0   \n",
      "\n",
      "Game: [3,3,3,3,2,4,1,0,1,1,1,3,4,4,4,2,1,3,1,4,4,0,0,6,0,0,0,6,6,6,2,2]   \n",
      "\n",
      "Board: [1,1,0,2,1,0,0,2,1,0,2,2,0,0,1,1,2,2,1,0,2,1,2,1,1,2,0,1,2,1,2,2,1,0,2,2,1,1,1,2,0,2] \n",
      "\n",
      "[1 1 0 2 1 0 0]\n",
      "[2 1 0 2 2 0 0]\n",
      "[1 1 2 2 1 0 2]\n",
      "[1 2 1 1 2 0 1]\n",
      "[2 1 2 2 1 0 2]\n",
      "[2 1 1 1 2 0 2]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "42 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [19], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2, 3]], 'time': [14.79], 'moves': [209], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [10], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0, 3]], 'time': [209.53], 'moves': [206], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [27], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3, 3]], 'time': [122.21], 'moves': [214], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0, 0]], 'time': [213.69], 'moves': [213], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [14], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0, 3]], 'time': [212.6], 'moves': [209], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [21], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2]], 'time': [68.46], 'moves': [198], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [12], 'games': [[0, 0, 0, 3, 0], [0, 3, 3, 3, 0, 0]], 'time': [65.49], 'moves': [193], 'speed': [0.3]})\n",
      "\n",
      "\n",
      "--------------------------\n",
      "agent_TheFast_mini_Max_4   vs   agent_TheFast_mini_Max_2\n",
      "-----------------------------------\n",
      "(0.0) . agent_TheFast_mini_Max_4 > 3  3 < agent_TheFast_mini_Max_2 . (0.0) --- 1 : [3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  3 < agent_TheFast_mini_Max_2 . (0.5) --- 2 : [3,3,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  4 < agent_TheFast_mini_Max_2 . (0.5) --- 3 : [3,3,3,3,2,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  0 < agent_TheFast_mini_Max_2 . (0.5) --- 4 : [3,3,3,3,2,4,1,0] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  4 < agent_TheFast_mini_Max_2 . (0.5) --- 5 : [3,3,3,3,2,4,1,0,1,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  4 < agent_TheFast_mini_Max_2 . (0.5) --- 6 : [3,3,3,3,2,4,1,0,1,4,4,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  2 < agent_TheFast_mini_Max_2 . (0.5) --- 7 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 2  4 < agent_TheFast_mini_Max_2 . (0.5) --- 8 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 3  3 < agent_TheFast_mini_Max_2 . (0.5) --- 9 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 4  1 < agent_TheFast_mini_Max_2 . (0.5) --- 10 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 1  1 < agent_TheFast_mini_Max_2 . (0.5) --- 11 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 0  1 < agent_TheFast_mini_Max_2 . (0.5) --- 12 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1] \n",
      "(0.5) . agent_TheFast_mini_Max_4 > 0  0 < agent_TheFast_mini_Max_2 . (0.5) --- 13 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1,0,0] \n",
      "(0.3) . agent_TheFast_mini_Max_4 > 2  2 < agent_TheFast_mini_Max_2 . (0.1) --- 14 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1,0,0,2,2] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 6  6 < agent_TheFast_mini_Max_2 . (0.0) --- 15 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1,0,0,2,2,6,6] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 6  6 < agent_TheFast_mini_Max_2 . (0.0) --- 16 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1,0,0,2,2,6,6,6,6] \n",
      "(0.0) . agent_TheFast_mini_Max_4 > 5  5 < agent_TheFast_mini_Max_2 . (0.0) --- 17 : [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1,0,0,2,2,6,6,6,6,5,5] \n",
      "\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "   2 2  2   2    2     2      agent_TheFast_mini_Max_2      2     2    2   2  2 2\n",
      "win in n win in n win in n win in n win in n win in n win in n win in n\n",
      "\n",
      "1.agent_TheFast_mini_Max_4 [0]   2.agent_TheFast_mini_Max_2 [3]   1.Time = 6.35   2.Time = 6.15   q.moves: 17   1.speed = 0.4   2.speed = 0.4   \n",
      "\n",
      "Game: [3,3,3,3,2,4,1,0,1,4,4,4,2,2,2,4,3,3,4,1,1,1,0,1,0,0,2,2,6,6,6,6,5,5]   \n",
      "\n",
      "Board: [0,2,2,2,1,0,0,0,2,1,1,2,0,0,2,1,1,2,2,0,2,1,2,2,1,1,0,1,1,1,1,2,2,2,2,2,1,1,1,2,1,1] \n",
      "\n",
      "[0 2 2 2 1 0 0]\n",
      "[0 2 1 1 2 0 0]\n",
      "[2 1 1 2 2 0 2]\n",
      "[1 2 2 1 1 0 1]\n",
      "[1 1 1 2 2 2 2]\n",
      "[2 1 1 1 2 1 1]\n",
      "\n",
      "=====================================================================================================\n",
      "\n",
      "\n",
      "43 :\n",
      "\n",
      "('agent__treebased__dott__', {'T': [19], 'games': [[3, 0, 0, 3, 0, 3], [3, 0, 0, 2, 2, 3]], 'time': [14.79], 'moves': [209], 'speed': [0.1]})\n",
      "('agent_MCTS_MatanTsipory_', {'T': [10], 'games': [[0, 0, 1, 3, 0, 0], [0, 0, 0, 3, 0, 3]], 'time': [209.53], 'moves': [206], 'speed': [1.0]})\n",
      "('MCTS_wAdaptive_Playouts_', {'T': [27], 'games': [[3, 3, 0, 0, 3, 0], [3, 3, 3, 3, 3, 3]], 'time': [122.21], 'moves': [214], 'speed': [0.6]})\n",
      "('MontyCarlo_JamesMcGuigan', {'T': [20], 'games': [[3, 3, 0, 3, 3, 0], [3, 2, 3, 0, 0, 0]], 'time': [213.69], 'moves': [213], 'speed': [1.0]})\n",
      "('MontyCarlo_BtS_JMcGuigan', {'T': [14], 'games': [[1, 0, 0, 3, 1, 3], [0, 0, 3, 0, 0, 3]], 'time': [212.6], 'moves': [209], 'speed': [1.0]})\n",
      "('agent_TheFast_mini_Max_2', {'T': [24], 'games': [[1, 3, 0, 3, 3, 3], [3, 3, 0, 0, 2, 3]], 'time': [74.61], 'moves': [215], 'speed': [0.3]})\n",
      "('agent_TheFast_mini_Max_4', {'T': [12], 'games': [[0, 0, 0, 3, 0, 0], [0, 3, 3, 3, 0, 0]], 'time': [71.84], 'moves': [210], 'speed': [0.3]})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debugger_c4([\n",
    "    agent__treebased__dott__,\n",
    "    agent_MCTS_MatanTsipory_,\n",
    "    MCTS_wAdaptive_Playouts_,\n",
    "    MontyCarlo_JamesMcGuigan,\n",
    "    MontyCarlo_BtS_JMcGuigan,\n",
    "    agent_TheFast_mini_Max_2,\n",
    "    agent_TheFast_mini_Max_4,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc00e43",
   "metadata": {
    "papermill": {
     "duration": 0.104739,
     "end_time": "2024-02-03T19:41:25.895205",
     "exception": false,
     "start_time": "2024-02-03T19:41:25.790466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## bot battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540bf483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:41:26.113341Z",
     "iopub.status.busy": "2024-02-03T19:41:26.112938Z",
     "iopub.status.idle": "2024-02-03T19:41:26.118274Z",
     "shell.execute_reply": "2024-02-03T19:41:26.116945Z"
    },
    "papermill": {
     "duration": 0.120107,
     "end_time": "2024-02-03T19:41:26.120721",
     "exception": false,
     "start_time": "2024-02-03T19:41:26.000614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# round_robin(agents=[\n",
    "#     agent__treebased__dott__,\n",
    "#     agent_MCTS_MatanTsipory_,\n",
    "#     MCTS_wAdaptive_Playouts_,\n",
    "# ], n_games=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb54d6a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:41:26.333763Z",
     "iopub.status.busy": "2024-02-03T19:41:26.333148Z",
     "iopub.status.idle": "2024-02-03T19:41:26.338843Z",
     "shell.execute_reply": "2024-02-03T19:41:26.337690Z"
    },
    "papermill": {
     "duration": 0.11691,
     "end_time": "2024-02-03T19:41:26.341288",
     "exception": false,
     "start_time": "2024-02-03T19:41:26.224378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# round_robin(agents=[\n",
    "#     agent__treebased__dott__,\n",
    "#     MontyCarlo_JamesMcGuigan,\n",
    "#     MontyCarlo_BtS_JMcGuigan,\n",
    "# ], n_games=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "167a1d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T19:41:26.557542Z",
     "iopub.status.busy": "2024-02-03T19:41:26.556579Z",
     "iopub.status.idle": "2024-02-03T19:41:26.564567Z",
     "shell.execute_reply": "2024-02-03T19:41:26.562207Z"
    },
    "papermill": {
     "duration": 0.119445,
     "end_time": "2024-02-03T19:41:26.567645",
     "exception": false,
     "start_time": "2024-02-03T19:41:26.448200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# round_robin(agents=[\n",
    "#     agent__treebased__dott__,\n",
    "#     agent_TheFast_mini_Max_2,\n",
    "#     agent_TheFast_mini_Max_4,\n",
    "# ], n_games=2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 54014,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 995.582737,
   "end_time": "2024-02-03T19:41:29.305615",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-03T19:24:53.722878",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
