{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd04e341",
   "metadata": {
    "papermill": {
     "duration": 0.003253,
     "end_time": "2024-02-19T06:09:28.392419",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.389166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "Reinforcement learning (RL) is an interdisciplinary area of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) and [optimal control](https://en.wikipedia.org/wiki/Optimal_control) concerned with how an intelligent agent ought to take [actions](https://en.wikipedia.org/wiki/Action_selection) in a dynamic environment in order to maximize the [cumulative reward](https://en.wikipedia.org/wiki/Reward-based_selection). RL is one of [three basic machine learning paradigms](https://en.wikipedia.org/wiki/Machine_learning#Approaches), alongside [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) and [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning).\n",
    "\n",
    "RL differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the long term reward, whose feedback might be incomplete or delayed.[[1]](https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-kaelbling-1)\n",
    "\n",
    "The environment is typically stated in the form of a [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process) (MDP), because many RL algorithms for this context use [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) techniques.[[2]](https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-2) The main difference between the classical dynamic programming methods and RL algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDP where exact methods become infeasible.[[3]](https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2abd1",
   "metadata": {
    "papermill": {
     "duration": 0.002362,
     "end_time": "2024-02-19T06:09:28.397569",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.395207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "RL is a subset of machine learning that allows an AI-driven system (sometimes referred to as an agent) to learn through trial and error using feedback from its actions.\n",
    "\n",
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abd720",
   "metadata": {
    "papermill": {
     "duration": 0.002321,
     "end_time": "2024-02-19T06:09:28.402418",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.400097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## How does RL work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346c6e8",
   "metadata": {
    "papermill": {
     "duration": 0.002418,
     "end_time": "2024-02-19T06:09:28.407350",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.404932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Describing fully how RL works in one article is no easy task. To get a good grounding in the subject, the book Reinforcement Learning: An Introduction by Andrew Barto and Richard S. Sutton is a good resource.\n",
    "\n",
    "To build an optimal policy, the RL agent is faced with the dilemma of whether to explore new states at the same time as maximising its reward. This is known as Exploration versus Exploitation trade-off. The aim is not to look for immediate reward, but to optimise for maximum cumulative reward over the length of training. Time is also important – the reward agent doesn’t just rely on the current state, but on the entire history of states. Policy iteration is an algorithm that helps find the optimal policy for given states and actions.\n",
    "\n",
    "The environment in a RL algorithm is commonly expressed as a MDP, and almost all RL problems are formalised using MDPs. [SARSA](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action) is an algorithm for learning a [Markov decision](https://en.wikipedia.org/wiki/Markov_decision_process). It’s a slight variation of the popular [Q-learning](https://en.wikipedia.org/wiki/Q-learning) [algorithm](https://habr.com/ru/articles/443240/). [SARSA and Q-learning](https://dilithjay.com/blog/q-learning-and-sarsa/) are the two most typically used RL algorithms.\n",
    "\n",
    "Some other frequently used methods include Actor-Critic, which is a [Temporal Difference](https://en.wikipedia.org/wiki/Temporal_difference_learning) (TD) version of [Policy Gradient](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/) methods. It’s similar to an algorithm called REINFORCE with baseline. The [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation) is one of the central elements of many RL algorithms. It usually refers to the [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) equation associated with [discrete-time optimisation problems](https://faculty.washington.edu/moishe/hanoiex/counting/discreteOptimization.pdf).\n",
    "\n",
    "The [Asynchrous Advantage Actor Critic](https://medium.com/serpdotai/asynchronous-advantage-actor-critic-f56af759fed0) (A3C) algorithm is one of the newest developed in the field of [Deep RL](https://en.wikipedia.org/wiki/Deep_reinforcement_learning) algorithms. Unlike other popular deep RL algorithms like [Deep Q-Learning](https://www.mlq.ai/deep-reinforcement-learning-q-learning/) ([DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)) which uses a single agent and a single environment, A3C uses multiple agents with their own network parameters and a copy of the environment. The agents interact with their environments asynchronously, learning with every interaction, contributing to the total knowledge of a global network. The global network also allows agents to have more diversified training data. This mimics the real-life environment in which humans gain knowledge from the experiences of others, allowing the entire global network to benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c7f27",
   "metadata": {
    "papermill": {
     "duration": 0.002268,
     "end_time": "2024-02-19T06:09:28.412229",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.409961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8e0cd",
   "metadata": {
    "papermill": {
     "duration": 0.002314,
     "end_time": "2024-02-19T06:09:28.417010",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.414696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Does RL need data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd50699",
   "metadata": {
    "papermill": {
     "duration": 0.002401,
     "end_time": "2024-02-19T06:09:28.421910",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.419509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "In RL, the data is accumulated from machine learning systems that use a trial-and-error method. Data is not part of the input that you would find in supervised or unsupervised machine learning.\n",
    "\n",
    "[TD learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) is a class of [model-free RL](https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)) methods that learn via bootstrapping from a current estimate of the value function.\n",
    "\n",
    "The name Temporal Difference comes from the fact that it uses changes – or differences – in predictions over successive time steps to push the learning process forward. \n",
    "\n",
    "At any given time step, the prediction is updated, bringing it closer to the prediction of the same quantity at the next time step. Often used to predict the total amount of future reward, TD learning is a combination of [Monte Carlo](https://www.investopedia.com/terms/m/montecarlosimulation.asp) ideas and [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming).\n",
    "\n",
    "However, whereas learning takes place at the end of any Monte Carlo method, learning takes place after each interaction in TD.\n",
    "\n",
    "TD Learning is one of the central ideas in RL, as it lies between Monte Carlo methods and Dynamic Programming in a spectrum of different Reinforcement Learning methods.\n",
    "\n",
    "Monte Carlo methods represent a broad class of algorithms that rely on repeated random sampling in order to gain numerical results that point to probability. Monte Carlo  methods can be used to calculate the probability of:\n",
    "\n",
    "* an opponent’s move in a game like chess\n",
    "* a weather event occurring in the future\n",
    "* the chances of a car crash under specific conditions\n",
    "\n",
    "Named after the casino in the city of the same name in Monaco, Monte Carlo methods first arose within the field of particle physics and contributed to the development of the first computers. Monte Carlo [*simulations*](https://www.investopedia.com/terms/m/montecarlosimulation.asp) allow people to account for risk in quantitative analysis and decision making. It’s a technique used in a wide variety of fields including finance, project management, manufacturing, engineering, research and development, insurance, transportation, and the environment.\n",
    "\n",
    "In machine learning or robotics, Monte Carlo methods provide a basis for estimating the likelihood of outcomes in artificial intelligence problems using simulation. The bootstrap method is built upon Monte Carlo methods, and is a resampling technique for estimating a quantity, such as the accuracy of a model on a limited dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7a3a6",
   "metadata": {
    "papermill": {
     "duration": 0.002311,
     "end_time": "2024-02-19T06:09:28.426757",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.424446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Applications of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5cc78",
   "metadata": {
    "papermill": {
     "duration": 0.002298,
     "end_time": "2024-02-19T06:09:28.431540",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.429242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[TD Gammon](https://en.wikipedia.org/wiki/TD-Gammon) is a computer backgammon program that was developed in 1992 by [Gerald Tesauro at IBM’s Thomas J. Watson Research Center](https://en.wikipedia.org/wiki/Thomas_J._Watson_Research_Center). It used RL and, specifically, a non-linear form of the TD algorithm to train computers to play backgammon to the level of grandmasters. It was an instrumental step in teaching machines how to play complex games.\n",
    "\n",
    "RL is the method used by [DeepMind](https://ru.wikipedia.org/wiki/Google_DeepMind) to initiate artificial intelligence in how to play complex games like chess, Go, and shogi (Japanese chess). It was used in the building of [AlphaGo](https://ru.wikipedia.org/wiki/AlphaGo), the first computer program to beat a professional human Go player. From this grew the deep neural network agent [AlphaZero]([AlphaZero](https://ru.wikipedia.org/wiki/AlphaZero)), which taught itself to play chess well enough to beat the chess engine [Stockfish](https://ru.wikipedia.org/wiki/Stockfish) in just four hours.\n",
    "\n",
    "AlphaZero has only two parts: a neural network, and an algorithm called [Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). Compare this with the brute force computing power of Deep Blue, which, even in 1997 when it beat world chess champion Garry Kasparov, allowed the consideration of 200 million possible chess positions per second. The representations of deep neural networks like those used by AlphaZero, however, are opaque, so our understanding of their decisions is restricted. The paper Acquisition of Chess Knowledge in AlphaZero explores this conundrum.\n",
    "\n",
    "[Deep RL](https://en.wikipedia.org/wiki/Deep_reinforcement_learning) is being proposed in the use of unmanned spacecraft to navigate new environments, whether it’s Mars or the Moon. MarsExplorer is an [OpenAI Gym](https://habr.com/ru/articles/458596/) compatible environment that has been developed by a group of Greek scientists. There are four deep reinforcement learning algorithms that the team has trained on the MarsExplorer environment, [A3C](https://medium.com/sciforce/reinforcement-learning-and-asynchronous-actor-critic-agent-a3c-algorithm-explained-f0f3146a14ab), Ranbow, [PPO](https://en.wikipedia.org/wiki/Proximal_Policy_Optimization), and SAC, with [PPO](https://en.wikipedia.org/wiki/Proximal_Policy_Optimization) performing best. MarsExplorer is the first open-AI compatible reinforcement learning framework that is optimised for the exploration of unknown terrain.\n",
    "\n",
    "RL is also used in self-driving cars, in trading and finance to predict stock prices, and in healthcare for diagnosing rare diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21442a",
   "metadata": {
    "papermill": {
     "duration": 0.002297,
     "end_time": "2024-02-19T06:09:28.436334",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.434037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Kaggle links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8add8a",
   "metadata": {
    "papermill": {
     "duration": 0.00232,
     "end_time": "2024-02-19T06:09:28.441326",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.439006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you'd like to dig more deeply into RL, Kaggle [recommend](https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning) checking out the following (free!) resources:\n",
    "- David Silver's videos - [here](https://www.youtube.com/watch?v=2pWv7GOvuf0)\n",
    "- Richard Sutton's and Andrew Barto's textbook - [here](http://www.incompleteideas.net/book/RLbook2018.pdf)\n",
    "- Denny Britz's GitHub repository - [here](https://github.com/dennybritz/reinforcement-learning)\n",
    "- The Deep RL Bootcamp - [here](https://sites.google.com/corp/view/deep-rl-bootcamp/lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce352241",
   "metadata": {
    "papermill": {
     "duration": 0.002303,
     "end_time": "2024-02-19T06:09:28.446071",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.443768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab86a2",
   "metadata": {
    "papermill": {
     "duration": 0.002301,
     "end_time": "2024-02-19T06:09:28.450809",
     "exception": false,
     "start_time": "2024-02-19T06:09:28.448508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[UC](https://rail.eecs.berkeley.edu/deeprlcourse/resources/) [Berkeley](https://rail.eecs.berkeley.edu/deeprlcourse/)\n",
    "\n",
    "[Stanford](https://web.stanford.edu/class/cs234/) [on](https://online.stanford.edu/courses/cs234-reinforcement-learning)[line](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)\n",
    "\n",
    "[University of York](https://online.york.ac.uk/what-is-reinforcement-learning/#:~:text=Reinforcement%20learning%20RL%20is%20a,using%20feedback%20from%20its%20actions.)\n",
    "\n",
    "[5 Policy Gradient Methods](https://www.kaggle.com/code/just4jcgeorge/5-policy-gradient-methods)\n",
    "\n",
    "[Temporal-Difference Learning](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html)\n",
    "\n",
    "[DAVID SILVER - UCL Course on RL](https://www.davidsilver.uk/teaching/)\n",
    "\n",
    "[Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "[Summary of all the top discussion posts](https://www.kaggle.com/competitions/connectx/discussion/332427)\n",
    "\n",
    "[Interactive Deep Q-Learning in Google Colab](https://www.kaggle.com/competitions/connectx/discussion/129145)\n",
    "\n",
    "[Useful courses, books, ... for Reinforcement Learning](https://www.kaggle.com/competitions/connectx/discussion/124421)\n",
    "\n",
    "[Deep Reinforcement Learning: Guide to Deep Q-Learning](https://www.mlq.ai/deep-reinforcement-learning-q-learning/)\n",
    "\n",
    "[Q-Learning and SARSA in RL – Similarities and Differences Explained](https://dilithjay.com/blog/q-learning-and-sarsa/)\n",
    "\n",
    "Video:\n",
    "\n",
    "\n",
    "[STANFORD 2019: Reinforcement Learning](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLMCTGXj1E09BYwKncK-_QXzfD7eC7bw7T)\n",
    "\n",
    "[UC BERKLEY 2021: Deep Reinforcement Learning](https://www.youtube.com/playlist?list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH)\n",
    "\n",
    "[An introduction to Reinforcement Learning (Arxiv Insights)](https://www.youtube.com/watch?v=JgvyzIkgxF0)\n",
    "\n",
    "[Reinforcement Learning, by  the  Book  (Mutual Information)](https://www.youtube.com/watch?v=NFo9v_yKQXA&list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr)\n",
    "\n",
    "[Reinforcement Learning from scratch (Graphics in 5 Minutes)](https://www.youtube.com/watch?v=vXtfdGphr3c)\n",
    "\n",
    "[Actor Critic Methods Are Easy With Keras  (Machine Learning with Phil\n",
    ")](https://www.youtube.com/watch?v=2vJtbAha3To&list=PL-9x0_FO_lgkwi8ES611NsV-cjYaH_nLa&index=6)\n",
    "\n",
    "[Q-Learning: Model Free Reinforcement Learning and Temporal Difference Learning](https://www.youtube.com/watch?v=0iqz4tcKN58&t=16s)\n",
    "\n",
    "[A friendly introduction to deep reinforcement learning, Q-networks and policy gradients](https://www.youtube.com/watch?v=SgC6AZss478)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 54014,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.272837,
   "end_time": "2024-02-19T06:09:28.670120",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-19T06:09:26.397283",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
