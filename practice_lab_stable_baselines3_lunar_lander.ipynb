{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable-Baselines3 practice lab Lunar Lander\n",
    "\n",
    "Code snippets are  \n",
    "- from the official documentation of [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) and  \n",
    "- from [Reinforcement Learning in Python with Stable Baselines 3](https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 08:59:34.109534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736150374.122749    2862 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736150374.126709    2862 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-06 08:59:34.140318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gymnasium as gym\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare output folders of model snapshots and logs for tensorboard\n",
    "models_dir, logs_dir = \"models/lunarlander/PPO\", \"logs/lunarlander\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.15966795e-04,  1.41220427e+00,  1.17322486e-02,  5.70709035e-02,\n",
       "        -1.27612017e-04, -2.65754969e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the lunar lander environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stonapse/Projects/kaggle.studies.rl/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create the model of choice\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, save the snapshots and create / save the logs\n",
    "TIMESTEPS = 1e4 # timesteps for each training episode\n",
    "episodes = 50 # number of training episodes\n",
    "for episode in range(episodes):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*(episode + 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, while the model trains, we can view the results over time by \n",
    "- opening a new terminal\n",
    "- invoking `tensorboard --logdir=logs/lunarlander`\n",
    "- start the browser with http://localhost:6006/ (look for the terminal output for the actual port on your system)\n",
    "\n",
    "To watch progress in near real-time, switch on the TensorBoard setting \"Reload data\" (look for the gear icon in the TensorBoards main menu bar).\n",
    "\n",
    "To see to which extend the GPU is used, type `nvidia-smi` into a new terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation of training process using Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Tensorboard shows that \n",
    "- PPO needed 100k timesteps to start producing positive rewards\n",
    "- PPO at first developed a strategy to approach landing slowly whilst increasing reward. This approach ended at around 140k timesteps with a length of ~770 timesteps per landing approach.\n",
    "- From that on PPO started to increase the efficiency of the landing by decreasing the timesteps per landing approach down to 320 at around ~320k timesteps.\n",
    "- Beyond the 500k timestep training budget, the model started to show a positive trend both in shorter lengths and bigger rewards. Thus further training might have been beneficial.\n",
    "\n",
    "![Tensorboard](images/Tensorboard_lunarlander_PPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and evaluate a formerly saved snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lines of code are repeats of code from previous sections of the notebook. I did this intentionally so that this section can be executed without executing any cell from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stonapse/Projects/kaggle.studies.rl/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean reward is 109.28687439999999 with a standard deviation of 134.06204899112822\n"
     ]
    }
   ],
   "source": [
    "# imports and path settings\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "models_dir, logs_dir = \"models/lunarlander/PPO\", \"logs/lunarlander\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment, load and evaluate model @ a saved snapshot\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "# to evaluate another snapshot explore the folder models_dir for saved snapshots\n",
    "model = PPO.load(f\"{models_dir}/500000.0\", env=env)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"The mean reward is {mean_reward} with a standard deviation of {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enjoy the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lines of code are repeats of code from previous sections of the notebook. I did this intentionally so that this section can be executed without executing any cell from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 15:00:02.027979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736431202.041124     456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736431202.045053     456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-09 15:00:02.058448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports and path settings\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "models_dir, logs_dir = \"models/lunarlander/PPO\", \"logs/lunarlander\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stonapse/Projects/kaggle.studies.rl/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/stonapse/Projects/kaggle.studies.rl/.venv/lib/python3.12/site-packages/cv2/qt/plugins\"\n",
      "Qt: Session management error: Could not open network socket\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward in episode 1 was [-43.39543]\n",
      "Total reward in episode 2 was [235.18979]\n",
      "Total reward in episode 3 was [-1.2233429]\n",
      "Total reward in episode 4 was [232.4602]\n",
      "Total reward in episode 5 was [-69.20262]\n"
     ]
    }
   ],
   "source": [
    "# recreate env and load a formerly saved snapshot and render animations from it\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "# to enjoy another snapshot explore the folder models_dir for saved snapshots\n",
    "model = PPO.load(f\"{models_dir}/500000.0\", env=env)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "\n",
    "episodes = 5\n",
    "total_reward_episode = 0\n",
    "\n",
    "try:\n",
    "    for episode in range(episodes):\n",
    "        # VecEnv resets automatically but one could optionally reset it here\n",
    "        # obs = vec_env.reset()\n",
    "        done = False    \n",
    "        total_reward_episode = 0\n",
    "        while not done:\n",
    "            action, _state = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = vec_env.step(action)\n",
    "            vec_env.render('human')\n",
    "            total_reward_episode += reward\n",
    "\n",
    "        print(f\"Total reward in episode {episode + 1} was {total_reward_episode}\")\n",
    "finally:\n",
    "    vec_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
