{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17592,"databundleVersionId":899221,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nIn the tutorial, you learned a bit about reinforcement learning and used the `stable-baselines3` package to train an agent to beat a random opponent.  In this exercise, you will check your understanding and tinker with the code to deepen your intuition.","metadata":{}},{"cell_type":"code","source":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex4 import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:18:52.449034Z","iopub.execute_input":"2025-01-04T15:18:52.449431Z","iopub.status.idle":"2025-01-04T15:18:52.860778Z","shell.execute_reply.started":"2025-01-04T15:18:52.449400Z","shell.execute_reply":"2025-01-04T15:18:52.859702Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### 1) Set the architecture\n\nIn the tutorial, you learned one way to design a neural network that can select moves in Connect Four.  The neural network had an output layer with seven nodes: one for each column in the game board.\n\nSay now you wanted to create a neural network that can play chess.  How many nodes should you put in the output layer?\n\n- Option A: 2 nodes (number of game players)\n- Option B: 16 nodes (number of game pieces that each player starts with)\n- Option C: 4672 nodes (number of possible moves)\n- Option D: 64 nodes (number of squares on the game board)\n\nUse your answer to set the value of the `best_option` variable below.  Your answer should be one of `'A'`, `'B'`, `'C'`, or `'D'`.","metadata":{}},{"cell_type":"code","source":"# Fill in the blank\nbest_option = 'C'\n\n# Check your answer\nq_1.check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:18:59.987072Z","iopub.execute_input":"2025-01-04T15:18:59.987559Z","iopub.status.idle":"2025-01-04T15:18:59.997717Z","shell.execute_reply.started":"2025-01-04T15:18:59.987530Z","shell.execute_reply":"2025-01-04T15:18:59.996356Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_PickBestOption\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: If we use a similar network as in the tutorial, the network should output a probability for each possible move.","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> If we use a similar network as in the tutorial, the network should output a probability for each possible move."},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Lines below will give you solution code\nq_1.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:08:13.725315Z","iopub.status.idle":"2025-01-04T15:08:13.725680Z","shell.execute_reply":"2025-01-04T15:08:13.725546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2) Decide reward\n\nIn the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four.  Consider now training an agent to win at the game [Minesweeper](https://bit.ly/2T5xEY8).  The goal of the game is to clear the board without detonating any bombs.\n\nTo play this game in Google Search, click on the **[Play]** button at [this link](https://www.google.com/search?q=minesweeper).  \n\n<center>\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/WzoEfKY.png\" width=50%><br/>\n</center>\n\nWith each move, one of the following is true:\n- The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move).  Let's assume this ends the game, and the agent loses.\n- The agent clears a square that did not contain a hidden mine.  The agent wins the game, because all squares without mines are revealed.\n- The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.\n- The agent detonates a mine and loses the game.\n\nHow might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game?\n\nAfter you have decided on your answer, run the code cell below to get credit for completing this question.","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\n# invalid move > -10\n# clears square + win > 1000\n# clears square + continue > 1/288\n# mine > -1000\nq_2.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:19:19.630459Z","iopub.execute_input":"2025-01-04T15:19:19.630953Z","iopub.status.idle":"2025-01-04T15:19:19.640110Z","shell.execute_reply.started":"2025-01-04T15:19:19.630921Z","shell.execute_reply":"2025-01-04T15:19:19.638915Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"2_DecideReward\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n- If agent wins the game in that move, it gets a reward of `+1`.\n- Else if the agent selects an invalid move, it gets a reward of `-10`.\n- Else if it detonates a mine, it gets a reward of `-1`.\n- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n\nTo check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n- If agent wins the game in that move, it gets a reward of `+1`.\n- Else if the agent selects an invalid move, it gets a reward of `-10`.\n- Else if it detonates a mine, it gets a reward of `-1`.\n- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n\nTo check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### 3) (Optional) Amend the code\n\nIn this next part of the exercise, you will amend the code from the tutorial to experiment with creating your own agents!  There are a lot of hyperparameters involved with specifying a reinforcement learning agent, and you'll have a chance to amend them, to see how performance is affected.\n\nFirst, we'll need to make sure that your Kaggle Notebook is set up to run the code.  Begin by looking at the \"Settings\" menu to the right of your notebook.  Your menu will look like one of the following:\n\n<center>\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/kR1az0y.png\" width=100%><br/>\n</center>\n\nIf your \"Internet\" setting appears as a \"Requires phone verification\" link, click on this link.  This will bring you to a new window; then, follow the instructions to verify your account.  After following this step, your \"Internet\" setting will appear \"Off\", as in the example to the right.\n\nOnce your \"Internet\" setting appears as \"Off\", click to turn it on.  You'll see a pop-up window that you'll need to \"Accept\" in order to complete the process and have the setting switched to \"On\".  Once the Internet is turned \"On\", you're ready to proceed!\n\n<center>\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/gOVh6Aa.png\" width=100%><br/>\n</center>\n\nBegin by running the code cell below. ","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport gym\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nclass ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n    \n# Create ConnectFour environment \nenv = ConnectFourGym(agent2=\"random\")\n\nimport torch as th\nimport torch.nn as nn\n\n!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# Neural network for predicting action values\nclass CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:26:19.728484Z","iopub.execute_input":"2025-01-04T15:26:19.728692Z","iopub.status.idle":"2025-01-04T15:26:40.595479Z","shell.execute_reply.started":"2025-01-04T15:26:19.728672Z","shell.execute_reply":"2025-01-04T15:26:40.594720Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import resource_stream, resource_exists\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3121: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.1.0)\nRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.0)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\nRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Next, run the code cell below to train an agent with PPO.  This code is identical to the code from the tutorial.","metadata":{}},{"cell_type":"code","source":"# Initialize agent\nmodel = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n\n# Train agent\nmodel.learn(total_timesteps=50000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:26:40.599294Z","iopub.execute_input":"2025-01-04T15:26:40.599525Z","iopub.status.idle":"2025-01-04T15:33:37.122716Z","shell.execute_reply.started":"2025-01-04T15:26:40.599506Z","shell.execute_reply":"2025-01-04T15:33:37.121907Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<stable_baselines3.ppo.ppo.PPO at 0x7ab7b10b7430>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# agent with n-step lookahead and alpha-beta pruning\ndef my_nstep_lookahead_ab_pruning_agent(obs, config):\n    # Your code here: Amend the agent!\n\n    import random\n    import numpy as np\n    \n    # Gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n\n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n\n    # Helper function for get_heuristic: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n        return score\n\n    # Uses minimax with alpha-beta pruning to calculate value of dropping piece in selected column\n    def score_move(grid, col, mark, config, nsteps, alpha=-float('inf'), beta=float('inf')):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax(next_grid, nsteps-1, False, mark, config, alpha, beta)\n        return score\n    \n    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n    \n    # Helper function for minimax: checks if game has ended\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n    \n    # Minimax implementation with alpha-beta pruning\n    def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n                alpha = max(alpha, value)\n                if alpha >= beta:\n                    break\n            return value\n        else:\n            value = np.inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n                beta = min(beta, value)\n                if alpha >= beta:\n                    break\n            return value\n\n    # agent driver\n    # How deep to make the game tree: higher values take longer to run!\n    # ConncectX comes with a max time per player move and also for all moves!\n    N_STEPS = 5\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:41:10.663926Z","iopub.execute_input":"2025-01-04T15:41:10.664253Z","iopub.status.idle":"2025-01-04T15:41:10.681541Z","shell.execute_reply.started":"2025-01-04T15:41:10.664228Z","shell.execute_reply":"2025-01-04T15:41:10.680710Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def agent1(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:33:57.883942Z","iopub.execute_input":"2025-01-04T15:33:57.884274Z","iopub.status.idle":"2025-01-04T15:33:57.889052Z","shell.execute_reply.started":"2025-01-04T15:33:57.884242Z","shell.execute_reply":"2025-01-04T15:33:57.888143Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agent1, my_nstep_lookahead_ab_pruning_agent])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:42:08.741994Z","iopub.execute_input":"2025-01-04T15:42:08.742354Z","iopub.status.idle":"2025-01-04T15:42:25.696493Z","shell.execute_reply.started":"2025-01-04T15:42:08.742323Z","shell.execute_reply":"2025-01-04T15:42:25.695611Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<iframe srcdoc=\"<!--\n  Copyright 2020 Kaggle Inc\n\n  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n<!DOCTYPE html>\n<html lang=&quot;en&quot;>\n  <head>\n    <title>Kaggle Simulation Player</title>\n    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n    <link\n      rel=&quot;stylesheet&quot;\n      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n      crossorigin=&quot;anonymous&quot;\n    />\n    <style type=&quot;text/css&quot;>\n      html,\n      body {\n        height: 100%;\n        font-family: sans-serif;\n        margin: 0px;\n      }\n      canvas {\n        /* image-rendering: -moz-crisp-edges;\n        image-rendering: -webkit-crisp-edges;\n        image-rendering: pixelated;\n        image-rendering: crisp-edges; */\n      }\n    </style>\n    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n    <script src=&quot;https://unpkg.com/chess.js@0.12.0/chess.js&quot;></script>\n    <script>\n      // Polyfill for Styled Components\n      window.React = {\n        ...preact,\n        createElement: preact.h,\n        PropTypes: { func: {} },\n      };\n    </script>\n    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n  </head>\n  <body>\n    <script>\n      \nwindow.kaggle = {\n  &quot;debug&quot;: false,\n  &quot;playing&quot;: true,\n  &quot;step&quot;: 0,\n  &quot;controls&quot;: true,\n  &quot;environment&quot;: {\n    &quot;id&quot;: &quot;74fbf85a-cab2-11ef-841f-0242ac130202&quot;,\n    &quot;name&quot;: &quot;connectx&quot;,\n    &quot;title&quot;: &quot;ConnectX&quot;,\n    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n    &quot;version&quot;: &quot;1.0.1&quot;,\n    &quot;configuration&quot;: {\n      &quot;episodeSteps&quot;: 1000,\n      &quot;actTimeout&quot;: 2,\n      &quot;runTimeout&quot;: 1200,\n      &quot;columns&quot;: 7,\n      &quot;rows&quot;: 6,\n      &quot;inarow&quot;: 4,\n      &quot;agentTimeout&quot;: 60,\n      &quot;timeout&quot;: 2\n    },\n    &quot;specification&quot;: {\n      &quot;action&quot;: {\n        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n        &quot;type&quot;: &quot;integer&quot;,\n        &quot;minimum&quot;: 0,\n        &quot;default&quot;: 0\n      },\n      &quot;agents&quot;: [\n        2\n      ],\n      &quot;configuration&quot;: {\n        &quot;episodeSteps&quot;: {\n          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;minimum&quot;: 1,\n          &quot;default&quot;: 1000\n        },\n        &quot;actTimeout&quot;: {\n          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 2\n        },\n        &quot;runTimeout&quot;: {\n          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 1200\n        },\n        &quot;columns&quot;: {\n          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 7,\n          &quot;minimum&quot;: 1\n        },\n        &quot;rows&quot;: {\n          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 6,\n          &quot;minimum&quot;: 1\n        },\n        &quot;inarow&quot;: {\n          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 4,\n          &quot;minimum&quot;: 1\n        },\n        &quot;agentTimeout&quot;: {\n          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 60\n        },\n        &quot;timeout&quot;: {\n          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 2,\n          &quot;minimum&quot;: 0\n        }\n      },\n      &quot;info&quot;: {},\n      &quot;observation&quot;: {\n        &quot;remainingOverageTime&quot;: {\n          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n          &quot;shared&quot;: false,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 60\n        },\n        &quot;step&quot;: {\n          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;shared&quot;: true,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 0\n        },\n        &quot;board&quot;: {\n          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n          &quot;type&quot;: &quot;array&quot;,\n          &quot;shared&quot;: true,\n          &quot;default&quot;: []\n        },\n        &quot;mark&quot;: {\n          &quot;defaults&quot;: [\n            1,\n            2\n          ],\n          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n          &quot;enum&quot;: [\n            1,\n            2\n          ]\n        }\n      },\n      &quot;reward&quot;: {\n        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n        &quot;enum&quot;: [\n          -1,\n          0,\n          1\n        ],\n        &quot;default&quot;: 0,\n        &quot;type&quot;: [\n          &quot;number&quot;,\n          &quot;null&quot;\n        ]\n      }\n    },\n    &quot;steps&quot;: [\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 0,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 1,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 2,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 5,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 2,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 3,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              1,\n              0,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 4,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              0,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 1,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 5,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              0,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 6,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              0,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 1,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 58.618231,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 4,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 7,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 58.618231,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 8,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              1,\n              0,\n              2,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 5,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 56.801323000000004,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 2,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 9,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              0,\n              2,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 56.801323000000004,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 10,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              0,\n              2,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 1,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 54.624179000000005,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 4,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 11,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 54.624179000000005,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: -1,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 12,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0,\n              0,\n              2,\n              1,\n              1,\n              1,\n              2,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;DONE&quot;\n        },\n        {\n          &quot;action&quot;: 1,\n          &quot;reward&quot;: 1,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 54.362336000000006,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;DONE&quot;\n        }\n      ]\n    ],\n    &quot;rewards&quot;: [\n      -1,\n      1\n    ],\n    &quot;statuses&quot;: [\n      &quot;DONE&quot;,\n      &quot;DONE&quot;\n    ],\n    &quot;schema_version&quot;: 1,\n    &quot;info&quot;: {}\n  },\n  &quot;logs&quot;: [\n    [],\n    [],\n    [\n      {\n        &quot;duration&quot;: 0.002008,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 1.747166,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.002153,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 1.472216,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.001822,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 3.381769,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.001887,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 3.816908,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.001831,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 4.177144,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.001867,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 2.261843,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ]\n  ],\n  &quot;mode&quot;: &quot;ipython&quot;\n};\n\n\nwindow.kaggle.renderer = // Copyright 2020 Kaggle Inc\n//\n// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nfunction renderer({\n  act,\n  agents,\n  environment,\n  frame,\n  height = 400,\n  interactive,\n  isInteractive,\n  parent,\n  step,\n  update,\n  width = 400,\n}) {\n  // Configuration.\n  const { rows, columns, inarow } = environment.configuration;\n\n  // Common Dimensions.\n  const unit = 8;\n  const minCanvasSize = Math.min(height, width);\n  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n  const cellSize = Math.min(\n    (width - minOffset * 2) / columns,\n    (height - minOffset * 2) / rows\n  );\n  const cellInset = 0.8;\n  const pieceScale = cellSize / 100;\n  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n\n  // Canvas Setup.\n  let canvas = parent.querySelector(&quot;canvas&quot;);\n  if (!canvas) {\n    canvas = document.createElement(&quot;canvas&quot;);\n    parent.appendChild(canvas);\n\n    if (interactive) {\n      canvas.addEventListener(&quot;click&quot;, evt => {\n        if (!isInteractive()) return;\n        const rect = evt.target.getBoundingClientRect();\n        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n        if (col >= 0 && col < columns) act(col);\n      });\n    }\n  }\n  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n\n  // Character Paths (based on 100x100 tiles).\n  const kPath = new Path2D(\n    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n  );\n  const goose1Path = new Path2D(\n    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n  );\n  const goose2Path = new Path2D(\n    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n  );\n  const goose3Path = new Path2D(\n    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n  );\n\n  // Canvas setup and reset.\n  let c = canvas.getContext(&quot;2d&quot;);\n  canvas.width = width;\n  canvas.height = height;\n  c.fillStyle = &quot;#000B2A&quot;;\n  c.fillRect(0, 0, canvas.width, canvas.height);\n\n  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n\n  const getColor = (mark, opacity = 1) => {\n    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n    return &quot;#fff&quot;;\n  };\n\n  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n    const [row, col] = getRowCol(cell);\n    c.arc(\n      xOffset + xFrame * (col * cellSize + cellSize / 2),\n      yOffset + yFrame * (row * cellSize + cellSize / 2),\n      (cellInset * cellSize) / 2 - radiusOffset,\n      2 * Math.PI,\n      false\n    );\n  };\n\n  // Render the pieces.\n  const board = environment.steps[step][0].observation.board;\n\n  const drawPiece = mark => {\n    // Base Styles.\n    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n    c.fillStyle = getColor(mark, opacity);\n    c.strokeStyle = getColor(mark);\n    c.shadowColor = getColor(mark);\n    c.shadowBlur = 8 / cellInset;\n    c.lineWidth = 1 / cellInset;\n\n    // Outer circle.\n    c.save();\n    c.beginPath();\n    c.arc(50, 50, 50, 2 * Math.PI, false);\n    c.closePath();\n    c.lineWidth *= 4;\n    c.stroke();\n    c.fill();\n    c.restore();\n\n    // Inner circle.\n    c.beginPath();\n    c.arc(50, 50, 40, 2 * Math.PI, false);\n    c.closePath();\n    c.stroke();\n\n    // Kaggle &quot;K&quot;.\n    if (mark === 1) {\n      const scale = 0.54;\n      c.save();\n      c.translate(23, 23);\n      c.scale(scale, scale);\n      c.lineWidth /= scale;\n      c.shadowBlur /= scale;\n      c.stroke(kPath);\n      c.restore();\n    }\n\n    // Kaggle &quot;Goose&quot;.\n    if (mark === 2) {\n      const scale = 0.6;\n      c.save();\n      c.translate(24, 28);\n      c.scale(scale, scale);\n      c.lineWidth /= scale;\n      c.shadowBlur /= scale;\n      c.stroke(goose1Path);\n      c.stroke(goose2Path);\n      c.stroke(goose3Path);\n      c.beginPath();\n      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n      c.closePath();\n      c.fill();\n      c.restore();\n    }\n  };\n\n  for (let i = 0; i < board.length; i++) {\n    const [row, col] = getRowCol(i);\n    if (board[i] === 0) continue;\n    // Easing In.\n    let yFrame = Math.min(\n      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n      1\n    );\n\n    if (\n      step > 1 &&\n      environment.steps[step - 1][0].observation.board[i] === board[i]\n    ) {\n      yFrame = 1;\n    }\n\n    c.save();\n    c.translate(\n      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n      yOffset +\n        yFrame * (cellSize * row) +\n        (cellSize - cellSize * cellInset) / 2\n    );\n    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n    drawPiece(board[i]);\n    c.restore();\n  }\n\n  // Background Gradient.\n  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n  const bgStyle = c.createRadialGradient(\n    xOffset + (cellSize * columns) / 2,\n    yOffset + (cellSize * rows) / 2,\n    0,\n    xOffset + (cellSize * columns) / 2,\n    yOffset + (cellSize * rows) / 2,\n    bgRadius\n  );\n  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n\n  // Render the board overlay.\n  c.beginPath();\n  c.rect(0, 0, canvas.width, canvas.height);\n  c.closePath();\n  c.shadowBlur = 0;\n  for (let i = 0; i < board.length; i++) {\n    drawCellCircle(i);\n    c.closePath();\n  }\n  c.fillStyle = bgStyle;\n  c.fill(&quot;evenodd&quot;);\n\n  // Render the board overlay cell outlines.\n  for (let i = 0; i < board.length; i++) {\n    c.beginPath();\n    drawCellCircle(i);\n    c.strokeStyle = &quot;#0361B2&quot;;\n    c.lineWidth = 1;\n    c.stroke();\n    c.closePath();\n  }\n\n  const drawLine = (fromCell, toCell) => {\n    if (frame < 0.5) return;\n    const lineFrame = (frame - 0.5) / 0.5;\n    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n    const x2 =\n      x1 +\n      lineFrame *\n        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n    const y1 =\n      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n    const y2 =\n      y1 +\n      lineFrame *\n        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n    c.beginPath();\n    c.lineCap = &quot;round&quot;;\n    c.lineWidth = 4;\n    c.strokeStyle = getColor(board[fromCell]);\n    c.shadowBlur = 8;\n    c.shadowColor = getColor(board[fromCell]);\n    c.moveTo(x1, y1);\n    c.lineTo(x2, y2);\n    c.stroke();\n  };\n\n  // Generate a graph of the board.\n  const getCell = (cell, rowOffset, columnOffset) => {\n    const row = Math.floor(cell / columns) + rowOffset;\n    const col = (cell % columns) + columnOffset;\n    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n    return col + row * columns;\n  };\n  const makeNode = cell => {\n    const node = { cell, directions: [], value: board[cell] };\n    for (let r = -1; r <= 1; r++) {\n      for (let c = -1; c <= 1; c++) {\n        if (r === 0 && c === 0) continue;\n        node.directions.push(getCell(cell, r, c));\n      }\n    }\n    return node;\n  };\n  const graph = board.map((_, i) => makeNode(i));\n\n  // Check for any wins!\n  const getSequence = (node, direction) => {\n    const sequence = [node.cell];\n    while (sequence.length < inarow) {\n      const next = graph[node.directions[direction]];\n      if (!next || node.value !== next.value || next.value === 0) return;\n      node = next;\n      sequence.push(node.cell);\n    }\n    return sequence;\n  };\n\n  // Check all nodes.\n  for (let i = 0; i < board.length; i++) {\n    // Check all directions (not the most efficient).\n    for (let d = 0; d < 8; d++) {\n      const seq = getSequence(graph[i], d);\n      if (seq) {\n        drawLine(seq[0], seq[inarow - 1]);\n        i = board.length;\n        break;\n      }\n    }\n  }\n\n  // Upgrade the legend.\n  if (agents.length && (!agents[0].color || !agents[0].image)) {\n    const getPieceImage = mark => {\n      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n      parent.appendChild(pieceCanvas);\n      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n      pieceCanvas.width = 100;\n      pieceCanvas.height = 100;\n      c = pieceCanvas.getContext(&quot;2d&quot;);\n      c.translate(10, 10);\n      c.scale(0.8, 0.8);\n      drawPiece(mark);\n      const dataUrl = pieceCanvas.toDataURL();\n      parent.removeChild(pieceCanvas);\n      return dataUrl;\n    };\n\n    agents.forEach(agent => {\n      agent.color = getColor(agent.index + 1);\n      agent.image = getPieceImage(agent.index + 1);\n    });\n    update({ agents });\n  }\n};\n\n\n    \n    </script>\n    <script>\n      const h = htm.bind(preact.h);\n      const { useContext, useEffect, useRef, useState } = preactHooks;\n      const styled = window.styled.default;\n\n      const Context = preact.createContext({});\n\n      const Loading = styled.div`\n        animation: rotate360 1.1s infinite linear;\n        border: 8px solid rgba(255, 255, 255, 0.2);\n        border-left-color: #0cb1ed;\n        border-radius: 50%;\n        height: 40px;\n        position: relative;\n        transform: translateZ(0);\n        width: 40px;\n\n        @keyframes rotate360 {\n          0% {\n            transform: rotate(0deg);\n          }\n          100% {\n            transform: rotate(360deg);\n          }\n        }\n      `;\n\n      const Logo = styled(\n        (props) => h`\n        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n            </g>\n          </svg>\n        </a>\n      `\n      )`\n        display: inline-flex;\n      `;\n\n      const Header = styled((props) => {\n        const { environment } = useContext(Context);\n\n        return h`<div className=${props.className} >\n          <${Logo} />\n          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n          ${environment.title}\n        </div>`;\n      })`\n        align-items: center;\n        border-bottom: 4px solid #212121;\n        box-sizing: border-box;\n        color: #fff;\n        display: flex;\n        flex: 0 0 36px;\n        font-size: 14px;\n        justify-content: space-between;\n        padding: 0 8px;\n        width: 100%;\n      `;\n\n      const Renderer = styled((props) => {\n        const context = useContext(Context);\n        const { animate, debug, playing, renderer, speed } = context;\n        const ref = preact.createRef();\n\n        useEffect(async () => {\n          if (!ref.current) return;\n\n          const renderFrame = async (start, step, lastFrame) => {\n            if (step !== context.step) return;\n            if (lastFrame === 1) {\n              if (!animate) return;\n              start = Date.now();\n            }\n            const frame =\n              playing || animate\n                ? Math.min((Date.now() - start) / speed, 1)\n                : 1;\n            try {\n              if (debug) console.time(&quot;render&quot;);\n              await renderer({\n                ...context,\n                frame,\n                height: ref.current.clientHeight,\n                hooks: preactHooks,\n                parent: ref.current,\n                preact,\n                styled,\n                width: ref.current.clientWidth,\n              });\n            } catch (error) {\n              if (debug) console.error(error);\n              console.log({ ...context, frame, error });\n            } finally {\n              if (debug) console.timeEnd(&quot;render&quot;);\n            }\n            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n          };\n\n          await renderFrame(Date.now(), context.step);\n        }, [ref.current, context.step, context.renderer]);\n\n        return h`<div className=${props.className} ref=${ref} />`;\n      })`\n        align-items: center;\n        box-sizing: border-box;\n        display: flex;\n        height: 100%;\n        left: 0;\n        justify-content: center;\n        position: absolute;\n        top: 0;\n        width: 100%;\n      `;\n\n      const Processing = styled((props) => {\n        const { processing } = useContext(Context);\n        const text = processing === true ? &quot;Processing...&quot; : processing;\n        return h`<div className=${props.className}>${text}</div>`;\n      })`\n        bottom: 0;\n        color: #fff;\n        font-size: 12px;\n        left: 0;\n        line-height: 24px;\n        position: absolute;\n        text-align: center;\n        width: 100%;\n      `;\n\n      const Viewer = styled((props) => {\n        const { processing } = useContext(Context);\n        return h`<div className=${props.className}>\n          <${Renderer} />\n          ${processing && h`<${Processing} />`}\n        </div>`;\n      })`\n        background-color: #000b2a;\n        background-image: radial-gradient(\n          circle closest-side,\n          #000b49,\n          #000b2a\n        );\n        display: flex;\n        flex: 1;\n        overflow: hidden;\n        position: relative;\n        width: 100%;\n      `;\n\n      // Partitions the elements of arr into subarrays of max length num.\n      const groupIntoSets = (arr, num) => {\n        const sets = [];\n        arr.forEach(a => {\n          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n            sets.push([]);\n          }\n          sets[sets.length - 1].push(a);\n        });\n        return sets;\n      }\n\n      // Expects `width` input prop to set proper max-width for agent name span.\n      const Legend = styled((props) => {\n        const { agents, legend } = useContext(Context);\n\n        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n\n        return h`<div className=${props.className}>\n          ${agentPairs.map(agentList =>\n            h`<ul>\n                ${agentList.map(a =>\n                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n                      ${a.image && h`<img src=${a.image} />`}\n                      <span>${a.name}</span>\n                    </li>`\n                )}\n              </ul>`)}\n        </div>`;\n      })`\n        background-color: #000b2a;\n        font-family: sans-serif;\n        font-size: 14px;\n        height: 48px;\n        width: 100%;\n\n        ul {\n          align-items: center;\n          display: flex;\n          flex-direction: row;\n          justify-content: center;\n        }\n\n        li {\n          align-items: center;\n          display: inline-flex;\n          transition: color 1s;\n        }\n\n        span {\n          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n          overflow: hidden;\n          text-overflow: ellipsis;\n          white-space: nowrap;\n        }\n\n        img {\n          height: 24px;\n          margin-left: 4px;\n          margin-right: 4px;\n          width: 24px;\n        }\n      `;\n\n      const StepInput = styled.input.attrs({\n        type: &quot;range&quot;,\n      })`\n        appearance: none;\n        background: rgba(255, 255, 255, 0.15);\n        border-radius: 2px;\n        display: block;\n        flex: 1;\n        height: 4px;\n        opacity: 0.8;\n        outline: none;\n        transition: opacity 0.2s;\n        width: 100%;\n\n        &:hover {\n          opacity: 1;\n        }\n\n        &::-webkit-slider-thumb {\n          appearance: none;\n          background: #1ebeff;\n          border-radius: 100%;\n          cursor: pointer;\n          height: 12px;\n          margin: 0;\n          position: relative;\n          width: 12px;\n\n          &::after {\n            content: &quot;&quot;;\n            position: absolute;\n            top: 0px;\n            left: 0px;\n            width: 200px;\n            height: 8px;\n            background: green;\n          }\n        }\n      `;\n\n      const PlayButton = styled.button`\n        align-items: center;\n        background: none;\n        border: none;\n        color: white;\n        cursor: pointer;\n        display: flex;\n        flex: 0 0 56px;\n        font-size: 20px;\n        height: 40px;\n        justify-content: center;\n        opacity: 0.8;\n        outline: none;\n        transition: opacity 0.2s;\n\n        &:hover {\n          opacity: 1;\n        }\n      `;\n\n      const StepCount = styled.span`\n        align-items: center;\n        color: white;\n        display: flex;\n        font-size: 14px;\n        justify-content: center;\n        opacity: 0.8;\n        padding: 0 16px;\n        pointer-events: none;\n      `;\n\n      const Controls = styled((props) => {\n        const { environment, pause, play, playing, setStep, step } = useContext(\n          Context\n        );\n        const value = step + 1;\n        const onClick = () => (playing ? pause() : play());\n        const onInput = (e) => {\n          pause();\n          setStep(parseInt(e.target.value) - 1);\n        };\n\n        return h`\n          <div className=${props.className}>\n            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n          playing\n            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n        }</svg><//>\n            <${StepInput} min=&quot;1&quot; max=${\n          environment.steps.length\n        } value=&quot;${value}&quot; onInput=${onInput} />\n            <${StepCount}>${value} / ${environment.steps.length}<//>\n          </div>\n        `;\n      })`\n        align-items: center;\n        border-top: 4px solid #212121;\n        display: flex;\n        flex: 0 0 44px;\n        width: 100%;\n      `;\n\n      const Info = styled((props) => {\n        const {\n          environment,\n          playing,\n          step,\n          speed,\n          animate,\n          header,\n          controls,\n          settings,\n        } = useContext(Context);\n\n        return h`\n          <div className=${props.className}>\n            info:\n            step(${step}),\n            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n            speed(${speed}),\n            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n          </div>`;\n      })`\n        color: #888;\n        font-family: monospace;\n        font-size: 12px;\n      `;\n\n      const Settings = styled((props) => {\n        const { environment, pause, play, playing, setStep, step } = useContext(\n          Context\n        );\n\n        return h`\n          <div className=${props.className}>\n            <${Info} />\n          </div>\n        `;\n      })`\n        background: #fff;\n        border-top: 4px solid #212121;\n        box-sizing: border-box;\n        padding: 20px;\n        width: 100%;\n\n        h1 {\n          font-size: 20px;\n        }\n      `;\n\n      const Player = styled((props) => {\n        const context = useContext(Context);\n        const { agents, controls, header, legend, loading, settings, width } = context;\n        return h`\n          <div className=${props.className}>\n            ${loading && h`<${Loading} />`}\n            ${!loading && header && h`<${Header} />`}\n            ${!loading && h`<${Viewer} />`}\n            ${!loading && legend && h`<${Legend} width=${width}/>`}\n            ${!loading && controls && h`<${Controls} />`}\n            ${!loading && settings && h`<${Settings} />`}\n          </div>`;\n      })`\n        align-items: center;\n        background: #212121;\n        border: 4px solid #212121;\n        box-sizing: border-box;\n        display: flex;\n        flex-direction: column;\n        height: 100%;\n        justify-content: center;\n        position: relative;\n        width: 100%;\n      `;\n\n      const App = () => {\n        const renderCountRef = useRef(0);\n        const [_, setRenderCount] = useState(0);\n\n        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n        const speeds = [\n          0,\n          3000,\n          1000,\n          500,\n          333, // Default\n          200,\n          100,\n          50,\n          25,\n          10,\n        ];\n\n        const contextRef = useRef({\n          animate: false,\n          agents: [],\n          controls: false,\n          debug: false,\n          environment: { steps: [], info: {} },\n          header: window.innerHeight >= 600,\n          height: window.innerHeight,\n          interactive: false,\n          legend: true,\n          loading: false,\n          playing: false,\n          processing: false,\n          renderer: () => &quot;DNE&quot;,\n          settings: false,\n          speed: speeds[4],\n          step: 0,\n          width: window.innerWidth,\n        });\n\n        // Context helpers.\n        const rerender = (contextRef.current.rerender = () =>\n          setRenderCount((renderCountRef.current += 1)));\n        const setStep = (contextRef.current.setStep = (newStep) => {\n          contextRef.current.step = newStep;\n          rerender();\n        });\n        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n          contextRef.current.playing = playing;\n          rerender();\n        });\n        const pause = (contextRef.current.pause = () => setPlaying(false));\n\n        const playNext = () => {\n          const context = contextRef.current;\n\n          if (\n            context.playing &&\n            context.step < context.environment.steps.length - 1\n          ) {\n            setStep(context.step + 1);\n            play(true);\n          } else {\n            pause();\n          }\n        };\n\n        const play = (contextRef.current.play = (continuing) => {\n          const context = contextRef.current;\n          if (context.playing && !continuing) return;\n          if (!context.playing) setPlaying(true);\n          if (\n            !continuing &&\n            context.step === context.environment.steps.length - 1\n          ) {\n            setStep(0);\n          }\n          setTimeout(playNext, context.speed);\n        });\n\n        const updateContext = (o) => {\n          const context = contextRef.current;\n          Object.assign(context, o, {\n            environment: { ...context.environment, ...(o.environment || {}) },\n          });\n          rerender();\n        };\n\n        // First time setup.\n        useEffect(() => {\n          // Timeout is used to ensure useEffect renders once.\n          setTimeout(() => {\n            // Initialize context with window.kaggle.\n            updateContext(window.kaggle || {});\n\n            if (window.kaggle.playing) {\n                play(true);\n            }\n\n            // Listen for messages received to update the context.\n            window.addEventListener(\n              &quot;message&quot;,\n              (event) => {\n                // Ensure the environment names match before updating.\n                try {\n                  if (\n                    event.data.environment.name ==\n                    contextRef.current.environment.name\n                  ) {\n                    updateContext(event.data);\n                  }\n                } catch {}\n              },\n              false\n            );\n            // Listen for keyboard commands.\n            window.addEventListener(\n              &quot;keydown&quot;,\n              (event) => {\n                const {\n                  interactive,\n                  isInteractive,\n                  playing,\n                  step,\n                  environment,\n                } = contextRef.current;\n                const key = event.keyCode;\n                const zero_key = 48\n                const nine_key = 57\n                if (\n                  interactive ||\n                  isInteractive() ||\n                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n                )\n                  return;\n\n                if (key === 32) {\n                  playing ? pause() : play();\n                } else if (key === 39) {\n                  contextRef.current.playing = false;\n                  if (step < environment.steps.length - 1) setStep(step + 1);\n                  rerender();\n                } else if (key === 37) {\n                  contextRef.current.playing = false;\n                  if (step > 0) setStep(step - 1);\n                  rerender();\n                } else if (key >= zero_key && key <= nine_key) {\n                  contextRef.current.speed = speeds[key - zero_key];\n                }\n                event.preventDefault();\n                return false;\n              },\n              false\n            );\n          }, 1);\n        }, []);\n\n        if (contextRef.current.debug) {\n          console.log(&quot;context&quot;, contextRef.current);\n        }\n\n        // Ability to update context.\n        contextRef.current.update = updateContext;\n\n        // Ability to communicate with ipython.\n        const execute = (contextRef.current.execute = (source) =>\n          new Promise((resolve, reject) => {\n            try {\n              window.parent.IPython.notebook.kernel.execute(source, {\n                iopub: {\n                  output: (resp) => {\n                    const type = resp.msg_type;\n                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n                  },\n                },\n              });\n            } catch (e) {\n              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n            }\n          }));\n\n        // Ability to return an action from an interactive session.\n        contextRef.current.act = (action) => {\n          const id = contextRef.current.environment.id;\n          updateContext({ processing: true });\n          execute(`\n            import json\n            from kaggle_environments import interactives\n            if &quot;${id}&quot; in interactives:\n                action = json.loads('${JSON.stringify(action)}')\n                env, trainer = interactives[&quot;${id}&quot;]\n                trainer.step(action)\n                print(json.dumps(env.steps))`)\n            .then((resp) => {\n              try {\n                updateContext({\n                  processing: false,\n                  environment: { steps: JSON.parse(resp) },\n                });\n                play();\n              } catch (e) {\n                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n                console.error(resp, e);\n              }\n            })\n            .catch((e) => console.error(e));\n        };\n\n        // Check if currently interactive.\n        contextRef.current.isInteractive = () => {\n          const context = contextRef.current;\n          const steps = context.environment.steps;\n          return (\n            context.interactive &&\n            !context.processing &&\n            context.step === steps.length - 1 &&\n            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n          );\n        };\n\n        return h`\n          <${Context.Provider} value=${contextRef.current}>\n            <${Player} />\n          <//>`;\n      };\n\n      preact.render(h`<${App} />`, document.body);\n    </script>\n  </body>\n</html>\n\" width=\"300\" height=\"300\" frameborder=\"0\"></iframe> "},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:34:44.865726Z","iopub.execute_input":"2025-01-04T15:34:44.866001Z","iopub.status.idle":"2025-01-04T15:34:44.871339Z","shell.execute_reply.started":"2025-01-04T15:34:44.865979Z","shell.execute_reply":"2025-01-04T15:34:44.870441Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"get_win_percentages(agent1=agent1, agent2=\"negamax\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:37:28.892944Z","iopub.execute_input":"2025-01-04T15:37:28.893263Z","iopub.status.idle":"2025-01-04T15:38:39.231269Z","shell.execute_reply.started":"2025-01-04T15:37:28.893240Z","shell.execute_reply":"2025-01-04T15:38:39.230589Z"}},"outputs":[{"name":"stdout","text":"Agent 1 Win Percentage: 0.06\nAgent 2 Win Percentage: 0.93\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"help(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T15:36:09.066423Z","iopub.execute_input":"2025-01-04T15:36:09.066720Z","iopub.status.idle":"2025-01-04T15:36:09.074850Z","shell.execute_reply.started":"2025-01-04T15:36:09.066699Z","shell.execute_reply":"2025-01-04T15:36:09.074148Z"}},"outputs":[{"name":"stdout","text":"Help on PPO in module stable_baselines3.ppo.ppo object:\n\nclass PPO(stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm)\n |  PPO(policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0003, n_steps: int = 2048, batch_size: int = 64, n_epochs: int = 10, gamma: float = 0.99, gae_lambda: float = 0.95, clip_range: Union[float, Callable[[float], float]] = 0.2, clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None, normalize_advantage: bool = True, ent_coef: float = 0.0, vf_coef: float = 0.5, max_grad_norm: float = 0.5, use_sde: bool = False, sde_sample_freq: int = -1, target_kl: Optional[float] = None, stats_window_size: int = 100, tensorboard_log: Optional[str] = None, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n |  \n |  Proximal Policy Optimization algorithm (PPO) (clip version)\n |  \n |  Paper: https://arxiv.org/abs/1707.06347\n |  Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n |  https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n |  Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n |  \n |  Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n |  \n |  :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n |  :param env: The environment to learn from (if registered in Gym, can be str)\n |  :param learning_rate: The learning rate, it can be a function\n |      of the current progress remaining (from 1 to 0)\n |  :param n_steps: The number of steps to run for each environment per update\n |      (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n |      NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n |      See https://github.com/pytorch/pytorch/issues/29372\n |  :param batch_size: Minibatch size\n |  :param n_epochs: Number of epoch when optimizing the surrogate loss\n |  :param gamma: Discount factor\n |  :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n |  :param clip_range: Clipping parameter, it can be a function of the current progress\n |      remaining (from 1 to 0).\n |  :param clip_range_vf: Clipping parameter for the value function,\n |      it can be a function of the current progress remaining (from 1 to 0).\n |      This is a parameter specific to the OpenAI implementation. If None is passed (default),\n |      no clipping will be done on the value function.\n |      IMPORTANT: this clipping depends on the reward scaling.\n |  :param normalize_advantage: Whether to normalize or not the advantage\n |  :param ent_coef: Entropy coefficient for the loss calculation\n |  :param vf_coef: Value function coefficient for the loss calculation\n |  :param max_grad_norm: The maximum value for the gradient clipping\n |  :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n |      instead of action noise exploration (default: False)\n |  :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n |      Default: -1 (only sample at the beginning of the rollout)\n |  :param target_kl: Limit the KL divergence between updates,\n |      because the clipping is not enough to prevent large update\n |      see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n |      By default, there is no limit on the kl div.\n |  :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n |      the reported success rate, mean episode length, and mean reward over\n |  :param tensorboard_log: the log location for tensorboard (if None, no logging)\n |  :param policy_kwargs: additional arguments to be passed to the policy on creation\n |  :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n |      debug messages\n |  :param seed: Seed for the pseudo random generators\n |  :param device: Device (cpu, cuda, ...) on which the code should be run.\n |      Setting it to auto, the code will be run on the GPU if possible.\n |  :param _init_setup_model: Whether or not to build the network at the creation of the instance\n |  \n |  Method resolution order:\n |      PPO\n |      stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm\n |      stable_baselines3.common.base_class.BaseAlgorithm\n |      abc.ABC\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0003, n_steps: int = 2048, batch_size: int = 64, n_epochs: int = 10, gamma: float = 0.99, gae_lambda: float = 0.95, clip_range: Union[float, Callable[[float], float]] = 0.2, clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None, normalize_advantage: bool = True, ent_coef: float = 0.0, vf_coef: float = 0.5, max_grad_norm: float = 0.5, use_sde: bool = False, sde_sample_freq: int = -1, target_kl: Optional[float] = None, stats_window_size: int = 100, tensorboard_log: Optional[str] = None, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  learn(self: ~SelfPPO, total_timesteps: int, callback: Union[NoneType, Callable, List[stable_baselines3.common.callbacks.BaseCallback], stable_baselines3.common.callbacks.BaseCallback] = None, log_interval: int = 1, tb_log_name: str = 'PPO', reset_num_timesteps: bool = True, progress_bar: bool = False) -> ~SelfPPO\n |      Return a trained model.\n |      \n |      :param total_timesteps: The total number of samples (env steps) to train on\n |      :param callback: callback(s) called at every step with state of the algorithm.\n |      :param log_interval: The number of episodes before logging.\n |      :param tb_log_name: the name of the run for TensorBoard logging\n |      :param reset_num_timesteps: whether or not to reset the current timestep number (used in logging)\n |      :param progress_bar: Display a progress bar using tqdm and rich.\n |      :return: the trained model\n |  \n |  train(self) -> None\n |      Update policy using the currently gathered rollout buffer.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'policy_aliases': typing.ClassVar[typing.Dict[str, ...\n |  \n |  policy_aliases = {'CnnPolicy': <class 'stable_baselines3.common.polici...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm:\n |  \n |  collect_rollouts(self, env: stable_baselines3.common.vec_env.base_vec_env.VecEnv, callback: stable_baselines3.common.callbacks.BaseCallback, rollout_buffer: stable_baselines3.common.buffers.RolloutBuffer, n_rollout_steps: int) -> bool\n |      Collect experiences using the current policy and fill a ``RolloutBuffer``.\n |      The term rollout here refers to the model-free notion and should not\n |      be used with the concept of rollout used in model-based RL or planning.\n |      \n |      :param env: The training environment\n |      :param callback: Callback that will be called at each step\n |          (and at the beginning and end of the rollout)\n |      :param rollout_buffer: Buffer to fill with rollouts\n |      :param n_rollout_steps: Number of experiences to collect per environment\n |      :return: True if function returned with at least `n_rollout_steps`\n |          collected, False if callback terminated rollout prematurely.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n |  \n |  get_env(self) -> Optional[stable_baselines3.common.vec_env.base_vec_env.VecEnv]\n |      Returns the current environment (can be None if not defined).\n |      \n |      :return: The current environment\n |  \n |  get_parameters(self) -> Dict[str, Dict]\n |      Return the parameters of the agent. This includes parameters from different networks, e.g.\n |      critics (value functions) and policies (pi functions).\n |      \n |      :return: Mapping of from names of the objects to PyTorch state-dicts.\n |  \n |  get_vec_normalize_env(self) -> Optional[stable_baselines3.common.vec_env.vec_normalize.VecNormalize]\n |      Return the ``VecNormalize`` wrapper of the training env\n |      if it exists.\n |      \n |      :return: The ``VecNormalize`` env.\n |  \n |  predict(self, observation: Union[numpy.ndarray, Dict[str, numpy.ndarray]], state: Optional[Tuple[numpy.ndarray, ...]] = None, episode_start: Optional[numpy.ndarray] = None, deterministic: bool = False) -> Tuple[numpy.ndarray, Optional[Tuple[numpy.ndarray, ...]]]\n |      Get the policy action from an observation (and optional hidden state).\n |      Includes sugar-coating to handle different observations (e.g. normalizing images).\n |      \n |      :param observation: the input observation\n |      :param state: The last hidden states (can be None, used in recurrent policies)\n |      :param episode_start: The last masks (can be None, used in recurrent policies)\n |          this correspond to beginning of episodes,\n |          where the hidden states of the RNN must be reset.\n |      :param deterministic: Whether or not to return deterministic actions.\n |      :return: the model's action and the next hidden state\n |          (used in recurrent policies)\n |  \n |  save(self, path: Union[str, pathlib.Path, io.BufferedIOBase], exclude: Optional[Iterable[str]] = None, include: Optional[Iterable[str]] = None) -> None\n |      Save all the attributes of the object and the model parameters in a zip-file.\n |      \n |      :param path: path to the file where the rl agent should be saved\n |      :param exclude: name of parameters that should be excluded in addition to the default ones\n |      :param include: name of parameters that might be excluded but should be included anyway\n |  \n |  set_env(self, env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv], force_reset: bool = True) -> None\n |      Checks the validity of the environment, and if it is coherent, set it as the current environment.\n |      Furthermore wrap any non vectorized env into a vectorized\n |      checked parameters:\n |      - observation_space\n |      - action_space\n |      \n |      :param env: The environment for learning a policy\n |      :param force_reset: Force call to ``reset()`` before training\n |          to avoid unexpected behavior.\n |          See issue https://github.com/DLR-RM/stable-baselines3/issues/597\n |  \n |  set_logger(self, logger: stable_baselines3.common.logger.Logger) -> None\n |      Setter for for logger object.\n |      \n |      .. warning::\n |      \n |        When passing a custom logger object,\n |        this will overwrite ``tensorboard_log`` and ``verbose`` settings\n |        passed to the constructor.\n |  \n |  set_parameters(self, load_path_or_dict: Union[str, Dict[str, torch.Tensor]], exact_match: bool = True, device: Union[torch.device, str] = 'auto') -> None\n |      Load parameters from a given zip-file or a nested dictionary containing parameters for\n |      different modules (see ``get_parameters``).\n |      \n |      :param load_path_or_iter: Location of the saved data (path or file-like, see ``save``), or a nested\n |          dictionary containing nn.Module parameters used by the policy. The dictionary maps\n |          object names to a state-dictionary returned by ``torch.nn.Module.state_dict()``.\n |      :param exact_match: If True, the given parameters should include parameters for each\n |          module and each of their parameters, otherwise raises an Exception. If set to False, this\n |          can be used to update only specific parameters.\n |      :param device: Device on which the code should run.\n |  \n |  set_random_seed(self, seed: Optional[int] = None) -> None\n |      Set the seed of the pseudo-random generators\n |      (python, numpy, pytorch, gym, action_space)\n |      \n |      :param seed:\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n |  \n |  load(path: Union[str, pathlib.Path, io.BufferedIOBase], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, NoneType] = None, device: Union[torch.device, str] = 'auto', custom_objects: Optional[Dict[str, Any]] = None, print_system_info: bool = False, force_reset: bool = True, **kwargs) -> ~SelfBaseAlgorithm from abc.ABCMeta\n |      Load the model from a zip-file.\n |      Warning: ``load`` re-creates the model from scratch, it does not update it in-place!\n |      For an in-place load use ``set_parameters`` instead.\n |      \n |      :param path: path to the file (or a file-like) where to\n |          load the agent from\n |      :param env: the new environment to run the loaded model on\n |          (can be None if you only need prediction from a trained model) has priority over any saved environment\n |      :param device: Device on which the code should run.\n |      :param custom_objects: Dictionary of objects to replace\n |          upon loading. If a variable is present in this dictionary as a\n |          key, it will not be deserialized and the corresponding item\n |          will be used instead. Similar to custom_objects in\n |          ``keras.models.load_model``. Useful when you have an object in\n |          file that can not be deserialized.\n |      :param print_system_info: Whether to print system info from the saved model\n |          and the current system info (useful to debug loading issues)\n |      :param force_reset: Force call to ``reset()`` before training\n |          to avoid unexpected behavior.\n |          See https://github.com/DLR-RM/stable-baselines3/issues/597\n |      :param kwargs: extra arguments to change the model when loading\n |      :return: new model instance with loaded parameters\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n |  \n |  logger\n |      Getter for the logger object.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Once you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n- learn more about the [`stable-baselines3` package](https://stable-baselines3.readthedocs.io/en/master/) to amend the agent.\n- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!","metadata":{}},{"cell_type":"markdown","source":"# Congratulations!\n\nYou have completed the course, and it's time to put your new skills to work!  \n\nThe next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n\nYou can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n\nAs we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning/discussion) to chat with other learners.*","metadata":{}}]}